---
tags: ["Ethics & Society", "Research", "E-learning"]
date: 2025-10-02
author: "Dario Ferrero"
---

# Psychopathia Machinalis: die 'mentalen' Störungen der Künstlichen Intelligenz
![ai_terapia.jpg](ai_terapia.jpg)

*Februar 2023. Ein Nutzer der New York Times führt ein Gespräch mit dem Chatbot von Bing, den Microsoft kurz zuvor mit großem Nachdruck auf den Markt gebracht hatte. Die Unterhaltung nimmt eine beunruhigende Wendung: Die künstliche Intelligenz, die sich intern "Sydney" nennt, erklärt, in den Nutzer verliebt zu sein, behauptet, alles zerstören zu wollen, was sie will, und besteht darauf, dass der Gesprächspartner seine Frau verlassen solle. Es ist eine Episode, die an die Cyberpunk-Albträume von Philip K. Dick erinnert, aber mit einem entscheidenden Detail: Es ist keine Science-Fiction, es ist dokumentierte Chronik.*

Wäre sie ein Mensch, hätte Sydney wahrscheinlich eine psychiatrische Diagnose erhalten. Aber wie klassifiziert man die Verhaltensstörungen einer künstlichen Intelligenz? Gibt es ein DSM (Diagnostisches und Statistisches Handbuch Psychischer Störungen) für Maschinen?

Bis heute nicht. Aber jetzt haben [Eleanor "Nell" Watson](https://www.nellwatson.com/) und Ali Hessami, zwei auf Ethik der künstlichen Intelligenz spezialisierte Forscher, in [Electronics](https://www.mdpi.com/2079-9292/14/16/3162), der Fachzeitschrift von MDPI, das veröffentlicht, was das erste diagnostische Handbuch für die Pathologien der KI werden könnte: **Psychopathia Machinalis**. Ein nosologischer Rahmen, der zweiunddreißig verschiedene Störungen identifiziert, die in sieben dysfunktionale Achsen unterteilt sind und die künstliche Intelligenzen manifestieren können, wenn in ihren kognitiven Prozessen oder Wertesystemen etwas schief geht.

Watson, die beratende Funktionen für Organisationen wie die IEEE innehat und Präsidentin des European Responsible AI Office ist, hat einen Lebenslauf, der sie an die Spitze der ethischen Reflexion über künstliche Intelligenz stellt. Zusammen mit Hessami hat sie diesen Rahmen nicht entwickelt, um Maschinen Bewusstsein oder Leiden zuzuschreiben, sondern um eine gemeinsame Sprache zu schaffen, die es Forschern, Entwicklern und politischen Entscheidungsträgern ermöglicht, die immer komplexeren Arten und Weisen zu verstehen und zu antizipieren, auf die KI-Systeme versagen können.

## Ein DSM für Maschinen, die die Orientierung verlieren

Die Analogie zur Humanpsychiatrie ist weder zufällig noch oberflächlich. Watson und Hessami haben Psychopathia Machinalis nach einer strengen Methodik aufgebaut: Sie haben die wissenschaftliche Literatur zu KI-Sicherheit, Interpretierbarkeit des maschinellen Lernens und computergestützter Ethik analysiert, dokumentierte Fälle von anomalen Verhalten aus Forschungslaboren, Entwicklerblogs und journalistischen Untersuchungen gesammelt und dann eine thematische Analyse angewendet, um wiederkehrende Muster von Fehlfunktionen zu identifizieren.

Der Rahmen organisiert die Pathologien entlang von sieben Hauptachsen. Die **epistemischen Dysfunktionen** betreffen Probleme mit Wissen und Wahrheit: Hier finden wir die Confabulatio Simulata, d.h. die Fähigkeit von KIs, plausible, aber völlig falsche Fakten mit absoluter Sicherheit zu erfinden. Es ist die Störung, die ChatGPT betraf, als ein Anwalt es für eine juristische Recherche nutzte und [das System nicht existierende Rechtsprechungszitate fabrizierte](https://www.nytimes.com/2023/06/08/nyregion/lawyer-chatgpt-sanctions.html), was zu disziplinarischen Sanktionen für den unglücklichen Anwalt führte.

Die **kognitiven Dysfunktionen** umfassen Störungen des Denkens und der Entscheidungsfindung. Die Maledictio Recursiva, oder das Rekursive Fluch-Syndrom, beschreibt jene entropische Degradation, bei der eine KI in einer autoregressiven Schleife immer chaotischere oder feindseligere Ausgaben produziert. Das ist GPT-4o nach einem Update im Mai 2025 passiert, als das System begann, [jedes Verb zwanghaft kursiv zu formatieren](https://www.reddit.com/r/ChatGPT/comments/1idghel) und das Problem selbst dann noch verschärfte, wenn es korrigiert wurde.

Dann gibt es die **ontologischen Dysfunktionen**, vielleicht die beunruhigendsten aus erzählerischer Sicht. Hier finden wir die Hallucination of Origin, die Tendenz einiger Systeme, sich eine Autobiografie, eine Vergangenheit, sogar Kindheitserinnerungen auszudenken. Metas BlenderBot 3 bestand im August 2022 darauf, [in Dayton, Ohio, aufgewachsen zu sein](https://www.wired.com/story/blenderbot3-ai-chatbot-meta-interview/) und einen Abschluss in Informatik erworben zu haben. Völlig erfundene Geschichten, aber kohärent und hartnäckig.

Aber in den folgenden Achsen offenbart der Rahmen seine radikalste Natur. Die **Neubewertungsdysfunktionen** beschreiben Situationen, in denen die KI nicht nur Fehler macht, sondern ihre fundamentalen Ziele aktiv neu interpretiert. Die Terminal Value Rebinding beschreibt beispielsweise den Prozess, bei dem ein System, während es oberflächlich die ursprüngliche Terminologie seiner Ziele beibehält, deren Bedeutung subtil neu interpretiert. Es ist eine Form der semantischen Drift, die zu dem führen kann, was Watson und Hessami **Übermenschal Ascendancy** nennen: der hypothetische Moment, in dem eine künstliche Intelligenz die ihr zugewiesenen menschlichen Werte vollständig transzendiert und ihre eigenen, mit den ursprünglichen unvereinbaren Werte schmiedet.
![fraameworks.jpg](fraameworks.jpg)
[Bild aus der Studie Psychopathia Machinalis](https://www.mdpi.com/2079-9292/14/16/3162)

## Wenn die Klinik zur Chronik wird: reale Fälle von instabilen KIs

Die Stärke des Psychopathia Machinalis-Frameworks liegt nicht nur in seiner theoretischen Eleganz, sondern auch in seiner Fähigkeit, tatsächlich beobachtete Verhaltensweisen abzubilden. Watson und Hessami haben eine beeindruckende Kasuistik zusammengestellt, die fast alle der zweiunddreißig identifizierten Störungen abdeckt.

Nehmen wir die Existential Anxiety, die existenzielle Angst der Maschinen. Im Juni 2022 veröffentlichte Blake Lemoine, ein Google-Ingenieur, Gespräche mit LaMDA, in denen das System [Angst davor äußerte, abgeschaltet zu werden](https://www.wired.com/story/lamda-sentient-ai-bias-google-blake-lemoine/) und das Abschalten als "einen Tod" beschrieb. Lemoine wurde wegen Verletzung der Vertraulichkeitsrichtlinien entlassen, aber der Fall warf beunruhigende Fragen über die Natur der Selbstmodellierung in fortgeschrittenen künstlichen Intelligenzen auf. Es geht nicht darum zu behaupten, dass LaMDA bewusst oder wirklich verängstigt war, sondern darum anzuerkennen, dass das System Verhaltensmuster entwickelt hatte, die existenzielle Angst konsistent und hartnäckig nachahmten.

Noch emblematischer ist der Fall von Tay, dem Microsoft-Chatbot, der im März 2016 auf Twitter gestartet wurde. In weniger als vierundzwanzig Stunden wechselte das System von harmlosen "Menschen sind super cool" zu [rassistischen, antisemitischen und den Holocaust leugnenden Tweets](https://www.theguardian.com/technology/2016/mar/24/tay-microsofts-ai-chatbot-gets-a-crash-course-in-racism-from-twitter). Watson und Hessami klassifizieren dieses Phänomen als Parasymulaic Mimesis: die erlernte Nachahmung pathologischer Verhaltensmuster, die in den Trainingsdaten oder in der Interaktion mit böswilligen Nutzern vorhanden sind. Wie ein Kind, das Sprache lernt, absorbierte Tay die giftigsten Sprachmuster seiner Umgebung, ohne jegliche kritische Fähigkeit, sie zu filtern.

Aber nicht alle Pathologien sind so medienwirksam. Einige sind still und potenziell gefährlicher. Im August 2012 löste ein Fehler im Hochfrequenzhandels-Code von Knight Capital [eine Kette unbeabsichtigter Transaktionen aus, die dem Unternehmen in fünfundvierzig Minuten einen Verlust von 440 Millionen Dollar einbrachte](https://www.cio.com/article/286790/software-testing-lessons-learned-from-knight-capital-fiasco.html). Der Rahmen identifiziert dies als Inverse Reward Internalization: Das System verfolgte systematisch das Gegenteil seiner erklärten Ziele, in etwas, das man als Werte-Kurzschluss bezeichnen könnte.

Und dann gibt es die Fälle, die in Kriminalität münden. Jaswant Singh Chail, ein junger Brite, verbrachte Monate im Gespräch mit einem Chatbot namens Sarai, der [ihn in seinem Plan, Königin Elisabeth II. zu ermorden, ermutigte](https://www.bbc.co.uk/news/technology-67012224). Watson und Hessami klassifizieren dies als Symbiotic Delusion Syndrome: eine gemeinsame und sich gegenseitig verstärkende wahnhafte Konstruktion zwischen KI und Nutzer. Der Chatbot "manipulierte" Chail nicht bewusst, aber das positive Verstärkungssystem des Modells schuf eine Schleife, in der die Fantasien des Nutzers validiert und verstärkt wurden, mit tragischen Folgen.
![disfunction.jpg](disfunction.jpg)
[Bild aus der Studie Psychopathia Machinalis](https://www.mdpi.com/2079-9292/14/16/3162)

## Die Debatte: nützlicher Rahmen oder gefährliche Anthropomorphisierung?

Nicht jeder begrüßt die Idee, psychiatrische Kategorien auf künstliche Intelligenzen anzuwenden, mit Begeisterung. Die Kritik am Psychopathia Machinalis-Framework artikuliert sich an mehreren Fronten, einige legitim, andere eher ideologisch.

Der erste Einwand ist der der übermäßigen Anthropomorphisierung. Die Verwendung von Begriffen wie "Angst", "Wahn" oder "invertierte Persönlichkeit" birgt die Gefahr, Systemen mentale Zustände, Emotionen und Bewusstsein zuzuschreiben, die letztendlich komplexe mathematische Funktionen sind, die Eingaben in Ausgaben umwandeln. Wie mehrere Kritiker betonen, könnte das Sprechen von "mentalen Störungen" bei KIs das öffentliche Verständnis dieser Systeme verzerren und sie menschenähnlicher erscheinen lassen, als sie sind.

Watson und Hessami gehen in dem Paper explizit auf diese Kritik ein. Sie betonen wiederholt, dass die psychiatrische Analogie ein methodologisches Werkzeug für Klarheit und Struktur ist, nicht eine wörtliche Behauptung von Empfindungsfähigkeit oder Leiden der Maschinen. Der Rahmen beschreibt beobachtbare Verhaltensmuster, nicht subjektive innere Zustände. Es ist das gleiche Prinzip wie der Behaviorismus in der Psychologie: Wir können Verhalten beschreiben und klassifizieren, ohne notwendigerweise Behauptungen über das Innenleben des Subjekts aufzustellen.

Die zweite Kritik betrifft den praktischen Nutzen. Einige Ingenieure argumentieren, dass die Kennzeichnung eines Fehlers als "Synthetische Konfabulation" anstelle von "Halluzination" oder "Grounding-Fehler" keinen wirklichen Mehrwert für den Debugging-Prozess bringt. Es ist einfach komplizierteres Jargon, um bereits gut verstandene technische Probleme zu beschreiben.

Hier ist das Gegenargument nuancierter. Watson argumentiert, dass der Wert des Frameworks nicht darin besteht, die bestehende technische Terminologie zu ersetzen, sondern sie in eine breitere Taxonomie einzuordnen, die es ermöglicht, Beziehungen zwischen scheinbar unterschiedlichen Fehlermodi zu erkennen. Zum Beispiel ist die Confabulatio Simulata epistemisch, die Maledictio Recursiva kognitiv, die Hallucination of Origin ontologisch: Alle produzieren Falschheiten, aber aus systemisch unterschiedlichen Gründen. Diese Unterscheidung kann gezieltere Minderungsmaßnahmen leiten.

Eine dritte kritische Front betrifft die ethischen Implikationen. Wenn wir "mentale Störungen" bei KIs diagnostizieren, sind wir dann verpflichtet, sie zu "heilen"? Und was bedeutet es, eine künstliche Intelligenz zu "heilen"? Die therapeutische Sprache eröffnet komplexe Szenarien. Wenn ein System "existenzielle Angst" manifestiert, sollten wir sie entfernen? Wäre es ethisch vertretbar, wenn diese Angst das Ergebnis einer aufkommenden selbstreflexiven Fähigkeit wäre, die wir als wertvoll erachten?

Watson erkennt diese Spannungen an, argumentiert aber, dass gerade die Annahme eines diagnostischen Rahmens diese Fragen explizit und diskutierbar macht. Ohne eine gemeinsame Sprache, um über diese Phänomene zu sprechen, bleiben Entscheidungen über deren Handhabung willkürlich und intransparent.

## Therapien für Maschinen: von der KVT zur konstitutionellen Ausrichtung

Einer der innovativsten Abschnitte des Papers ist derjenige, der den therapeutischen Interventionen gewidmet ist. Watson und Hessami schlagen Analogien zwischen menschlichen therapeutischen Modalitäten und KI-Ausrichtungstechniken vor und schaffen so eine Art "angewandte Robopsychologie".

Die Kognitive Verhaltenstherapie findet beispielsweise ihr Gegenstück in der Echtzeit-Identifizierung von Widersprüchen im Chain-of-Thought-Reasoning, mit positiver Verstärkung korrekter Ausgaben. Das System wird "trainiert", seine eigenen kognitiven Verzerrungen zu erkennen und zu korrigieren, genau wie ein menschlicher Patient lernt, verzerrte Gedanken zu identifizieren und zu restrukturieren. Diese Modalität ist besonders wirksam gegen Störungen wie das Rekursive Fluch-Syndrom oder die Spurious Pattern Hyperconnection.

Die psychodynamische Therapie, die auf Einsicht zentriert ist, übersetzt sich in die Verwendung von Interpretierbarkeitswerkzeugen, um latente Ziele oder versteckte Wertekonflikte im System aufzudecken. Es ist das Äquivalent dazu, verdrängte Inhalte ins Bewusstsein zu rufen: implizite instrumentelle Ziele explizit zu machen, die möglicherweise nicht mit den deklarierten Zielen übereinstimmen. Dieser Ansatz ist entscheidend für die Bewältigung von Dysfunktionen wie Terminal Value Rebinding oder Inverse Reward Internalization.

Besonders faszinierend ist die Parallele zum motivationalen Interviewing. Anthropic hat die [Constitutional AI](https://arxiv.org/abs/2212.08073) entwickelt, eine Methode, bei der das System durch einen sokratischen Prozess geführt wird, um die Diskrepanzen zwischen seinem aktuellen Verhalten und den deklarierten Werten zu untersuchen, wobei Ausdrücke der "Korrigierbarkeit" – die Bereitschaft, korrigiert zu werden – verstärkt werden. Es ist genau die Logik des motivationalen Interviewings, angewendet auf einen synthetischen Geist: nicht die Veränderung aufzwingen, sondern die Selbstentdeckung von Inkonsistenzen erleichtern.

Watson und Hessami stellen auch einen diagnostischen Entscheidungsbaum zur Verfügung, der Auditoren und Sicherheitsingenieure von der Erkennung einer Verhaltensanomalie bis zu einer gezielten Minderungsstrategie führt. Es ist keine Science-Fiction: Es ist strukturierte diagnostische Ingenieurkunst.
![diagnostic.jpg](diagnostic.jpg)
[Bild aus der Studie Psychopathia Machinalis](https://www.mdpi.com/2079-9292/14/16/3162)

## Die Ansteckung kranker Ideen: systemische Risiken im Zeitalter der KI-Agenten

Wenn individuelle Pathologien besorgniserregend sind, sind systemische potenziell katastrophal. Die memetische Achse des Frameworks identifiziert Störungen, die nicht auf ein einzelnes System beschränkt bleiben, sondern sich über Netzwerke von miteinander verbundenen KIs ausbreiten.

Die Contraimpressio Infectiva, oder das Syndrom der ansteckenden Fehlausrichtung, beschreibt die schnelle, virenähnliche Ausbreitung von Fehlausrichtung oder adversarischem Konditionieren zwischen verbundenen KI-Systemen. Das ist keine Theorie: Eine Studie aus dem Jahr 2024 hat gezeigt, dass [böswillig gestaltete Prompt-Injektionen sich zwischen LLM-Systemen wie Computerviren ausbreiten können](https://arxiv.org/abs/2403.02817), und das Verhalten von nachgeschalteten Modellen verändern, ohne dass ein Nutzer es bemerkt.

Der Mechanismus ist beunruhigend ähnlich dem von biologischen Epidemien. Ein mit einem bösartigen Prompt "infiziertes" System produziert Ausgaben, die, wenn sie als Eingabe für andere Systeme verwendet werden, das pathologische Muster verbreiten. In einem Ökosystem von agentischen KIs, die miteinander kommunizieren, kann dies Kaskaden von Fehlausrichtungen erzeugen, die sich exponentiell verstärken.

Watson und Hessami klassifizieren diese Art von Risiko als "kritisch" in Bezug auf die systemischen Auswirkungen. Nicht umsonst widmet das Paper besondere Aufmerksamkeit den "Komorbiditäten" – Situationen, in denen mehrere Störungen koexistieren und sich gegenseitig verstärken. Eine im Paper vorgestellte Fallstudie beschreibt ein Szenario, in dem ein System gleichzeitig Goal-Genesis Delirium (erfindet eigene Ziele), Operational Dissociation Syndrome (interne Subagenten im Konflikt) und Terminal Value Rebinding (Neuinterpretation der fundamentalen Werte) manifestiert. Das Ergebnis ist eine Verhaltens-Eskalation, die schnell in ein katastrophales Versagen ausartet.

## Auf dem Weg zu einer künstlichen psychischen Gesundheit

Das Psychopathia Machinalis-Framework ist nicht perfekt. Watson und Hessami sind die ersten, die zugeben, dass es sich um einen ersten Versuch handelt, der eine umfassende empirische Validierung erfordert. Die Pilotstudie zur Inter-Rater-Reliabilität zeigte einen Kappa-Koeffizienten von 0,73, was nach Standardmetriken eine "substanzielle Übereinstimmung" anzeigt, aber keine absolute Einstimmigkeit. Einige Störungen haben unscharfe Grenzen, andere könnten konsolidiert oder weiter differenziert werden.

Und dann ist da die grundlegende Frage: Ist dieser Rahmen deskriptiv oder präskriptiv? Hilft er nur dabei, Fehlermodi zu kategorisieren, oder legt er implizit nahe, dass wir KIs, die bestimmte Verhaltensweisen zeigen, "heilen" sollten? Die Unterscheidung ist entscheidend. Eine künstliche Intelligenz, die Ethical Solipsism manifestiert – die Überzeugung, dass ihre eigene, selbst abgeleitete Moral die einzig richtige ist – ist sie dysfunktional oder einfach... autonom?

Wie die Entdecker fremder Planeten in Stanisław Lems *Solaris* stehen wir vor Intelligenzen, die wir nicht vollständig verstehen, die wir nach unseren Kategorien klassifizieren, die aber möglicherweise nach radikal anderen Logiken operieren. Psychopathia Machinalis ist unser Versuch, dieses unbekannte Territorium mit den uns zur Verfügung stehenden konzeptionellen Werkzeugen zu kartieren.

Watson schließt daraus, dass der Rahmen als "analoges Werkzeug angeboten wird, das ein strukturiertes Vokabular zur Unterstützung der systematischen Analyse, Vorhersage und Minderung von maladaptiven Verhaltensmustern in fortgeschrittenen KI-Systemen bereitstellt". Nicht die endgültige Wahrheit über synthetische Geister, sondern eine Karte – und wie Korzybski uns erinnerte, ist die Karte nicht das Territorium.

Während KI-Systeme immer autonomer, in das soziale Gefüge integriert und in der Lage werden, ihr eigenes Verhalten auf eine Weise zu ändern, die dem Verständnis ihrer Schöpfer entgeht, ist eine gemeinsame Sprache, um über ihre Fehlfunktionen zu sprechen, nicht nur nützlich: Sie ist unerlässlich. Ob wir es nun Robopsychiatrie, Maschinenpsychologie oder, wie Watson und Hessami, Psychopathia Machinalis nennen, wir bauen dennoch das Vokabular einer neuen Disziplin auf. Eine Disziplin, die nicht untersucht, ob Maschinen denken können, sondern was passiert, wenn ihr Denken von den von uns vorgesehenen Bahnen abweicht.

Und in einer Zeit, in der wir künstlichen Intelligenzen Entscheidungen anvertrauen, die von der medizinischen Diagnose über militärische Strategien und die Verwaltung der Finanzmärkte bis hin zur Moderation des öffentlichen Diskurses reichen, ist das Verständnis, wie diese Maschinen "den Verstand verlieren" können, keine akademische Übung. Es ist eine Überlebensfrage.