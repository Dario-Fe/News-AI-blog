---
tags: ["Generative AI", "Business", "Startups"]
date: 2025-10-13
author: "Dario Ferrero"
youtube_url: "https://youtu.be/m_qMQVY0_I8?si=lE-LycwJ09z6z7Uh"
---

# L'esplosione dell'AI Video Generativa. Tra l'hype delle big e l'ombra degli open source. Ovi e gli altri ribelli.
![ai-video-gen.jpg](ai-video-gen.jpg)


*Il 2024 è stato per l'intelligenza artificiale generativa video quello che il 1991 fu per il grunge: l'esplosione improvvisa di qualcosa che covava da tempo. Se a febbraio 2024 OpenAI sganciava la bomba con Sora 1, mostrando video generati che facevano gridare al miracolo tecnologico, a distanza di appena un anno il panorama si è moltiplicato in modo esponenziale.*

Google ha risposto con Veo, Meta ha tirato fuori dal cilindro Movie Gen, e decine di laboratori universitari e startup hanno iniziato a pubblicare i propri modelli con una frequenza febbrile. Come in quell'esplosione musicale di Seattle, dove accanto ai Nirvana spuntavano Soundgarden, Pearl Jam e Alice in Chains, oggi accanto ai colossi tech emergono progetti open source che promettono di democratizzare una tecnologia ancora largamente chiusa nei data center delle big tech.

La timeline dell'accelerazione è vertiginosa. Da quando Sora 1 ha mostrato al mondo che era possibile generare video fotorealistici da semplici prompt testuali, il settore ha vissuto una corsa forsennata. A settembre 2024 Google ha lanciato Veo 2, puntando su uno stile cinematografico e una qualità visiva impressionante. Meta, non volendo restare indietro, ha presentato Movie Gen a ottobre 2024, un sistema capace di produrre video fino a 16 secondi con audio musicale e effetti sonori sincronizzati. E poi, a settembre 2025, è arrivato Sora 2, con l'aggiunta tanto attesa dell'audio sincronizzato, compresi dialoghi ed effetti sonori. Nel frattempo, il mondo open source non è stato a guardare: progetti come [HunyuanVideo di Tencent](https://github.com/Tencent/HunyuanVideo), Mochi 1, Open-Sora 1.3 e ora Ovi hanno iniziato a offrire alternative concrete, anche se con approcci e risultati molto diversi.

Eppure, dietro il marketing scintillante e i video dimostrativi da togliere il fiato, la realtà è più complessa e meno democratica di quanto i comunicati stampa vogliano far credere. I modelli dei giganti tech sono ancora largamente inaccessibili, chiusi dietro waitlist infinite, abbonamenti premium o semplicemente non disponibili al pubblico. E qui entra in gioco la questione centrale: l'hype generato dalle grandi aziende è proporzionale ai risultati effettivamente accessibili agli utenti? O siamo di fronte all'ennesima operazione di marketing che promette rivoluzioni mentre consegna accessi limitati e modelli proprietari impossibili da modificare o studiare?

## I giganti e i loro lucchetti d'oro

Sora 2, presentato da OpenAI con grande fanfara mediatica a settembre 2025, rappresenta sulla carta un salto evolutivo significativo. Il modello genera video fino a 20 secondi con [audio sincronizzato, dialoghi ed effetti sonori](https://openai.com/it-IT/index/sora-2/), promettendo una qualità cinematografica e una coerenza narrativa impressionante. I video dimostrativi mostrano scene complesse, con movimenti di camera fluidi e un realismo che rasenta il fotorealismo. Tuttavia, come spesso accade nel mondo OpenAI, l'accesso è tutt'altro che aperto. Sora 2 è disponibile solo per gli abbonati ChatGPT Plus e Pro, con costi che partono da 20 dollari al mese e possono salire considerevolmente per chi vuole generare più di qualche clip al mese. Non esiste una versione API pubblica stabile, nessuna possibilità di scaricare i pesi del modello, nessuna documentazione tecnica che permetta di capire realmente come funziona il sistema. È come se i Led Zeppelin avessero pubblicato "Stairway to Heaven" ma solo su un jukebox accessibile pagando una fee mensile, senza mai far uscire il disco.

Google, dal canto suo, ha puntato tutto su [Veo 3](https://aistudio.google.com/models/veo-3), un modello che enfatizza lo stile cinematografico e la qualità visiva di livello professionale. Veo 3 genera video fino a 2 minuti, supporta risoluzioni elevate e include audio musicale e ambientale sincronizzato. Anche in questo caso, però, l'accesso è riservato agli utenti premium di Google One AI Premium, con costi che partono da 19,99 dollari al mese. La piattaforma VideoFX, dove Veo 3 è implementato, offre un'interfaccia user-friendly, ma resta un sistema completamente chiuso: nessun codice disponibile, nessuna possibilità di fine-tuning, nessuna trasparenza sui dati di training. È il classico approccio walled garden: funziona bene, ha un'estetica impeccabile, ma sei completamente in balia delle decisioni di Mountain View.

Meta ha scelto una strada apparentemente più aperta con Movie Gen e la piattaforma [Vibes](https://www.meta.ai/vibes/), integrata nell'app Meta AI. Movie Gen genera video fino a 16 secondi con musica ed effetti sonori, e Vibes permette agli utenti di creare, remixare e condividere brevi clip in un'esperienza social e creativa. Al momento, il servizio è gratuito, il che lo rende il più accessibile tra i modelli dei giganti. Ma qui sorge la domanda cruciale, ben evidenziata da un'[analisi critica di Facta](https://www.facta.news/articoli/vibes-meta-feed-intelligenza-artificiale): fino a quando resterà gratuito? La strategia di Meta è chiara: creare dipendenza dall'ecosistema, raccogliere dati utente in quantità industriale, e poi eventualmente monetizzare quando la base utenti sarà sufficientemente ampia e vincolata. Movie Gen non è disponibile per il download, non esiste una versione self-hosted, e la documentazione tecnica è vaga e parziale. È il modello freemium portato all'estremo: gratis oggi, ma domani chissà.

Il punto è che tutti questi modelli, per quanto impressionanti tecnicamente, condividono lo stesso limite strutturale: sono sistemi proprietari, inaccessibili nel loro funzionamento interno, modificabili solo nei parametri superficiali che le aziende decidono di esporre. Non puoi studiarli, non puoi adattarli alle tue esigenze specifiche, non puoi verificare come sono stati addestrati né quali dati hanno visto. E soprattutto, non puoi garantirti che continueranno a esistere o a essere accessibili tra sei mesi o un anno. Quando ti affidi a un modello closed source, stai di fatto affittando tecnologia, non possedendola. E l'affitto può salire, il contratto può cambiare, il servizio può essere dismesso. È successo con API di Google, con servizi di Amazon, con piattaforme Microsoft. Perché dovrebbe essere diverso con i modelli generativi?
![sora2.jpg](sora2.jpg)
[Immagine tratta dal trailer di Sora 2 ](https://openai.com/it-IT/index/sora-2/)

## Ovi: garage band contro major label

Ed è qui che entra in scena [Ovi](https://github.com/character-ai/Ovi), un progetto sviluppato da Character.AI e rilasciato come completamente open source a settembre 2025. Per continuare con la metafora musicale, se Sora e Veo sono le produzioni milionarie delle major label, con studi di registrazione all'avanguardia e budget illimitati, Ovi è la band che registra l'EP nel garage, con attrezzatura assemblata e tanta passione. Ma come spesso accade nella storia della musica, è proprio da quei garage che escono i dischi più interessanti e innovativi.

Ovi si basa su un'architettura che i suoi creatori definiscono [twin backbone cross-modal fusion](https://arxiv.org/abs/2510.01284), un approccio che suona complicato ma è concettualmente elegante. Invece di generare prima il video e poi aggiungere l'audio in post-produzione, o viceversa, Ovi modella le due modalità come un unico processo generativo. Il sistema utilizza due moduli DiT (Diffusion Transformer) identici, uno per il video e uno per l'audio, che vengono addestrati congiuntamente attraverso meccanismi di cross-attention bidirezionale. In pratica, i due moduli si parlano costantemente durante il processo di generazione, scambiandosi informazioni temporali e semantiche. Questo permette una sincronizzazione naturale tra immagini e suoni, senza bisogno di pipeline separate o allineamenti a posteriori.

La documentazione tecnica pubblicata su [arXiv](https://arxiv.org/abs/2510.01284) è trasparente e dettagliata. Il training si articola in due fasi: prima viene inizializzato un audio tower che rispecchia l'architettura di un modello video pre-addestrato, e viene addestrato da zero su centinaia di migliaia di ore di audio grezzo. In questa fase, il modello impara a generare effetti sonori realistici e parlato con ricca identità ed emozione. Nella seconda fase, le due torri vengono addestrate congiuntamente su un vasto corpus video, scambiando timing attraverso embeddings RoPE scalati e semantica attraverso cross-attention bidirezionale. Il risultato è un modello capace di generare simultaneamente video e audio sincronizzato, compresi dialoghi, effetti sonori e musica di sottofondo.

Ora, va detto chiaramente: Ovi non batte Sora 2 o Veo 3 in qualità visiva pura. I video generati sono limitati a 5 secondi, contro i 20 di Sora o i 120 di Veo. La risoluzione è inferiore, la fluidità dei movimenti meno raffinata, e la capacità di gestire scene complesse con molti elementi in movimento è ancora acerba. Ma questo confronto, per quanto inevitabile, è anche un po' fuorviante. È come confrontare una produzione hollywoodiana con un cortometraggio indipendentee: certo, la differenza tecnica è evidente, ma non necessariamente il secondo è meno interessante o utile del primo.

Il vero punto di forza di Ovi non è la superiorità tecnica, che onestamente non esiste, ma la filosofia open source che lo governa. Il codice è completamente disponibile su [GitHub](https://github.com/character-ai/Ovi), i pesi del modello sono scaricabili, la documentazione è accessibile e comprensibile. Puoi studiare come funziona il sistema, modificarlo, adattarlo alle tue esigenze, integrarlo in progetti più ampi. Puoi fare fine-tuning su dataset specifici, sperimentare con architetture diverse, contribuire alla community con miglioramenti e fix. E soprattutto, puoi farlo localmente, sul tuo hardware, senza dipendere da server remoti, API che possono cambiare o essere disattivate, o politiche di utilizzo che mutano da un giorno all'altro.

Certo, i requisiti hardware non sono banali. Per far girare Ovi in modo fluido serve almeno una GPU di fascia alta, con 24GB di VRAM o più, e una buona quantità di RAM di sistema. Non è esattamente qualcosa che puoi far girare sul portatile mentre sei in treno. Ma per una piccola azienda, uno studio creativo, un laboratorio universitario o anche un appassionato con un budget dedicato, è assolutamente fattibile. Stiamo parlando di qualche migliaio di euro di hardware, contro centinaia di euro al mese di abbonamenti a servizi closed source che potrebbero cambiare condizioni o sparire domani.

E c'è un altro aspetto spesso sottovalutato: la possibilità di verificare cosa il modello ha imparato e come lo usa. Con i modelli closed source, sei completamente al buio sui dati di training. Hanno incluso materiale protetto da copyright? Hanno usato video senza il consenso dei creatori? Hanno introdotto bias problematici? Non puoi saperlo. Con Ovi, almeno in teoria, puoi analizzare il codice, studiare le decisioni architetturali, e avere una comprensione più profonda di cosa sta realmente succedendo sotto il cofano. Non è solo una questione di trasparenza etica, ma anche di controllo tecnico e capacità di debugging.
![ovi.jpg](ovi.jpg)
[Immagine tratta dal trailer di Ovi ](https://github.com/character-ai/Ovi)

## Gli altri "ribelli" dell'open source video

Ovi non è solo nella sua battaglia per democratizzare l'AI video generativa. Attorno a lui si è formato un ecosistema variegato di progetti open source, ciascuno con approcci, punti di forza e limiti specifici. È un po' come la scena hardcore punk degli anni Ottanta: tante piccole etichette indipendenti, tante band che suonano in scantinati, poche risorse ma tanta determinazione.

[HunyuanVideo](https://github.com/Tencent/HunyuanVideo), sviluppato da Tencent, è forse il progetto più ambizioso in questo panorama. Punta a generare video fino a 10 secondi con una qualità visiva che si avvicina ai modelli commerciali, e supporta risoluzioni elevate. L'architettura è basata su diffusion transformer, simile a quella di Sora, e il modello è addestrato su un dataset enorme di video cinesi e internazionali. Il punto di forza è la fluidità dei movimenti e la coerenza temporale, ma l'audio è ancora assente. E qui si vede la differenza tra un progetto sostenuto da una corporation come Tencent e iniziative più piccole: le risorse ci sono, i risultati anche, ma l'accessibilità è limitata da requisiti hardware proibitivi e una configurazione complessa.

[Mochi 1](https://mochi1ai.com/it), invece, è un progetto più sperimentale, sviluppato dalla community e focalizzato sull'animazione di immagini statiche. L'idea è prendere un'immagine, magari generata con Stable Diffusion o DALL-E, e animarla con movimenti realistici. È particolarmente popolare tra gli artisti digitali che vogliono dare vita ai propri artwork senza dover imparare software di animazione tradizionali. La qualità è variabile, ma il potenziale creativo è notevole. Anche qui, però, l'audio è completamente assente, e la durata massima è di 3-4 secondi.

[Open-Sora 1.3](https://github.com/hpcaitech/Open-Sora) è un tentativo della community di replicare l'architettura di Sora originale basandosi sulle informazioni pubbliche disponibili. Non avendo accesso al codice di OpenAI, gli sviluppatori hanno dovuto fare reverse engineering delle descrizioni tecniche e dei paper correlati, creando un'architettura che in teoria dovrebbe funzionare in modo simile. I risultati sono interessanti ma ancora lontani dalla qualità di Sora 1, figuriamoci di Sora 2. La fluidità è spesso interrotta da artefatti, la coerenza temporale è fragile, e la gestione di scene complesse è problematica. Ma è un progetto vivo, con una community attiva che continua a migliorare il codice.

[AnimateDiff](https://github.com/guoyww/AnimateDiff) merita una menzione particolare perché ha un approccio completamente diverso. Invece di essere un modello standalone, è un'estensione di Stable Diffusion che aggiunge capacità di animazione. Installi AnimateDiff, lo colleghi al tuo setup di Stable Diffusion, e puoi trasformare le tue generazioni in brevi animazioni. È popolare tra chi già usa Stable Diffusion per l'arte generativa, perché permette di integrare l'animazione nel workflow esistente senza dover imparare un nuovo sistema da zero. Ma anche qui, audio assente e durate brevissime.

[CogVideoX](https://github.com/zai-org/CogVideo) di Tsinghua University merita sicuramente una menzione. Sviluppato dal laboratorio THUDM e aggiornato a novembre 2024 con la versione 1.5, è uno dei progetti open source più maturi nel panorama. CogVideoX-5B genera video fino a 10 secondi a risoluzione 720x480, e la versione 1.5 introduce il supporto per l'image-to-video a qualsiasi risoluzione. L'architettura si basa su diffusion transformer con un expert transformer che gestisce meglio i movimenti complessi rispetto ai modelli precedenti. È particolarmente apprezzato nella community per la qualità della coerenza temporale e per avere superato nei benchmark competitor come VideoCrafter-2.0 e Open-Sora. Il codice è completamente disponibile su GitHub e Hugging Face, con documentazione dettagliata. L'unico limite resta l'assenza di audio, ma per chi cerca generazione video pura con buona qualità e durate decenti, CogVideoX è una delle opzioni più solide.

[LTX-Video](https://huggingface.co/Lightricks/LTX-Video) di Lightricks, lanciato a novembre 2024 con una promessa ambiziosa: generazione video in tempo reale. Con 2 miliardi di parametri nella versione iniziale (e 13 miliardi nella versione lanciata a maggio 2025), LTX-Video è il primo modello DiT capace di generare video a 30 FPS con risoluzione 1216x704 più velocemente di quanto si impieghi a guardarli. Lightricks sostiene che sia 30 volte più veloce dei modelli comparabili, e questo lo rende particolarmente interessante per applicazioni che richiedono iterazioni rapide. È completamente open source, integrato in ComfyUI per chi già usa quel workflow, e ha una community attiva che sta contribuendo con miglioramenti sulla consistenza del movimento e la coerenza delle scene. Anche qui, niente audio, ma la velocità di generazione è un vantaggio competitivo non da poco per chi fa prototipazione o lavoro creativo iterativo.

Il quadro che emerge è chiaro: l'ecosistema open source è vivace, creativo, e in rapida evoluzione. Ma è anche frammentato, spesso sottodimensionato in termini di risorse, e ancora parecchio indietro rispetto ai giganti commerciali in termini di qualità assoluta. La maggior parte di questi progetti non include audio, e quando lo fa, la qualità è inferiore. Le durate sono corte, i requisiti hardware impegnativi, la configurazione complessa. Non sono soluzioni plug-and-play, ma strumenti che richiedono competenze tecniche, pazienza, e voglia di sperimentare.

## Democrazia o illusione?

Arriviamo così al nocciolo della questione: l'open source nel campo dell'AI video generativa è una vera alternativa democratica, o è solo un'illusione per smanettoni con troppe GPU e troppo tempo libero? La risposta, come spesso accade, è sfumata e dipende da chi sei e cosa vuoi fare.

Se sei un'azienda che ha bisogno di produrre contenuti video di alta qualità per campagne marketing, probabilmente i modelli commerciali come Sora 2 o Veo 3 sono ancora la scelta migliore. La qualità è superiore, l'interfaccia è user-friendly, e il supporto tecnico esiste. Paghi di più, certo, ma ottieni risultati immediati senza dover gestire infrastrutture complesse.

Ma se sei un ricercatore, uno sviluppatore, un artista che vuole sperimentare, o una piccola realtà che ha competenze tecniche ma budget limitati, allora progetti come Ovi diventano preziosi. Ti offrono libertà, controllo, e la possibilità di costruire qualcosa di personalizzato. Puoi integrare il modello in pipeline creative più ampie, adattarlo a esigenze specifiche, e non dipendere da decisioni aziendali che sfuggono al tuo controllo.

La vera democratizzazione, però, richiede più di semplice codice aperto. Richiede documentazione chiara, community attive, risorse educative, e una riduzione progressiva dei requisiti hardware. Serve che questi progetti diventino più accessibili, che l'installazione diventi più semplice, che i tutorial siano comprensibili anche a chi non è un esperto di machine learning. E serve sostenibilità economica: molti progetti open source nel campo dell'AI sono sviluppati da piccoli team o addirittura singoli individui che lavorano nel tempo libero, senza finanziamenti stabili. Quanto possono durare? Quanto possono competere con i laboratori di ricerca dei giganti tech che hanno budget di milioni di dollari?

Il futuro prossimo sarà probabilmente uno scenario ibrido. I modelli commerciali continueranno a dominare in termini di qualità assoluta e facilità d'uso, ma i progetti open source colonizzeranno nicchie specifiche: ricerca accademica, applicazioni artistiche sperimentali, integrazioni personalizzate, casi d'uso dove il controllo e la trasparenza contano più della perfezione visiva. Ovi e i suoi compagni di strada non sostituiranno Sora o Veo, ma offriranno un'alternativa concreta per chi vuole o ha bisogno di quell'alternativa.

E forse, proprio come accadde con il punk e l'indie rock, da questi garage band nasceranno le idee e le innovazioni che domani i giganti corporativi assorbiranno nei loro prodotti mainstream. La storia della tecnologia è piena di esempi di progetti open source che hanno anticipato trend successivamente adottati dall'industria. Linux, Python, TensorFlow stesso. Non sarebbe la prima volta che il garage batte lo studio di registrazione milionario. Non in termini di budget o luccichio, ma di idee, libertà, e capacità di cambiare le regole del gioco.