---
tags: ["Ethics & Society", "Security", "Generative AI"]
date: 2025-10-15
author: "Dario Ferrero"
youtube_url: "https://youtu.be/Eg61-05-jTs?si=lgfaqGhx4HUeXlNE"
---

# L'IA Strumento e non Oracolo: il caso Deloitte e l'illusione dell'infallibilità
![strumenti-oracoli-caso-deloitte.jpg](strumenti-oracoli-caso-deloitte.jpg)


*Agosto 2025. Christopher Rudge, ricercatore della Sydney University e deputy director del Sydney Health Law research centre, sta leggendo un [corposo report di 237 pagine](https://www.notizie.ai/deloitte-australia-consegna-al-governo-un-rapporto-con-citazioni-inventate-da-gpt/) pubblicato poche settimane prima dal governo australiano.*

Il documento, commissionato dal Dipartimento per l'Occupazione e le Relazioni sul Lavoro al colosso della consulenza Deloitte per la modica cifra di 439.000 dollari australiani, dovrebbe verificare il "Targeted Compliance Framework", un sistema automatizzato che controlla l'erogazione delle indennità sociali. Mentre scorre le pagine dense di riferimenti accademici e giurisprudenziali, qualcosa non torna. Una citazione in particolare attira la sua attenzione: una sentenza della Corte Federale che dovrebbe essere pietra angolare dell'argomentazione. Rudge va a verificare. La sentenza non esiste. Non è mai esistita.

Come un detective che tira un filo e si ritrova con un'intera matassa, il ricercatore comincia a scavare. Le citazioni fantasma si moltiplicano: articoli accademici mai pubblicati, decisioni giudiziarie mai pronunciate, riferimenti bibliografici che rimandano al nulla. Non si tratta di errori occasionali o refusi. È un intero castello costruito su fondamenta inesistenti, con la precisione formale e la sicumera tipica dei documenti professionali di alto livello. Rudge segnala tutto ai media e al governo. Il risultato? Uno degli scandali più imbarazzanti nella storia recente della consulenza aziendale e un caso di studio perfetto su come *non* usare l'intelligenza artificiale.

## L'ammissione di colpa e il prezzo della negligenza

La reazione di Deloitte, va detto, non si è fatta attendere. A fine settembre 2025, la società [ha pubblicamente riconosciuto](https://fortune.com/2025/10/07/deloitte-ai-australia-government-report-hallucinations-technology-290000-refund/) di aver utilizzato Azure OpenAI GPT-4o per la redazione iniziale del documento. Non un tool di supporto marginale, non un assistente per le bozze: il modello di linguaggio aveva generato ampie sezioni del report, citazioni comprese, che poi erano finite nel documento finale senza un'adeguata verifica. L'ammissione è stata accompagnata da una versione corretta del report, epurato dalle citazioni inventate, e dall'annuncio di un rimborso parziale al governo australiano. Ma il danno, come si dice, era fatto.

Quello che rende il caso Deloitte particolarmente significativo non è tanto l'errore tecnologico in sé, quanto l'atteggiamento che lo ha reso possibile. Stiamo parlando di una delle Big Four della consulenza globale, un'organizzazione con processi di quality assurance stratificati, revisori esperti, partner responsabili. Eppure, da qualche parte lungo la catena di produzione, qualcuno ha deciso che il testo generato da GPT-4o fosse sufficientemente affidabile da non richiedere una verifica capillare delle fonti. È come se un archeologo pubblicasse una scoperta sensazionale basandosi sulle voci del quartiere invece che sugli scavi, o come se Borges avesse scritto la sua biblioteca di Babele non come finzione letteraria ma come catalogo reale. Solo che qui non c'è ironia postmoderna: c'è un report governativo da quasi mezzo milione di dollari che avrebbe dovuto guidare politiche pubbliche.
![fortune.jpg](fortune.jpg)
[Immagine tratta da fortune.com](https://fortune.com/2025/10/07/deloitte-ai-australia-government-report-hallucinations-technology-290000-refund/)

## L'epidemia silenziosa delle allucinazioni digitali

Il caso Deloitte non è un cigno nero, un'anomalia isolata nel panorama dell'adozione dell'IA. È piuttosto la punta più visibile di un iceberg che sta emergendo con crescente chiarezza. Nel maggio 2023, un avvocato di New York di nome Steven Schwartz [è finito sotto i riflettori](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/) per aver citato in un ricorso giudiziario sei casi precedenti completamente inventati da ChatGPT. Il giudice Kevin Castel non l'ha presa bene e ha comminato sanzioni. Nel settembre 2025, un avvocato californiano, Amir Mostafavi, [ha ricevuto una multa storica](https://calmatters.org/economy/technology/2025/09/chatgpt-lawyer-fine-ai-regulation/) dalla Corte d'Appello dopo che è emerso che 21 delle 23 citazioni nel suo atto processuale erano false, generate da ChatGPT e mai verificate.

Il problema non riguarda solo consulenza e professioni legali. Pochi giorni fa, il quotidiano italiano "La Provincia" di Civitavecchia ha pubblicato un [articolo di cronaca](https://www.professionereporter.eu/2025/10/e-chatgpt-mando-in-pagina-un-pezzo-senza-intervento-umano-a-civitavecchia/) che si concludeva con una frase rivelatrice: “Vuoi che lo trasformi in un articolo da pubblicare su un quotidiano (con titolo, occhiello e impaginazione giornalistica) o in una versione più narrativa da magazine d’inchiesta?”. Era l'inequivocabile output di ChatGPT, finito in stampa senza che nessuno si accorgesse che non si trattava di testo redazionale ma del prompt di chiusura tipico dell'assistente AI.

L'incidente, per quanto imbarazzante, è significativo non per la gaffe in sé ma per quello che rivela: anche nel giornalismo, ambito dove la verifica delle fonti dovrebbe essere DNA professionale, la tentazione di delegare la scrittura all'IA senza supervisione adeguata sta producendo scivoloni. Il direttore ha parlato di "errore umano" e annunciato procedure più stringenti, ma il pattern è lo stesso visto con Deloitte: strumenti potenti usati con leggerezza, nella convinzione che la forma impeccabile del testo generato corrisponda a contenuti affidabili. La differenza è che qui l'errore era così plateale da essere immediatamente visibile, mentre citazioni accademiche inventate o riferimenti giurisprudenziali falsi possono passare inosservati per mesi.

Ma il fenomeno va ben oltre i casi finiti sui giornali o generati dai giornali. Secondo uno [studio di Stanford](https://hai.stanford.edu/news/ai-trial-legal-models-hallucinate-1-out-6-or-more-benchmarking-queries) pubblicato nel 2025, anche i modelli di linguaggio specializzati per il settore legale producono allucinazioni in almeno un caso su sei, mentre i chatbot generalisti raggiungono tassi di errore compresi tra il 58% e l'82% quando interpellati su questioni giuridiche. Un ricercatore legale di nome Damien Charlotin ha identificato tre tipologie ricorrenti di queste allucinazioni: citazioni a casi completamente inventati, citazioni corrette da casi reali con contenuti falsificati, e la più insidiosa di tutte, citazioni formalmente esatte ma che non supportano affatto l'argomentazione per cui vengono invocate.

Quest'ultimo tipo è particolarmente pericoloso perché supera la verifica superficiale. Il caso esiste, la citazione è formattata correttamente, ma se vai effettivamente a leggere la sentenza scopri che dice tutt'altro. È come se il Necronomicon di Lovecraft citasse opere realmente esistenti di Poe e Bierce, ma attribuendo loro contenuti completamente diversi: la forma è perfetta, la sostanza è fantasmagorica.
![stanford.jpg](stanford.jpg)
[Immagine tratta Stanford.edu](https://hai.stanford.edu/news/ai-trial-legal-models-hallucinate-1-out-6-or-more-benchmarking-queries)

## Anatomia di un'allucinazione: perché l'IA inventa con tale sicurezza

Per capire cosa è successo nel caso Deloitte, bisogna fare un passo indietro e comprendere come funzionano realmente i Large Language Models come GPT-4o. Questi sistemi sono straordinarie macchine per la predizione statistica del linguaggio: dato un contesto, calcolano quale sequenza di parole è più probabilmente coerente con quel contesto, basandosi su miliardi di esempi visti durante l'addestramento. Sono incredibilmente bravi a catturare pattern, strutture retoriche, convenzioni di scrittura. Quando gli chiedi di generare una citazione accademica nel formato giusto, con autori, titolo, anno e rivista, il modello sa perfettamente *come* dovrebbe apparire quella citazione. Ha visto migliaia di esempi simili.

Ma qui sta il problema fondamentale: il modello non *sa* se quella citazione esiste davvero. Non ha accesso a un database di verifiche, non controlla se l'articolo "Smith, J. (2023). AI Governance in Welfare Systems. Journal of Public Administration" è mai stato pubblicato. Semplicemente genera una stringa di testo che *sembra* una citazione plausibile, basandosi su pattern linguistici. È come un falsario talmente abile da ricreare alla perfezione lo stile di Vermeer, ma che non ha mai visto il dipinto originale perché non esiste: sta creando un falso di un'opera immaginaria.

Questa caratteristica degli LLM è nota nella letteratura tecnica come "allucinazione", un termine forse troppo poetico per descrivere quello che è essenzialmente un bug del design. I modelli di linguaggio sono stati ottimizzati per essere convincenti, fluenti, coerenti nella forma. La verità fattuale è una proprietà emergente, non l'obiettivo primario dell'architettura. Quando il contesto fornisce abbastanza informazioni verificabili, i modelli moderni come GPT-4o possono essere sorprendentemente accurati. Ma quando devono "riempire i vuoti", quando il database di addestramento non contiene la risposta esatta, passano senza soluzione di continuità dalla restituzione di fatti memorizzati all'invenzione plausibile. E lo fanno con lo stesso tono assertivo, la stessa formattazione impeccabile, lo stesso registro professionale.

## Il vero problema: l'illusione dell'infallibilità

Il cuore del disastro Deloitte non è tecnologico ma culturale. È l'atteggiamento con cui ci si è approcciati allo strumento. Da qualche parte, in quella catena di produzione del report, è stata presa una decisione: considerare GPT-4o non come un generatore di bozze da verificare parola per parola, ma come una fonte affidabile, una sorta di ghost writer esperto che producesse contenuti già pronti per la pubblicazione. È un errore di categoria fondamentale, paragonabile a scambiare un simulatore di volo per un aereo vero: la somiglianza superficiale maschera differenze sostanziali e critiche.

Questo atteggiamento acritico è alimentato da una narrativa pericolosa che circonda l'intelligenza artificiale contemporanea. Le aziende tecnologiche, comprensibilmente interessate a massimizzare l'adozione, tendono a enfatizzare le capacità dei loro sistemi presentandoli come quasi-onniscienti. Il marketing parla di "reasoning", di "understanding", di sistemi che "pensano" e "comprendono". Questa [antropomorfizzazione deliberata](https://aitalk.it/it/ai-cosciente) crea aspettative irrealistiche e abbassa le guardie. Se l'IA "capisce" il mio problema, perché dovrei dubitare delle sue risposte?

Il caso [documentato da NPR](https://www.npr.org/2025/07/10/nx-s1-5463512/ai-courts-lawyers-mypillow-fines) relativo agli avvocati di MyPillow è esemplare in questo senso. Professionisti del diritto, formati per il pensiero critico e la verifica delle fonti, hanno affidato a ChatGPT la ricerca giurisprudenziale e poi hanno depositato gli atti senza controllare. Non per malafede, ma perché l'interfaccia conversazionale del chatbot, la sua sicurezza formale, la coerenza delle risposte hanno creato un'illusione di competenza. Il sistema non ha mai detto "attenzione, questa citazione potrebbe essere inventata". Ha semplicemente generato testo plausibile, come è stato progettato per fare.

## Le implicazioni sistemiche: oltre l'imbarazzo corporativo

Quando si tratta del report Deloitte, le conseguenze vanno ben oltre l'imbarazzo di una multinazionale e il rimborso di qualche centinaia di migliaia di dollari. Il documento doveva verificare un sistema automatizzato che influenza l'erogazione di welfare a persone vulnerabili. Le raccomandazioni politiche basate su fondamenta inesistenti avrebbero potuto guidare decisioni con impatti reali sulla vita di cittadini. Se il Targeted Compliance Framework fosse stato modificato seguendo suggerimenti costruiti su citazioni false, gli effetti si sarebbero propagati attraverso l'intera catena del sistema sociale.

Questo solleva una questione più ampia sulla fiducia epistemica nelle società contemporanee. Viviamo in un'epoca in cui la verifica diretta delle fonti è sempre più difficile per il cittadino medio. Ci affidiamo necessariamente a intermediari considerati affidabili: università, giornali, società di consulenza, istituzioni governative. Quando questi intermediari iniziano a delegare la produzione di conoscenza a sistemi che inventano con disinvoltura, senza implementare rigorosi protocolli di verifica, l'intera catena della fiducia si incrina. Non è paranoia complottista notare che se Deloitte può consegnare al governo australiano un report pieno di citazioni fantasma, altri documenti simili potrebbero circolare non scoperti in altri contesti.

Il [Washington Post ha documentato](https://www.washingtonpost.com/nation/2025/06/03/attorneys-court-ai-hallucinations-judges/) come i giudici americani stiano reagendo con crescente severità a questi casi, comminando sanzioni sempre più pesanti. Ma le multe agli avvocati negligenti sono solo cerotti su una ferita più profonda. Il problema strutturale è che stiamo attraversando una fase di transizione tecnologica senza aver ancora sviluppato i protocolli culturali e professionali adeguati. È come se la stampa a caratteri mobili fosse stata inventata ma le convenzioni editoriali moderne, dalla revisione tra pari alla bibliografia verificata, non esistessero ancora.

## La lezione metodologica: strumenti, non oracoli

La tecnologia dell'intelligenza artificiale generativa è straordinariamente potente. GPT-4o e i suoi contemporanei possono accelerare drammaticamente processi creativi, sintetizzare vasti corpus di informazioni, suggerire connessioni non ovvie, generare molteplici varianti di un testo in secondi. Usati correttamente, sono moltiplicatori di capacità umane. Il problema emerge quando vengono trattati come sostituti del giudizio umano anziché come amplificatori.

La distinzione è cruciale. Un amplificatore potenzia quello che già esiste: se hai competenza, l'IA può renderti più veloce ed efficiente. Ma se deleghi completamente la tua competenza allo strumento, l'amplificazione funziona anche per la negligenza e l'errore. È il paradosso del feedback positivo: l'IA ti permette di produrre di più, ma se "di più" include contenuti non verificati, stai semplicemente moltiplicando la quantità di spazzatura che circola nel sistema.

Nel contesto specifico della consulenza professionale, questo significa reimpostare completamente il workflow. Quando Deloitte [ha ammesso l'accaduto](https://www.linnify.com/resources/the-deloitte-ai-report-scandal-why-ai-evaluation-is-no-longer-optional), ha anche annunciato l'implementazione di protocolli più stringenti per l'uso dell'IA. Ma quali dovrebbero essere questi protocolli? La risposta emerge chiaramente dal fallimento stesso: ogni citazione generata dall'IA deve essere verificata manualmente. Ogni claim fattuale deve essere controllato contro fonti primarie. Ogni argomentazione deve essere tracciabile a documenti realmente esistenti. In sostanza, l'output dell'IA deve essere trattato come una bozza di qualità variabile prodotta da uno stagista brillante ma inaffidabile, non come un prodotto finito.

Questo approccio richiede tempo e attenzione, esattamente le risorse che l'IA dovrebbe risparmiare. Ma è l'unico modo per evitare disastri. Il guadagno di efficienza dell'IA non può venire dal saltare la fase di verifica, ma dall'accelerare la fase di generazione della prima bozza. Se Deloitte avesse impiegato GPT-4o per produrre rapidamente una struttura iniziale e poi avesse dedicato il tempo risparmiato a una verifica meticolosa, il risultato sarebbe stato sia efficiente che accurato. Invece, il tempo è stato "risparmiato" sulla parte sbagliata del processo, quella che separa il professionismo dalla negligenza.

## Verso una cultura della responsabilità algoritmica

Il caso Deloitte arriva in un momento particolare nella storia dell'intelligenza artificiale. Siamo oltre l'entusiasmo acritico degli early adopters ma non ancora approdati a una maturità diffusa nell'uso di questi strumenti. Le organizzazioni si trovano in una terra di mezzo scomoda: la pressione competitiva impone di adottare l'IA per non restare indietro, ma le competenze per usarla responsabilmente sono distribuite in modo diseguale, spesso concentrate in pochi specialisti mentre la massa degli utenti tratta i LLM come motori di ricerca infallibili.

Serve una rivoluzione culturale più che tecnologica. Le aziende devono smettere di considerare l'IA come un vantaggio competitivo da implementare in fretta e iniziare a vederla come una tecnologia dual-use che richiede governance rigorosa. Questo significa formazione seria, non tutorial di tre ore. Significa audit regolari dei processi in cui l'IA è coinvolta. Significa, soprattutto, premiare la cautela piuttosto che la velocità, valorizzare chi solleva dubbi metodologici piuttosto che chi consegna rapidamente.

Il mondo accademico e professionale sta reagendo. Sempre più riviste scientifiche richiedono agli autori di dichiarare esplicitamente l'uso di IA e molte hanno proibito del tutto l'uso di LLM per generare sezioni sostanziali degli articoli. Tribunali in diversi paesi stanno introducendo regole che richiedono la certificazione dell'accuratezza delle citazioni negli atti processuali, con responsabilità personale per gli avvocati. Queste sono risposte necessarie ma ancora frammentarie.

Servono standard industriali trasversali. Se un documento professionale contiene contenuti generati dall'IA, questo dovrebbe essere dichiarato chiaramente, con indicazione delle sezioni coinvolte e dei protocolli di verifica applicati. Non per stigmatizzare l'uso della tecnologia, ma per rendere trasparente il processo di produzione della conoscenza. È lo stesso principio che ci fa richiedere la lista degli ingredienti sugli alimenti o la metodologia negli studi scientifici: non perché assumiamo malafede, ma perché la trasparenza permette valutazione e fiducia informata.

## Né apocalisse né utopia: l'equilibrio necessario

Sarebbe facile, guardando il caso Deloitte e i suoi fratelli minori nel mondo legale, scivolare in due retoriche opposte e ugualmente controproducenti. La prima è l'apocalisse: l'IA ci inonderà di falsità, renderà impossibile distinguere il vero dal falso, distruggerà la fiducia epistemica su cui si reggono le società complesse. La seconda è l'utopia: sono solo incidenti di percorso, l'errore è umano (o in questo caso, dell'utente umano), la tecnologia continuerà a migliorare e questi problemi scompariranno da soli.

Entrambe le narrative sono insufficienti. L'IA generativa non sta portando l'apocalisse della verità, ma sta certamente rendendo più facile produrre contenuti plausibili ma falsi su scala industriale. Ignorare questo rischio è irresponsabile. D'altra parte, i problemi non si risolveranno magicamente con la prossima generazione di modelli. Anche se GPT-5 o Claude-4 avranno tassi di allucinazione più bassi, non saranno mai perfetti. Il punto non è aspettare l'IA infallibile, ma sviluppare pratiche di lavoro che assumano l'imperfezione come punto di partenza.

Christopher Rudge, il ricercatore che ha scoperchiato il caso, non è un luddista anti-tecnologia. È uno studioso che ha semplicemente fatto quello che dovrebbe fare chiunque confronti un documento professionale: ha verificato le fonti. Il suo gesto non è eroico, è banalmente professionale. Il fatto che sia stato necessario, che il sistema di quality control di una Big Four non abbia intercettato quelle citazioni fantasma, è il vero scandalo. E la lezione è limpida: non importa quanto sofisticata diventi l'IA, l'ultima linea di difesa contro l'errore è sempre l'attenzione critica umana.

I 439.000 dollari australiani rimborsati da Deloitte sono poco più che una nota a piè di pagina in un bilancio da miliardi. Il vero costo di questo caso è reputazionale, ma potrebbe trasformarsi in un beneficio sistemico se viene letto correttamente: non come l'incidente imbarazzante di un'azienda, ma come il sintomo di un problema diffuso che richiede una risposta collettiva. Gli strumenti sono neutrali. È l'uso che ne facciamo a definire se amplificano la nostra intelligenza o la nostra negligenza. E questa scelta rimane, e deve rimanere, ostinatamente umana.