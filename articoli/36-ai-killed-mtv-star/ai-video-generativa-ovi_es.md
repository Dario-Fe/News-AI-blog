---
tags: ["Generative AI", "Business", "Startups"]
date: 2025-10-17
author: "Dario Ferrero"
---

# La explosión del vídeo generativo con IA. Entre el bombo de las grandes y la sombra del código abierto. Ovi y los otros rebeldes.
![ai-video-gen.jpg](ai-video-gen.jpg)


*El 2024 fue para el vídeo generativo con inteligencia artificial lo que el 1991 fue para el grunge: la explosión repentina de algo que se venía gestando desde hacía tiempo. Si en febrero de 2024 OpenAI lanzaba la bomba con Sora 1, mostrando vídeos generados que hacían gritar al milagro tecnológico, a distancia de apenas un año el panorama se ha multiplicado de forma exponencial.*

Google respondió con Veo, Meta sacó de la chistera a Movie Gen, y decenas de laboratorios universitarios y startups comenzaron a publicar sus propios modelos con una frecuencia febril. Como en aquella explosión musical de Seattle, donde junto a Nirvana surgían Soundgarden, Pearl Jam y Alice in Chains, hoy junto a los colosos tecnológicos emergen proyectos de código abierto que prometen democratizar una tecnología todavía en gran parte encerrada en los centros de datos de las grandes tecnológicas.

La cronología de la aceleración es vertiginosa. Desde que Sora 1 mostró al mundo que era posible generar vídeos fotorrealistas a partir de simples indicaciones de texto, el sector ha vivido una carrera desenfrenada. En septiembre de 2024 Google lanzó Veo 2, apostando por un estilo cinematográfico y una calidad visual impresionante. Meta, para no quedarse atrás, presentó Movie Gen en octubre de 2024, un sistema capaz de producir vídeos de hasta 16 segundos con audio musical y efectos de sonido sincronizados. Y luego, en septiembre de 2025, llegó Sora 2, con la tan esperada adición de audio sincronizado, incluyendo diálogos y efectos de sonido. Mientras tanto, el mundo del código abierto no se ha quedado de brazos cruzados: proyectos como [HunyuanVideo de Tencent](https://github.com/Tencent/HunyuanVideo), Mochi 1, Open-Sora 1.3 y ahora Ovi han comenzado a ofrecer alternativas concretas, aunque con enfoques y resultados muy diferentes.

Sin embargo, detrás del marketing brillante y los vídeos de demostración impresionantes, la realidad es más compleja y menos democrática de lo que los comunicados de prensa quieren hacernos creer. Los modelos de los gigantes tecnológicos siguen siendo en gran parte inaccesibles, cerrados detrás de listas de espera interminables, suscripciones premium o simplemente no disponibles para el público. Y aquí entra en juego la cuestión central: ¿el bombo generado por las grandes empresas es proporcional a los resultados realmente accesibles para los usuarios? ¿O estamos ante otra operación de marketing que promete revoluciones mientras entrega accesos limitados y modelos propietarios imposibles de modificar o estudiar?

## Los gigantes y sus candados de oro

Sora 2, presentado por OpenAI con gran fanfarria mediática en septiembre de 2025, representa sobre el papel un salto evolutivo significativo. El modelo genera vídeos de hasta 20 segundos con [audio, diálogos y efectos de sonido sincronizados](https://openai.com/it-IT/index/sora-2/), prometiendo una calidad cinematográfica y una coherencia narrativa impresionante. Los vídeos de demostración muestran escenas complejas, con movimientos de cámara fluidos y un realismo que roza el fotorrealismo. Sin embargo, como suele ocurrir en el mundo de OpenAI, el acceso es cualquier cosa menos abierto. Sora 2 solo está disponible para los suscriptores de ChatGPT Plus y Pro, con costes que parten de los 20 dólares al mes y pueden aumentar considerablemente para quienes quieran generar más de unos pocos clips al mes. No existe una API pública estable, ninguna posibilidad de descargar los pesos del modelo, ninguna documentación técnica que permita entender realmente cómo funciona el sistema. Es como si Led Zeppelin hubiera publicado "Stairway to Heaven" pero solo en una gramola accesible pagando una cuota mensual, sin lanzar nunca el disco.

Google, por su parte, ha apostado todo por [Veo 3](https://aistudio.google.com/models/veo-3), un modelo que enfatiza el estilo cinematográfico y la calidad visual de nivel profesional. Veo 3 genera vídeos de hasta 2 minutos, admite altas resoluciones e incluye audio musical y ambiental sincronizado. También en este caso, sin embargo, el acceso está reservado a los usuarios premium de Google One AI Premium, con costes que parten de los 19,99 dólares al mes. La plataforma VideoFX, donde se implementa Veo 3, ofrece una interfaz fácil de usar, pero sigue siendo un sistema completamente cerrado: no hay código disponible, no hay posibilidad de ajuste fino, no hay transparencia sobre los datos de entrenamiento. Es el clásico enfoque de jardín amurallado: funciona bien, tiene una estética impecable, pero estás completamente a merced de las decisiones de Mountain View.

Meta ha elegido un camino aparentemente más abierto con Movie Gen y la plataforma [Vibes](https://www.meta.ai/vibes/), integrada en la aplicación Meta AI. Movie Gen genera vídeos de hasta 16 segundos con música y efectos de sonido, y Vibes permite a los usuarios crear, remezclar y compartir clips cortos en una experiencia social y creativa. Por el momento, el servicio es gratuito, lo que lo convierte en el más accesible entre los modelos de los gigantes. Pero aquí surge la pregunta crucial, bien destacada por un [análisis crítico de Facta](https://www.facta.news/articoli/vibes-meta-feed-intelligenza-artificiale): ¿hasta cuándo seguirá siendo gratuito? La estrategia de Meta es clara: crear dependencia del ecosistema, recopilar datos de los usuarios en cantidades industriales y luego, eventualmente, monetizar cuando la base de usuarios sea lo suficientemente grande y esté vinculada. Movie Gen no está disponible para su descarga, no existe una versión autoalojada y la documentación técnica es vaga y parcial. Es el modelo freemium llevado al extremo: gratis hoy, pero mañana quién sabe.

El punto es que todos estos modelos, por muy impresionantes que sean técnicamente, comparten la misma limitación estructural: son sistemas propietarios, inaccesibles en su funcionamiento interno y modificables solo en los parámetros superficiales que las empresas deciden exponer. No puedes estudiarlos, no puedes adaptarlos a tus necesidades específicas, no puedes verificar cómo fueron entrenados ni qué datos han visto. Y, sobre todo, no puedes garantizar que seguirán existiendo o siendo accesibles dentro de seis meses o un año. Cuando confías en un modelo de código cerrado, de hecho estás alquilando tecnología, no poseyéndola. Y el alquiler puede subir, el contrato puede cambiar, el servicio puede ser descontinuado. Ha sucedido con las API de Google, con los servicios de Amazon, con las plataformas de Microsoft. ¿Por qué debería ser diferente con los modelos generativos?
![sora2.jpg](sora2.jpg)
[Imagen del tráiler de Sora 2](https://openai.com/it-IT/index/sora-2/)

## Ovi: banda de garaje contra gran discográfica

Y aquí es donde entra en escena [Ovi](https://github.com/character-ai/Ovi), un proyecto desarrollado por Character.AI y lanzado como completamente de código abierto en septiembre de 2025. Para continuar con la metáfora musical, si Sora y Veo son las producciones millonarias de las grandes discográficas, con estudios de grabación de última generación y presupuestos ilimitados, Ovi es la banda que graba su EP en el garaje, con equipo ensamblado y mucha pasión. Pero como suele ocurrir en la historia de la música, es precisamente de esos garajes de donde salen los discos más interesantes e innovadores.

Ovi se basa en una arquitectura que sus creadores definen como [fusión transmodal de doble columna vertebral](https://arxiv.org/abs/2510.01284), un enfoque que suena complicado pero es conceptualmente elegante. En lugar de generar primero el vídeo y luego añadir el audio en postproducción, o viceversa, Ovi modela las dos modalidades como un único proceso generativo. El sistema utiliza dos módulos DiT (Diffusion Transformer) idénticos, uno para vídeo y otro para audio, que se entrenan conjuntamente mediante mecanismos de atención cruzada bidireccional. En la práctica, los dos módulos se comunican constantemente durante el proceso de generación, intercambiando información temporal y semántica. Esto permite una sincronización natural entre imágenes y sonidos, sin necesidad de pipelines separadas o alineaciones a posteriori.

La documentación técnica publicada en [arXiv](https://arxiv.org/abs/2510.01284) es transparente y detallada. El entrenamiento se articula en dos fases: primero se inicializa una torre de audio que refleja la arquitectura de un modelo de vídeo preentrenado, y se entrena desde cero con cientos de miles de horas de audio en bruto. En esta fase, el modelo aprende a generar efectos de sonido realistas y habla con una rica identidad y emoción. En la segunda fase, las dos torres se entrenan conjuntamente en un vasto corpus de vídeo, intercambiando la sincronización a través de incrustaciones RoPE escaladas y la semántica a través de la atención cruzada bidireccional. El resultado es un modelo capaz de generar simultáneamente vídeo y audio sincronizado, incluyendo diálogos, efectos de sonido y música de fondo.

Ahora, hay que decirlo claramente: Ovi no supera a Sora 2 o Veo 3 en calidad visual pura. Los vídeos generados se limitan a 5 segundos, frente a los 20 de Sora o los 120 de Veo. La resolución es inferior, la fluidez de los movimientos menos refinada y la capacidad de gestionar escenas complejas con muchos elementos en movimiento es todavía incipiente. Pero esta comparación, aunque inevitable, también es un poco engañosa. Es como comparar una producción de Hollywood con un cortometraje independiente: claro, la diferencia técnica es evidente, pero no necesariamente el segundo es menos interesante o útil que el primero.

El verdadero punto fuerte de Ovi no es la superioridad técnica, que honestamente no existe, sino la filosofía de código abierto que lo rige. El código está completamente disponible en [GitHub](https://github.com/character-ai/Ovi), los pesos del modelo se pueden descargar y la documentación es accesible y comprensible. Puedes estudiar cómo funciona el sistema, modificarlo, adaptarlo a tus necesidades e integrarlo en proyectos más grandes. Puedes hacer un ajuste fino en conjuntos de datos específicos, experimentar con diferentes arquitecturas y contribuir a la comunidad con mejoras y correcciones. Y, sobre todo, puedes hacerlo localmente, en tu propio hardware, sin depender de servidores remotos, API que pueden cambiar o desactivarse, o políticas de uso que cambian de un día para otro.

Por supuesto, los requisitos de hardware no son triviales. Para ejecutar Ovi con fluidez se necesita al menos una GPU de gama alta, con 24 GB de VRAM o más, y una buena cantidad de RAM del sistema. No es exactamente algo que puedas ejecutar en tu portátil mientras viajas en tren. Pero para una pequeña empresa, un estudio creativo, un laboratorio universitario o incluso un aficionado con un presupuesto dedicado, es absolutamente factible. Estamos hablando de unos pocos miles de euros de hardware, frente a cientos de euros al mes de suscripciones a servicios de código cerrado que podrían cambiar de condiciones o desaparecer mañana.

Y hay otro aspecto a menudo subestimado: la posibilidad de verificar qué ha aprendido el modelo y cómo lo utiliza. Con los modelos de código cerrado, estás completamente a oscuras sobre los datos de entrenamiento. ¿Incluyeron material protegido por derechos de autor? ¿Usaron vídeos sin el consentimiento de los creadores? ¿Introdujeron sesgos problemáticos? No puedes saberlo. Con Ovi, al menos en teoría, puedes analizar el código, estudiar las decisiones arquitectónicas y tener una comprensión más profunda de lo que realmente está sucediendo bajo el capó. No es solo una cuestión de transparencia ética, sino también de control técnico y capacidad de depuración.
![ovi.jpg](ovi.jpg)
[Imagen del tráiler de Ovi](https://github.com/character-ai/Ovi)

## Los otros "rebeldes" del vídeo de código abierto

Ovi no está solo en su batalla por democratizar el vídeo generativo con IA. A su alrededor se ha formado un ecosistema variado de proyectos de código abierto, cada uno con sus propios enfoques, puntos fuertes y limitaciones. Es un poco como la escena del hardcore punk de los ochenta: muchos pequeños sellos independientes, muchas bandas tocando en sótanos, pocos recursos pero mucha determinación.

[HunyuanVideo](https://github.com/Tencent/HunyuanVideo), desarrollado por Tencent, es quizás el proyecto más ambicioso de este panorama. Su objetivo es generar vídeos de hasta 10 segundos con una calidad visual que se acerca a los modelos comerciales, y admite altas resoluciones. La arquitectura se basa en transformadores de difusión, similar a la de Sora, y el modelo se entrena en un enorme conjunto de datos de vídeos chinos e internacionales. Su punto fuerte es la fluidez de los movimientos y la coherencia temporal, pero el audio sigue ausente. Y aquí se ve la diferencia entre un proyecto respaldado por una corporación como Tencent y las iniciativas más pequeñas: los recursos están ahí, los resultados también, pero la accesibilidad está limitada por requisitos de hardware prohibitivos y una configuración compleja.

[Mochi 1](https://mochi1ai.com/it), por otro lado, es un proyecto más experimental, desarrollado por la comunidad y centrado en la animación de imágenes estáticas. La idea es tomar una imagen, quizás generada con Stable Diffusion o DALL-E, y animarla con movimientos realistas. Es particularmente popular entre los artistas digitales que quieren dar vida a sus obras de arte sin tener que aprender software de animación tradicional. La calidad es variable, pero el potencial creativo es notable. También aquí, sin embargo, el audio está completamente ausente, y la duración máxima es de 3-4 segundos.

[Open-Sora 1.3](https://github.com/hpcaitech/Open-Sora) es un intento de la comunidad de replicar la arquitectura del Sora original basándose en la información pública disponible. Al no tener acceso al código de OpenAI, los desarrolladores tuvieron que hacer ingeniería inversa de las descripciones técnicas y los artículos relacionados, creando una arquitectura que en teoría debería funcionar de manera similar. Los resultados son interesantes pero todavía están lejos de la calidad de Sora 1, y mucho menos de Sora 2. La fluidez a menudo se ve interrumpida por artefactos, la coherencia temporal es frágil y la gestión de escenas complejas es problemática. Pero es un proyecto vivo, con una comunidad activa que sigue mejorando el código.

[AnimateDiff](https://github.com/guoyww/AnimateDiff) merece una mención especial porque tiene un enfoque completamente diferente. En lugar de ser un modelo independiente, es una extensión de Stable Diffusion que añade capacidades de animación. Instalas AnimateDiff, lo conectas a tu configuración de Stable Diffusion y puedes convertir tus generaciones en animaciones cortas. Es popular entre quienes ya usan Stable Diffusion para el arte generativo, porque permite integrar la animación en el flujo de trabajo existente sin tener que aprender un nuevo sistema desde cero. Pero también aquí, sin audio y con duraciones muy cortas.

[CogVideoX](https://github.com/zai-org/CogVideo) de la Universidad de Tsinghua ciertamente merece una mención. Desarrollado por el laboratorio THUDM y actualizado en noviembre de 2024 con la versión 1.5, es uno de los proyectos de código abierto más maduros del panorama. CogVideoX-5B genera vídeos de hasta 10 segundos con una resolución de 720x480, y la versión 1.5 introduce soporte para la conversión de imagen a vídeo en cualquier resolución. La arquitectura se basa en transformadores de difusión con un transformador experto que gestiona mejor los movimientos complejos que los modelos anteriores. Es especialmente apreciado en la comunidad por la calidad de su coherencia temporal y por haber superado en los benchmarks a competidores como VideoCrafter-2.0 y Open-Sora. El código está completamente disponible en GitHub y Hugging Face, con documentación detallada. La única limitación sigue siendo la ausencia de audio, pero para quienes buscan una generación de vídeo pura con buena calidad y duraciones decentes, CogVideoX es una de las opciones más sólidas.

[LTX-Video](https://huggingface.co/Lightricks/LTX-Video) de Lightricks, lanzado en noviembre de 2024 con una promesa ambiciosa: generación de vídeo en tiempo real. Con 2.000 millones de parámetros en la versión inicial (y 13.000 millones en la versión lanzada en mayo de 2025), LTX-Video es el primer modelo DiT capaz de generar vídeo a 30 FPS con una resolución de 1216x704 más rápido de lo que se tarda en verlos. Lightricks afirma que es 30 veces más rápido que los modelos comparables, lo que lo hace especialmente interesante para aplicaciones que requieren iteraciones rápidas. Es completamente de código abierto, está integrado en ComfyUI para quienes ya usan ese flujo de trabajo y tiene una comunidad activa que está contribuyendo con mejoras en la consistencia del movimiento y la coherencia de las escenas. También aquí, sin audio, pero la velocidad de generación es una ventaja competitiva considerable para quienes hacen prototipos o trabajos creativos iterativos.

El panorama que emerge es claro: el ecosistema de código abierto es vibrante, creativo y está en rápida evolución. Pero también está fragmentado, a menudo con recursos insuficientes y todavía bastante por detrás de los gigantes comerciales en términos de calidad absoluta. La mayoría de estos proyectos no incluyen audio, y cuando lo hacen, la calidad es inferior. Las duraciones son cortas, los requisitos de hardware exigentes y la configuración compleja. No son soluciones listas para usar, sino herramientas que requieren conocimientos técnicos, paciencia y ganas de experimentar.

## ¿Democracia o ilusión?

Llegamos así al meollo de la cuestión: ¿es el código abierto en el campo del vídeo generativo con IA una verdadera alternativa democrática, o es solo una ilusión para los manitas con demasiadas GPU y demasiado tiempo libre? La respuesta, como suele ocurrir, es matizada y depende de quién seas y qué quieras hacer.

Si eres una empresa que necesita producir contenido de vídeo de alta calidad para campañas de marketing, los modelos comerciales como Sora 2 o Veo 3 probablemente sigan siendo la mejor opción. La calidad es superior, la interfaz es fácil de usar y existe soporte técnico. Pagas más, por supuesto, pero obtienes resultados inmediatos sin tener que gestionar infraestructuras complejas.

Pero si eres un investigador, un desarrollador, un artista que quiere experimentar, o una pequeña entidad con conocimientos técnicos pero un presupuesto limitado, entonces proyectos como Ovi se vuelven invaluables. Te ofrecen libertad, control y la posibilidad de construir algo personalizado. Puedes integrar el modelo en pipelines creativas más amplias, adaptarlo a necesidades específicas y no depender de decisiones corporativas que escapan a tu control.

La verdadera democratización, sin embargo, requiere algo más que simple código abierto. Requiere una documentación clara, comunidades activas, recursos educativos y una reducción progresiva de los requisitos de hardware. Es necesario que estos proyectos se vuelvan más accesibles, que la instalación sea más sencilla y que los tutoriales sean comprensibles incluso para quienes no son expertos en aprendizaje automático. Y se necesita sostenibilidad económica: muchos proyectos de código abierto en el campo de la IA son desarrollados por pequeños equipos o incluso por individuos que trabajan en su tiempo libre, sin financiación estable. ¿Cuánto pueden durar? ¿Cómo pueden competir con los laboratorios de investigación de los gigantes tecnológicos que tienen presupuestos de millones de dólares?

El futuro próximo será probablemente un escenario híbrido. Los modelos comerciales seguirán dominando en términos de calidad absoluta y facilidad de uso, pero los proyectos de código abierto colonizarán nichos específicos: investigación académica, aplicaciones artísticas experimentales, integraciones personalizadas, casos de uso donde el control y la transparencia importan más que la perfección visual. Ovi y sus compañeros de viaje no reemplazarán a Sora o Veo, pero ofrecerán una alternativa concreta para quienes quieran o necesiten esa alternativa.

Y quizás, al igual que ocurrió con el punk y el rock independiente, de estas bandas de garaje surgirán las ideas y las innovaciones que mañana los gigantes corporativos absorberán en sus productos principales. La historia de la tecnología está llena de ejemplos de proyectos de código abierto que anticiparon tendencias adoptadas posteriormente por la industria. Linux, Python, el propio TensorFlow. No sería la primera vez que el garaje vence al estudio de grabación millonario. No en términos de presupuesto o brillo, sino de ideas, libertad y capacidad para cambiar las reglas del juego.