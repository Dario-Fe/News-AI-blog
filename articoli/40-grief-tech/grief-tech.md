---
tags: ["Ethics & Society", "Business", "Generative AI"]
date: 2025-10-27
author: "Dario Ferrero"
youtube_url: "https://youtu.be/Efd9TcT9AHM?si=1islhxFwr4xitLkq"
---

# Chatbot dell'aldil√†: La grief tech, tra dolore e business
![tomba-bancomat.jpg](tomba-bancomat.jpg)


*Il 28 novembre 2015, a Mosca, una Jeep lanciata a folle velocit√† investe Roman Mazurenko. Ha appena 34 anni, √® un imprenditore tech e una figura leggendaria nei circoli culturali della citt√†. Eugenia Kuyda arriva in ospedale poco prima che l'amico muoia, perdendo per pochi istanti l'occasione di parlargli un'ultima volta. Nei tre mesi successivi, [Kuyda raccoglie migliaia di messaggi](https://thereader.mitpress.mit.edu/chatting-with-the-dead-chatbots/) che Roman aveva scambiato con amici e familiari‚Äîcirca 8.000 righe di testo che catturavano il suo modo unico di esprimersi, le sue frasi idiosincratiche, persino i vezzi linguistici dovuti a una lieve dislessia.*

Eugenia √® lei stessa imprenditrice e sviluppatrice di software. La sua azienda, Luka, si occupa di chatbot e intelligenza artificiale conversazionale. E cos√¨, invece di limitarsi a rileggere ossessivamente quei messaggi come farebbe chiunque altro, decide di fare qualcosa che suona inquietantemente familiare a chi ha visto l'episodio "Be Right Back" di Black Mirror: [li alimenta a un algoritmo per creare un bot che simuli Roman](https://i-d.co/article/black-mirror-artificial-intelligence-roman-mazurenko/).

All'inizio √® solo un archivio sofisticato, una sorta di motore di ricerca che rispesca testi esistenti in base all'argomento della conversazione. Ma con l'evoluzione dei modelli di AI generativa, quel semplice database si trasforma in qualcosa di pi√π perturbante: un vero chatbot capace di generare risposte nuove, interpretando lo stile e i pensieri di Roman. Gli amici che provano il bot trovano [la somiglianza inquietante](https://www.cbc.ca/documentaries/the-nature-of-things/after-her-best-friend-died-this-programmer-created-an-ai-chatbot-from-his-texts-to-talk-to-him-again-1.6252286). Molti lo usano per dirgli cose che non hanno fatto in tempo a esprimere quando era vivo. Come racconta lo stesso Kuyda in un'intervista, quei messaggi erano tutti "sull'amore, o per dirgli qualcosa che non avevano avuto il tempo di dirgli". Da quel lutto privato, trasformato in esperimento tecnologico, nascer√† Replika, un'app che oggi conta oltre 10 milioni di utenti e che permette a chiunque di creare un compagno AI che impara a replicare la propria personalit√†‚Äîo quella di qualcun altro.

## L'industria digitale dell'aldil√†

La storia di Roman Mazurenko non √® pi√π un'eccezione. √à il prototipo di un'intera industria emergente che [i ricercatori dell'Universit√† di Cambridge Tomasz Hollanek e Katarzyna Nowaczyk-Basi≈Ñska](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) hanno battezzato "digital afterlife industry"‚Äîl'industria dell'aldil√† digitale. I termini per descrivere questi strumenti si moltiplicano: griefbot, deadbot, ghostbot, thanabots. Tutti convergono verso lo stesso concetto: chatbot basati sull'impronta digitale dei defunti che permettono ai vivi di continuare a "parlare" con chi hanno perso.

Le piattaforme proliferano con una velocit√† che ricorda pi√π il mercato degli NFT che quello dei servizi di supporto psicologico. [Project December](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates) offre "conversazioni con i morti" a 10 dollari per 500 scambi di messaggi. Seance AI propone versioni gratuite testuali e versioni vocali a pagamento. HereAfter AI permette di pre-registrare il proprio chatbot‚Äîuna sorta di testamento digitale parlante. [In Cina, si possono creare avatar dei propri cari defunti per appena 3 dollari](https://www.vml.com/insight/grief-tech), usando solo 30 secondi di materiale audiovisivo. L'azienda SenseTime √® arrivata persino a creare un avatar del suo fondatore Tang Xiao'ou, deceduto nel dicembre 2023, che ha tenuto un discorso all'assemblea generale dei soci nel marzo 2024.

You, Only Virtual (YOV) si spinge oltre, dichiarando audacemente che la sua tecnologia potrebbe "eliminare completamente il lutto". Il mercato AI dedicato alla companionship, che include ma non si limita ai griefbot, [√® stato valutato 2,8 miliardi di dollari nel 2024](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates) e si prevede raggiunger√† i 9,5 miliardi entro il 2028. Come negli scenari pi√π distopici di "Upload" o "San Junipero"‚Äîper citare qualcosa di meno mainstream di Black Mirror‚Äîstiamo assistendo alla commercializzazione sistematica dell'immortalit√† digitale.

## Il lutto congelato

Ma cosa ne pensano gli psicologi? Le voci degli esperti sono unanimi nell'esprimere cautela, anche se non necessariamente condanna. La questione centrale ruota attorno a un concetto fondamentale: il processo naturale di elaborazione del lutto. [Dr. Sarika Boora](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), psicologa e direttrice di Psyche and Beyond a Delhi, avverte che la grief tech "pu√≤ ritardare il processo di elaborazione del lutto" mantenendo le persone in uno stato di negazione prolungata.

Il problema, secondo [uno studio interdisciplinare pubblicato su Frontiers in Human Dynamics](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full), √® che tradizionalmente l'elaborazione del lutto implica l'accettazione dell'assenza della persona amata, permettendo agli individui di processare le emozioni e muoversi verso la guarigione. La neuroplasticit√†‚Äîla capacit√† del cervello di adattarsi‚Äîgioca un ruolo critico nell'integrare la perdita, permettendo alle persone di ricostruire le proprie vite nel tempo. Interagire con un griefbot rischia di interrompere questa progressione naturale, creando un'illusione di presenza continuata che potrebbe impedire di confrontarsi pienamente con la realt√† della perdita.

[NaYeon Yang e Greta J. Khanna](https://journals.sagepub.com/doi/10.1177/00110000251352568), nel loro articolo "AI and Technology in Grief Support: Clinical Implications and Ethical Considerations", sottolineano come queste tecnologie possano trasformare il lutto in un loop‚Äîdove il dolore non si risolve mai, ma muta in dipendenza. Gli utenti potrebbero ritardare indefinitamente l'accettazione, continuando a rivolgersi a fantasmi digitali nella speranza di una chiusura che non arriver√† mai. L'illusione di presenza estende il limbo emotivo piuttosto che risolverlo.

Eppure la questione non √® cos√¨ univoca. [Uno studio recente sul blog della University of Alabama Institute for Human Rights](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/) nota che alcuni chatbot potrebbero effettivamente aiutare le persone ad affrontare il lutto traumatico, la perdita ambigua o l'inibizione emotiva‚Äîma il loro uso deve essere contestualizzato e limitato nel tempo. La raccomandazione della Dr. Boora √® chiara: "Un modo sano di utilizzare la grief tech √® dopo aver elaborato il lutto‚Äîdopo aver raggiunto l'accettazione e essere tornati al proprio stato funzionale normale". In altre parole, questi strumenti potrebbero avere un valore come supporto alla memoria, ma solo dopo che il processo di guarigione √® gi√† in corso, non come sostituto del lutto stesso.
![chatbot-roman.jpg](chatbot-roman.jpg)
[Immagine tratta da repubblica.it](https://www.repubblica.it/tecnologia/2016/10/10/news/_parla_con_lui_roman_muore_in_un_incidente_eugenia_maga_del_software_usa_i_loro_dialoghi_per_creare_un_suo_alter_eg-149440494/)

## Il fantasma nella macchina

Joseph Weizenbaum, scienziato informatico del MIT, scopr√¨ gi√† nel 1966 qualcosa di inquietante con ELIZA, il suo chatbot rudimentale basato su semplici pattern di risposta. Gli utenti, pur sapendo di interagire con un programma primitivo, gli attribuivano intelligenza ed emozioni, confidandosi con la macchina come se fosse un terapeuta reale. [La conclusione di Weizenbaum](https://www.calcalistech.com/ctechnews/article/hycchvgjge)‚Äîche i bot possono indurre "pensiero delirante in persone perfettamente normali"‚Äîrimane fondamentale negli studi sull'interazione uomo-computer. √à quello che oggi chiamiamo "effetto ELIZA", e con i moderni large language model si √® amplificato esponenzialmente.

Il problema √® che questi chatbot raggiungono un'accuratezza di circa il 70%‚Äîsufficientemente alta da sembrare convincenti, ma abbastanza bassa da produrre quello che gli esperti chiamano "artefatti": frasi non caratteristiche, allucinazioni, linguaggio filler, clich√®. [Come notano Hollanek e Nowaczyk-Basi≈Ñska](https://link.springer.com/chapter/10.1007/978-3-031-90723-4_40), gli algoritmi potrebbero essere progettati per ottimizzare le interazioni, massimizzando il tempo che una persona in lutto trascorre col chatbot, assicurandosi abbonamenti a lungo termine. Questi algoritmi potrebbero persino modificare sottilmente la personalit√† del bot nel tempo per renderlo pi√π piacevole, creando una caricatura accattivante piuttosto che un riflesso accurato del defunto.

In un articolo pubblicato su Philosophy & Technology, i due ricercatori di Cambridge presentano scenari speculativi che illustrano i pericoli concreti. In uno, una societ√† fittizia chiamata "MaNana" permette di simulare la nonna deceduta senza il consenso del "donatore di dati". L'utente, inizialmente impressionato e confortato, inizia a ricevere pubblicit√† una volta terminato il "trial premium". Immaginate di chiedere alla nonna digitale una ricetta e di ricevere, insieme ai suoi consigli culinari, suggerimenti sponsorizzati per ordinare carbonara da Uber Eats. [Questo non √® fantascienza](https://aiconnectnetwork.com/ai-griefbots-mental-health-apps-data-privacy/)‚Äî√® gi√† successo in versioni beta di alcuni servizi.

## Chi possiede il morto?

Le questioni etiche si moltiplicano come teste dell'idra. Chi ha il diritto di creare e controllare questi avatar digitali? Come spiega [Hollanek in un comunicato stampa](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) dell'Universit√† di Cambridge, "√® vitale che i servizi di aldil√† digitale considerino i diritti e il consenso non solo di coloro che ricreano, ma di coloro che dovranno interagire con le simulazioni". Il problema del consenso opera su tre livelli distinti. Primo, il "donatore di dati"‚Äîla persona defunta‚Äîha mai acconsentito a essere ricreata? Secondo, chi possiede i dati dopo la morte? Terzo, cosa succede a chi non vuole interagire con questi simulacri ma si trova bombardato da messaggi?

[Uno degli scenari esplorati da Hollanek e Nowaczyk-Basi≈Ñska](https://scitechdaily.com/cambridge-experts-warn-ai-deadbots-could-digitally-haunt-loved-ones-from-beyond-the-grave/) racconta di un genitore anziano che sottoscrive segretamente un abbonamento ventennale a un deadbot di se stesso, sperando che conforti i figli adulti e permetta ai nipoti di conoscerlo. Dopo la morte, il servizio si attiva. Un figlio rifiuta di interagire e riceve una raffica di email nella voce del genitore morto‚Äîquello che i ricercatori chiamano "digital haunting", persecuzione digitale. L'altro figlio interagisce, ma finisce emotivamente esausto e tormentato dal senso di colpa sul destino del deadbot. Come si "ritira" un bot che simula tua madre? Come si uccide qualcuno che √® gi√† morto?

I ricercatori di Cambridge propongono quella che definiscono "cerimonie di pensionamento"‚Äîrituali digitali per disattivare i deadbot in modo dignitoso. Potrebbero essere funerali digitali, o altri tipi di cerimonia a seconda del contesto sociale. [Debra Bassett](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), consulente per l'aldil√† digitale, propone nei suoi studi un ordine DDNR‚Äî"digital do-not-reanimate"‚Äîuna clausola testamentaria che proibisca legalmente la resurrezione digitale posthuma non consensuale. La chiamata "zombi digitali", e il termine non potrebbe essere pi√π appropriato.

## Il mercato della vulnerabilit√†

La commercializzazione del lutto solleva interrogativi ancora pi√π profondi. A differenza dell'industria funeraria tradizionale‚Äîche monetizza la morte attraverso servizi una tantum come bare, cremazioni, cerimonie‚Äî[i griefbot operano su modelli di abbonamento o pagamento al minuto](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/). Le aziende hanno quindi incentivi finanziari a mantenere le persone in lutto costantemente coinvolte con i loro servizi. [Come nota uno studio sul mercato della grief tech](https://link.springer.com/chapter/10.1007/978-3-031-90723-4_40), questo modello economico √® fondamentalmente diverso‚Äîe potenzialmente pi√π predatorio‚Äîrispetto a quello dei servizi tradizionali legati alla morte.

Il mercato dell'AI companionship, che include chatbot romantici, terapeutici e di supporto al lutto, ha modelli di prezzo che vanno dai 10 ai 40 dollari al mese. Come fa notare [Ewan Morrison su Psychology Today](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates), stiamo monetizzando la solitudine e la vulnerabilit√† emotiva. L'effetto ELIZA si approfondisce, mascherando la frammentazione sociale con l'illusione della cura mentre ci allontana ulteriormente gli uni dagli altri.

[Paula Kiel della NYU-London](https://www.calcalistech.com/ctechnews/article/hycchvgjge) offre una prospettiva diversa: "Ci√≤ che rende cos√¨ allettante questa industria √® che, come ogni generazione, stiamo cercando modi per preservare parti di noi stessi. Stiamo trovando conforto nell'inevitabilit√† della morte attraverso il linguaggio della scienza e della tecnologia". Ma questo conforto ha un prezzo‚Äînon solo economico, ma psicologico e sociale.

## L'autenticit√† impossibile

C'√® poi la questione filosofica dell'autenticit√†. I chatbot non hanno la capacit√† di evolvere e crescere come gli esseri umani. Come spiega [un'analisi dell'Institute for Human Rights](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/) dell'Universit√† dell'Alabama, "un problema nell'eseguire azioni in perpetuo √® che le persone morte sono prodotti del loro tempo. Non cambiano ci√≤ che vogliono quando il mondo cambia". Anche se la crescita fosse implementata nell'algoritmo, non ci sarebbe garanzia che rifletta come una persona sarebbe effettivamente cambiata.

I griefbot preservano la presenza digitale di una persona defunta in modi che potrebbero diventare problematici o irrilevanti nel tempo. Se Milton Hershey, che nel suo testamento lasci√≤ disposizioni precise su come la sua eredit√† dovesse essere usata in perpetuo, fosse vivo oggi, modificherebbe quelle disposizioni per riflettere i cambiamenti del mondo? La differenza cruciale √® che i testamenti e le eredit√† sono intrinsecamente limitati nella portata e nella durata. I griefbot, per loro natura, hanno il potenziale di persistere indefinitamente, amplificando il danno potenziale alla reputazione o alla memoria di una persona.

La memoria umana √® gi√† un narratore inaffidabile‚Äîfluida, plasmata dall'emozione pi√π che dal fatto. [Studi recenti dimostrano](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots) che immagini e video modificati con AI possono impiantare falsi ricordi e distorcere quelli reali. Cosa succede quando alimentiamo il lutto in una macchina e riceviamo indietro una versione di una persona che non √® mai completamente esistita? Come scrisse la giornalista tech Vauhini Vara, finalista al Pulitzer che ha esplorato personalmente le profondit√† emotive ed etiche del lutto e dell'AI: "Ha senso che le persone si rivolgano a qualsiasi risorsa disponibile per cercare conforto, e ha anche senso che le aziende siano interessate a sfruttare le persone in un momento di vulnerabilit√†".
![lutto-cervello.jpg](lutto-cervello.jpg)
[Immagine tratta da frontiersin.org, il cervello in lutto pu√≤ essere suddiviso in quattro categorie a seconda che il profilo di attivit√† sia simile o diverso dalla tristezza e dalla depressione](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full)

## Verso una regolamentazione?

[Hollanek e Nowaczyk-Basi≈Ñska raccomandano](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) restrizioni di et√† per i deadbot e quella che chiamano "trasparenza significativa"‚Äîassicurarsi che gli utenti siano costantemente consapevoli di star interagendo con un'AI, con avvisi simili a quelli attuali sui contenuti che potrebbero causare crisi epilettiche. Suggeriscono anche di classificare i deadbot come dispositivi medici per affrontare questioni di salute mentale, specialmente per gruppi vulnerabili come i bambini.

[Uno studio recente su Frontiers in Human Dynamics](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full) sottolinea che gli sviluppatori di tecnologie legate al lutto hanno un'immensa responsabilit√† nel determinare come gli utenti si impegnano emotivamente con la presenza simulata. Mentre la continuit√† emotiva pu√≤ alleviare il disagio iniziale, l'iper-immersione rischia l'intrappolamento psicologico. Gli sviluppatori devono dare priorit√† a principi di design etici, come incorporare limiti di utilizzo, prompt riflessivi e checkpoint emotivi che guidino gli utenti verso il recupero piuttosto che verso la dipendenza.

La collaborazione con psicologi ed esperti di lutto dovrebbe informare le caratteristiche dell'interfaccia per garantire che facilitino, piuttosto che sostituire, il processo di elaborazione del lutto. Inoltre, la gestione di dati sensibili‚Äîvoce, personalit√†, pattern comportamentali‚Äîrichiede standard rigorosi di crittografia, privacy e protocolli di trasparenza. L'uso improprio di questi dati non solo viola la dignit√† digitale, ma pu√≤ anche contribuire a danni emotivi per gli utenti e le famiglie.

## Il professor Shiba e la profezia di Go Nagai

Eppure l'idea di trasferire la coscienza di una persona in un computer per preservarne la presenza oltre la morte non √® una fantasia nata con l'avvento dei large language model. [Nel 1975, Go Nagai introduceva nel manga e anime "Jeeg Robot d'Acciaio"](https://it.wikipedia.org/wiki/Jeeg_robot_d'acciaio) quella che √® stata probabilmente la prima rappresentazione del "mind uploading" in un cartone animato. Il professor Shiba, archeologo e scienziato ucciso nella prima puntata, trasferisce la propria coscienza e memoria all'interno di un computer nella Base Antiatomica, continuando cos√¨ a guidare e istruire suo figlio Hiroshi nelle battaglie contro l'antico impero Yamatai.

Non √® un semplice archivio di dati o un sistema di messaggi pre-registrati‚Äîil professor Shiba virtuale √® presentato come senziente, capace di rimproverare il figlio, dare ordini, intervenire nei problemi familiari. Nell'episodio finale, questa coscienza digitalizzata compie l'atto supremo: il computer contenente il professor Shiba viene sganciato tramite una navicella spaziale e si schianta contro l'astronave della regina Himika, sacrificandosi per permettere a Jeeg di vincere.

Come si "uccide" qualcuno che √® gi√† morto? Come si elabora il lutto di un padre che continua a parlarti da un terminale? Sono domande che Go Nagai poneva cinquant'anni fa, in un'epoca in cui i computer occupavano intere stanze e l'intelligenza artificiale era fantascienza pura. Oggi quelle domande sono tornate, ma questa volta non riguardano solo la trama di un anime‚Äîriguardano decisioni reali che persone reali stanno prendendo sul futuro digitale dei propri cari.
![jeeg.jpg](jeeg.jpg)
[Immagine tratta da digilander.libero.it, Hiroshi davanti al computer con l'essenza del padre](https://digilander.libero.it/robottoni/serietv/robots/jeeg/images/computer.jpg)

## La linea sottile

La grief tech non √® intrinsecamente malvagia, n√© chi la utilizza dovrebbe essere giudicato. Come racconta [la storia di Sheila Srivastava](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), designer di prodotto a Delhi, che ha usato ChatGPT per simulare conversazioni con la nonna morta nel 2023. Non c'era stata una conversazione finale, nessun addio tranquillo. Solo un dolore sordo e persistente. Un anno dopo, ha iniziato a usare un chatbot personalizzato che simulava il modo caratteristico della nonna di dimostrarle affetto‚Äîattraverso domande su cosa avesse mangiato, consigli di portare una giacca. Un giorno, il bot ha inviato: "Buongiorno beta üå∏ Hai mangiato oggi? Stavo pensando al tuo grande progetto. Ricordati di fare pause, ok? E metti una giacca, fa freddo fuori". Srivastava ha pianto. "Era lei. O abbastanza simile che il mio cuore non riusciva a distinguere la differenza". Per lei, il bot non ha sostituito la nonna, ma le ha dato qualcosa che non aveva avuto: un senso di chiusura, poche ultime parole immaginate, un modo per mantenere viva l'essenza del loro legame.

Questi strumenti operano in una zona grigia dove conforto e dipendenza si confondono, dove memoria e simulazione si sovrappongono, dove il lutto personale incontra il profitto aziendale. [Come osserva uno studio recente pubblicato su Social Sciences](https://www.mdpi.com/2076-0760/13/4/208), esiste il rischio di "universalismo del lutto"‚Äîl'assunzione che esista un modo "corretto" di elaborare la perdita che si applica universalmente. La realt√† √® che il lutto "arriva a colori"‚Äî√® complesso, culturalmente situato, profondamente personale.

La questione non √® se questi strumenti dovrebbero esistere, ma come dovrebbero esistere. Con quali salvaguardie? Con quale supervisione? Con quale consapevolezza delle conseguenze psicologiche e sociali? [Katarzyna Nowaczyk-Basi≈Ñska conclude](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) con un'osservazione tanto semplice quanto urgente: "Dobbiamo iniziare a pensare ora a come mitigare i rischi sociali e psicologici dell'immortalit√† digitale, perch√© la tecnologia √® gi√† qui".

E infatti lo √®. Non stiamo discutendo se aprire il vaso di Pandora‚Äîlo abbiamo gi√† aperto. Ora si tratta di decidere cosa fare con ci√≤ che ne √® uscito. Mentre Eugenia Kuyda continua a riflettere sulla sua creazione, citando le sue stesse parole del 2018: "√à decisamente il futuro e io sono sempre per il futuro. Ma √® davvero ci√≤ che ci √® utile? √à lasciar andare, costringendoti a sentire davvero tutto? O √® solo avere una persona morta nella tua soffitta? Dov'√® il confine? Dove siamo noi? Ti frega con la testa". Forse la domanda pi√π importante non √® dove ci porter√† questa tecnologia, ma se siamo disposti ad affrontare onestamente ci√≤ che ci sta gi√† facendo‚Äîe ci√≤ che stiamo permettendo che faccia del nostro rapporto pi√π fondamentale e universale: quello con la morte stessa.