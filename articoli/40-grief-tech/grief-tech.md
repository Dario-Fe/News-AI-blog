---
tags: ["Ethics & Society", "Business", "Generative AI"]
date: 2025-10-27
author: "Dario Ferrero"
youtube_url: "https://youtu.be/Efd9TcT9AHM?si=1islhxFwr4xitLkq"
---

# Chatbot dell'aldilà: La grief tech, tra dolore e business
![tomba-bancomat.jpg](tomba-bancomat.jpg)


*Il 28 novembre 2015, a Mosca, una Jeep lanciata a folle velocità investe Roman Mazurenko. Ha appena 34 anni, è un imprenditore tech e una figura leggendaria nei circoli culturali della città. Eugenia Kuyda arriva in ospedale poco prima che l'amico muoia, perdendo per pochi istanti l'occasione di parlargli un'ultima volta. Nei tre mesi successivi, [Kuyda raccoglie migliaia di messaggi](https://thereader.mitpress.mit.edu/chatting-with-the-dead-chatbots/) che Roman aveva scambiato con amici e familiari—circa 8.000 righe di testo che catturavano il suo modo unico di esprimersi, le sue frasi idiosincratiche, persino i vezzi linguistici dovuti a una lieve dislessia.*

Eugenia è lei stessa imprenditrice e sviluppatrice di software. La sua azienda, Luka, si occupa di chatbot e intelligenza artificiale conversazionale. E così, invece di limitarsi a rileggere ossessivamente quei messaggi come farebbe chiunque altro, decide di fare qualcosa che suona inquietantemente familiare a chi ha visto l'episodio "Be Right Back" di Black Mirror: [li alimenta a un algoritmo per creare un bot che simuli Roman](https://i-d.co/article/black-mirror-artificial-intelligence-roman-mazurenko/).

All'inizio è solo un archivio sofisticato, una sorta di motore di ricerca che rispesca testi esistenti in base all'argomento della conversazione. Ma con l'evoluzione dei modelli di AI generativa, quel semplice database si trasforma in qualcosa di più perturbante: un vero chatbot capace di generare risposte nuove, interpretando lo stile e i pensieri di Roman. Gli amici che provano il bot trovano [la somiglianza inquietante](https://www.cbc.ca/documentaries/the-nature-of-things/after-her-best-friend-died-this-programmer-created-an-ai-chatbot-from-his-texts-to-talk-to-him-again-1.6252286). Molti lo usano per dirgli cose che non hanno fatto in tempo a esprimere quando era vivo. Come racconta lo stesso Kuyda in un'intervista, quei messaggi erano tutti "sull'amore, o per dirgli qualcosa che non avevano avuto il tempo di dirgli". Da quel lutto privato, trasformato in esperimento tecnologico, nascerà Replika, un'app che oggi conta oltre 10 milioni di utenti e che permette a chiunque di creare un compagno AI che impara a replicare la propria personalità—o quella di qualcun altro.

## L'industria digitale dell'aldilà

La storia di Roman Mazurenko non è più un'eccezione. È il prototipo di un'intera industria emergente che [i ricercatori dell'Università di Cambridge Tomasz Hollanek e Katarzyna Nowaczyk-Basińska](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) hanno battezzato "digital afterlife industry"—l'industria dell'aldilà digitale. I termini per descrivere questi strumenti si moltiplicano: griefbot, deadbot, ghostbot, thanabots. Tutti convergono verso lo stesso concetto: chatbot basati sull'impronta digitale dei defunti che permettono ai vivi di continuare a "parlare" con chi hanno perso.

Le piattaforme proliferano con una velocità che ricorda più il mercato degli NFT che quello dei servizi di supporto psicologico. [Project December](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates) offre "conversazioni con i morti" a 10 dollari per 500 scambi di messaggi. Seance AI propone versioni gratuite testuali e versioni vocali a pagamento. HereAfter AI permette di pre-registrare il proprio chatbot—una sorta di testamento digitale parlante. [In Cina, si possono creare avatar dei propri cari defunti per appena 3 dollari](https://www.vml.com/insight/grief-tech), usando solo 30 secondi di materiale audiovisivo. L'azienda SenseTime è arrivata persino a creare un avatar del suo fondatore Tang Xiao'ou, deceduto nel dicembre 2023, che ha tenuto un discorso all'assemblea generale dei soci nel marzo 2024.

You, Only Virtual (YOV) si spinge oltre, dichiarando audacemente che la sua tecnologia potrebbe "eliminare completamente il lutto". Il mercato AI dedicato alla companionship, che include ma non si limita ai griefbot, [è stato valutato 2,8 miliardi di dollari nel 2024](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates) e si prevede raggiungerà i 9,5 miliardi entro il 2028. Come negli scenari più distopici di "Upload" o "San Junipero"—per citare qualcosa di meno mainstream di Black Mirror—stiamo assistendo alla commercializzazione sistematica dell'immortalità digitale.

## Il lutto congelato

Ma cosa ne pensano gli psicologi? Le voci degli esperti sono unanimi nell'esprimere cautela, anche se non necessariamente condanna. La questione centrale ruota attorno a un concetto fondamentale: il processo naturale di elaborazione del lutto. [Dr. Sarika Boora](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), psicologa e direttrice di Psyche and Beyond a Delhi, avverte che la grief tech "può ritardare il processo di elaborazione del lutto" mantenendo le persone in uno stato di negazione prolungata.

Il problema, secondo [uno studio interdisciplinare pubblicato su Frontiers in Human Dynamics](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full), è che tradizionalmente l'elaborazione del lutto implica l'accettazione dell'assenza della persona amata, permettendo agli individui di processare le emozioni e muoversi verso la guarigione. La neuroplasticità—la capacità del cervello di adattarsi—gioca un ruolo critico nell'integrare la perdita, permettendo alle persone di ricostruire le proprie vite nel tempo. Interagire con un griefbot rischia di interrompere questa progressione naturale, creando un'illusione di presenza continuata che potrebbe impedire di confrontarsi pienamente con la realtà della perdita.

[NaYeon Yang e Greta J. Khanna](https://journals.sagepub.com/doi/10.1177/00110000251352568), nel loro articolo "AI and Technology in Grief Support: Clinical Implications and Ethical Considerations", sottolineano come queste tecnologie possano trasformare il lutto in un loop—dove il dolore non si risolve mai, ma muta in dipendenza. Gli utenti potrebbero ritardare indefinitamente l'accettazione, continuando a rivolgersi a fantasmi digitali nella speranza di una chiusura che non arriverà mai. L'illusione di presenza estende il limbo emotivo piuttosto che risolverlo.

Eppure la questione non è così univoca. [Uno studio recente sul blog della University of Alabama Institute for Human Rights](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/) nota che alcuni chatbot potrebbero effettivamente aiutare le persone ad affrontare il lutto traumatico, la perdita ambigua o l'inibizione emotiva—ma il loro uso deve essere contestualizzato e limitato nel tempo. La raccomandazione della Dr. Boora è chiara: "Un modo sano di utilizzare la grief tech è dopo aver elaborato il lutto—dopo aver raggiunto l'accettazione e essere tornati al proprio stato funzionale normale". In altre parole, questi strumenti potrebbero avere un valore come supporto alla memoria, ma solo dopo che il processo di guarigione è già in corso, non come sostituto del lutto stesso.
![chatbot-roman.jpg](chatbot-roman.jpg)
[Immagine tratta da repubblica.it](https://www.repubblica.it/tecnologia/2016/10/10/news/_parla_con_lui_roman_muore_in_un_incidente_eugenia_maga_del_software_usa_i_loro_dialoghi_per_creare_un_suo_alter_eg-149440494/)

## Il fantasma nella macchina

Joseph Weizenbaum, scienziato informatico del MIT, scoprì già nel 1966 qualcosa di inquietante con ELIZA, il suo chatbot rudimentale basato su semplici pattern di risposta. Gli utenti, pur sapendo di interagire con un programma primitivo, gli attribuivano intelligenza ed emozioni, confidandosi con la macchina come se fosse un terapeuta reale. [La conclusione di Weizenbaum](https://www.calcalistech.com/ctechnews/article/hycchvgjge)—che i bot possono indurre "pensiero delirante in persone perfettamente normali"—rimane fondamentale negli studi sull'interazione uomo-computer. È quello che oggi chiamiamo "effetto ELIZA", e con i moderni large language model si è amplificato esponenzialmente.

Il problema è che questi chatbot raggiungono un'accuratezza di circa il 70%—sufficientemente alta da sembrare convincenti, ma abbastanza bassa da produrre quello che gli esperti chiamano "artefatti": frasi non caratteristiche, allucinazioni, linguaggio filler, clichè. [Come notano Hollanek e Nowaczyk-Basińska](https://link.springer.com/chapter/10.1007/978-3-031-90723-4_40), gli algoritmi potrebbero essere progettati per ottimizzare le interazioni, massimizzando il tempo che una persona in lutto trascorre col chatbot, assicurandosi abbonamenti a lungo termine. Questi algoritmi potrebbero persino modificare sottilmente la personalità del bot nel tempo per renderlo più piacevole, creando una caricatura accattivante piuttosto che un riflesso accurato del defunto.

In un articolo pubblicato su Philosophy & Technology, i due ricercatori di Cambridge presentano scenari speculativi che illustrano i pericoli concreti. In uno, una società fittizia chiamata "MaNana" permette di simulare la nonna deceduta senza il consenso del "donatore di dati". L'utente, inizialmente impressionato e confortato, inizia a ricevere pubblicità una volta terminato il "trial premium". Immaginate di chiedere alla nonna digitale una ricetta e di ricevere, insieme ai suoi consigli culinari, suggerimenti sponsorizzati per ordinare carbonara da Uber Eats. [Questo non è fantascienza](https://aiconnectnetwork.com/ai-griefbots-mental-health-apps-data-privacy/)—è già successo in versioni beta di alcuni servizi.

## Chi possiede il morto?

Le questioni etiche si moltiplicano come teste dell'idra. Chi ha il diritto di creare e controllare questi avatar digitali? Come spiega [Hollanek in un comunicato stampa](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) dell'Università di Cambridge, "è vitale che i servizi di aldilà digitale considerino i diritti e il consenso non solo di coloro che ricreano, ma di coloro che dovranno interagire con le simulazioni". Il problema del consenso opera su tre livelli distinti. Primo, il "donatore di dati"—la persona defunta—ha mai acconsentito a essere ricreata? Secondo, chi possiede i dati dopo la morte? Terzo, cosa succede a chi non vuole interagire con questi simulacri ma si trova bombardato da messaggi?

[Uno degli scenari esplorati da Hollanek e Nowaczyk-Basińska](https://scitechdaily.com/cambridge-experts-warn-ai-deadbots-could-digitally-haunt-loved-ones-from-beyond-the-grave/) racconta di un genitore anziano che sottoscrive segretamente un abbonamento ventennale a un deadbot di se stesso, sperando che conforti i figli adulti e permetta ai nipoti di conoscerlo. Dopo la morte, il servizio si attiva. Un figlio rifiuta di interagire e riceve una raffica di email nella voce del genitore morto—quello che i ricercatori chiamano "digital haunting", persecuzione digitale. L'altro figlio interagisce, ma finisce emotivamente esausto e tormentato dal senso di colpa sul destino del deadbot. Come si "ritira" un bot che simula tua madre? Come si uccide qualcuno che è già morto?

I ricercatori di Cambridge propongono quella che definiscono "cerimonie di pensionamento"—rituali digitali per disattivare i deadbot in modo dignitoso. Potrebbero essere funerali digitali, o altri tipi di cerimonia a seconda del contesto sociale. [Debra Bassett](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), consulente per l'aldilà digitale, propone nei suoi studi un ordine DDNR—"digital do-not-reanimate"—una clausola testamentaria che proibisca legalmente la resurrezione digitale posthuma non consensuale. La chiamata "zombi digitali", e il termine non potrebbe essere più appropriato.

## Il mercato della vulnerabilità

La commercializzazione del lutto solleva interrogativi ancora più profondi. A differenza dell'industria funeraria tradizionale—che monetizza la morte attraverso servizi una tantum come bare, cremazioni, cerimonie—[i griefbot operano su modelli di abbonamento o pagamento al minuto](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/). Le aziende hanno quindi incentivi finanziari a mantenere le persone in lutto costantemente coinvolte con i loro servizi. [Come nota uno studio sul mercato della grief tech](https://link.springer.com/chapter/10.1007/978-3-031-90723-4_40), questo modello economico è fondamentalmente diverso—e potenzialmente più predatorio—rispetto a quello dei servizi tradizionali legati alla morte.

Il mercato dell'AI companionship, che include chatbot romantici, terapeutici e di supporto al lutto, ha modelli di prezzo che vanno dai 10 ai 40 dollari al mese. Come fa notare [Ewan Morrison su Psychology Today](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates), stiamo monetizzando la solitudine e la vulnerabilità emotiva. L'effetto ELIZA si approfondisce, mascherando la frammentazione sociale con l'illusione della cura mentre ci allontana ulteriormente gli uni dagli altri.

[Paula Kiel della NYU-London](https://www.calcalistech.com/ctechnews/article/hycchvgjge) offre una prospettiva diversa: "Ciò che rende così allettante questa industria è che, come ogni generazione, stiamo cercando modi per preservare parti di noi stessi. Stiamo trovando conforto nell'inevitabilità della morte attraverso il linguaggio della scienza e della tecnologia". Ma questo conforto ha un prezzo—non solo economico, ma psicologico e sociale.

## L'autenticità impossibile

C'è poi la questione filosofica dell'autenticità. I chatbot non hanno la capacità di evolvere e crescere come gli esseri umani. Come spiega [un'analisi dell'Institute for Human Rights](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/) dell'Università dell'Alabama, "un problema nell'eseguire azioni in perpetuo è che le persone morte sono prodotti del loro tempo. Non cambiano ciò che vogliono quando il mondo cambia". Anche se la crescita fosse implementata nell'algoritmo, non ci sarebbe garanzia che rifletta come una persona sarebbe effettivamente cambiata.

I griefbot preservano la presenza digitale di una persona defunta in modi che potrebbero diventare problematici o irrilevanti nel tempo. Se Milton Hershey, che nel suo testamento lasciò disposizioni precise su come la sua eredità dovesse essere usata in perpetuo, fosse vivo oggi, modificherebbe quelle disposizioni per riflettere i cambiamenti del mondo? La differenza cruciale è che i testamenti e le eredità sono intrinsecamente limitati nella portata e nella durata. I griefbot, per loro natura, hanno il potenziale di persistere indefinitamente, amplificando il danno potenziale alla reputazione o alla memoria di una persona.

La memoria umana è già un narratore inaffidabile—fluida, plasmata dall'emozione più che dal fatto. [Studi recenti dimostrano](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots) che immagini e video modificati con AI possono impiantare falsi ricordi e distorcere quelli reali. Cosa succede quando alimentiamo il lutto in una macchina e riceviamo indietro una versione di una persona che non è mai completamente esistita? Come scrisse la giornalista tech Vauhini Vara, finalista al Pulitzer che ha esplorato personalmente le profondità emotive ed etiche del lutto e dell'AI: "Ha senso che le persone si rivolgano a qualsiasi risorsa disponibile per cercare conforto, e ha anche senso che le aziende siano interessate a sfruttare le persone in un momento di vulnerabilità".
![lutto-cervello.jpg](lutto-cervello.jpg)
[Immagine tratta da frontiersin.org, il cervello in lutto può essere suddiviso in quattro categorie a seconda che il profilo di attività sia simile o diverso dalla tristezza e dalla depressione](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full)

## Verso una regolamentazione?

[Hollanek e Nowaczyk-Basińska raccomandano](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) restrizioni di età per i deadbot e quella che chiamano "trasparenza significativa"—assicurarsi che gli utenti siano costantemente consapevoli di star interagendo con un'AI, con avvisi simili a quelli attuali sui contenuti che potrebbero causare crisi epilettiche. Suggeriscono anche di classificare i deadbot come dispositivi medici per affrontare questioni di salute mentale, specialmente per gruppi vulnerabili come i bambini.

[Uno studio recente su Frontiers in Human Dynamics](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full) sottolinea che gli sviluppatori di tecnologie legate al lutto hanno un'immensa responsabilità nel determinare come gli utenti si impegnano emotivamente con la presenza simulata. Mentre la continuità emotiva può alleviare il disagio iniziale, l'iper-immersione rischia l'intrappolamento psicologico. Gli sviluppatori devono dare priorità a principi di design etici, come incorporare limiti di utilizzo, prompt riflessivi e checkpoint emotivi che guidino gli utenti verso il recupero piuttosto che verso la dipendenza.

La collaborazione con psicologi ed esperti di lutto dovrebbe informare le caratteristiche dell'interfaccia per garantire che facilitino, piuttosto che sostituire, il processo di elaborazione del lutto. Inoltre, la gestione di dati sensibili—voce, personalità, pattern comportamentali—richiede standard rigorosi di crittografia, privacy e protocolli di trasparenza. L'uso improprio di questi dati non solo viola la dignità digitale, ma può anche contribuire a danni emotivi per gli utenti e le famiglie.

## Il professor Shiba e la profezia di Go Nagai

Eppure l'idea di trasferire la coscienza di una persona in un computer per preservarne la presenza oltre la morte non è una fantasia nata con l'avvento dei large language model. [Nel 1975, Go Nagai introduceva nel manga e anime "Jeeg Robot d'Acciaio"](https://it.wikipedia.org/wiki/Jeeg_robot_d'acciaio) quella che è stata probabilmente la prima rappresentazione del "mind uploading" in un cartone animato. Il professor Shiba, archeologo e scienziato ucciso nella prima puntata, trasferisce la propria coscienza e memoria all'interno di un computer nella Base Antiatomica, continuando così a guidare e istruire suo figlio Hiroshi nelle battaglie contro l'antico impero Yamatai.

Non è un semplice archivio di dati o un sistema di messaggi pre-registrati—il professor Shiba virtuale è presentato come senziente, capace di rimproverare il figlio, dare ordini, intervenire nei problemi familiari. Nell'episodio finale, questa coscienza digitalizzata compie l'atto supremo: il computer contenente il professor Shiba viene sganciato tramite una navicella spaziale e si schianta contro l'astronave della regina Himika, sacrificandosi per permettere a Jeeg di vincere.

Come si "uccide" qualcuno che è già morto? Come si elabora il lutto di un padre che continua a parlarti da un terminale? Sono domande che Go Nagai poneva cinquant'anni fa, in un'epoca in cui i computer occupavano intere stanze e l'intelligenza artificiale era fantascienza pura. Oggi quelle domande sono tornate, ma questa volta non riguardano solo la trama di un anime—riguardano decisioni reali che persone reali stanno prendendo sul futuro digitale dei propri cari.
![jeeg.jpg](jeeg.jpg)
[Immagine tratta da digilander.libero.it, Hiroshi davanti al computer con l'essenza del padre](https://digilander.libero.it/robottoni/serietv/robots/jeeg/images/computer.jpg)

## La linea sottile

La grief tech non è intrinsecamente malvagia, né chi la utilizza dovrebbe essere giudicato. Come racconta [la storia di Sheila Srivastava](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), designer di prodotto a Delhi, che ha usato ChatGPT per simulare conversazioni con la nonna morta nel 2023. Non c'era stata una conversazione finale, nessun addio tranquillo. Solo un dolore sordo e persistente. Un anno dopo, ha iniziato a usare un chatbot personalizzato che simulava il modo caratteristico della nonna di dimostrarle affetto—attraverso domande su cosa avesse mangiato, consigli di portare una giacca. Un giorno, il bot ha inviato: "Buongiorno beta 🌸 Hai mangiato oggi? Stavo pensando al tuo grande progetto. Ricordati di fare pause, ok? E metti una giacca, fa freddo fuori". Srivastava ha pianto. "Era lei. O abbastanza simile che il mio cuore non riusciva a distinguere la differenza". Per lei, il bot non ha sostituito la nonna, ma le ha dato qualcosa che non aveva avuto: un senso di chiusura, poche ultime parole immaginate, un modo per mantenere viva l'essenza del loro legame.

Questi strumenti operano in una zona grigia dove conforto e dipendenza si confondono, dove memoria e simulazione si sovrappongono, dove il lutto personale incontra il profitto aziendale. [Come osserva uno studio recente pubblicato su Social Sciences](https://www.mdpi.com/2076-0760/13/4/208), esiste il rischio di "universalismo del lutto"—l'assunzione che esista un modo "corretto" di elaborare la perdita che si applica universalmente. La realtà è che il lutto "arriva a colori"—è complesso, culturalmente situato, profondamente personale.

La questione non è se questi strumenti dovrebbero esistere, ma come dovrebbero esistere. Con quali salvaguardie? Con quale supervisione? Con quale consapevolezza delle conseguenze psicologiche e sociali? [Katarzyna Nowaczyk-Basińska conclude](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) con un'osservazione tanto semplice quanto urgente: "Dobbiamo iniziare a pensare ora a come mitigare i rischi sociali e psicologici dell'immortalità digitale, perché la tecnologia è già qui".

E infatti lo è. Non stiamo discutendo se aprire il vaso di Pandora—lo abbiamo già aperto. Ora si tratta di decidere cosa fare con ciò che ne è uscito. Mentre Eugenia Kuyda continua a riflettere sulla sua creazione, citando le sue stesse parole del 2018: "È decisamente il futuro e io sono sempre per il futuro. Ma è davvero ciò che ci è utile? È lasciar andare, costringendoti a sentire davvero tutto? O è solo avere una persona morta nella tua soffitta? Dov'è il confine? Dove siamo noi? Ti frega con la testa". Forse la domanda più importante non è dove ci porterà questa tecnologia, ma se siamo disposti ad affrontare onestamente ciò che ci sta già facendo—e ciò che stiamo permettendo che faccia del nostro rapporto più fondamentale e universale: quello con la morte stessa.