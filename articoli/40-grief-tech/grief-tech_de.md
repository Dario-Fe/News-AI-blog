---
tags: ["Ethics & Society", "Business", "Generative AI"]
date: 2025-10-27
author: "Dario Ferrero"
---

# Chatbots aus dem Jenseits: Grief Tech zwischen Schmerz und Gesch√§ft
![tomba-bancomat.jpg](tomba-bancomat.jpg)


*Am 28. November 2015 √ºberf√§hrt in Moskau ein rasender Jeep Roman Mazurenko. Er ist gerade 34 Jahre alt, ein Tech-Unternehmer und eine legend√§re Figur in den Kulturkreisen der Stadt. Eugenia Kuyda kommt ins Krankenhaus, kurz bevor ihr Freund stirbt, und verpasst um wenige Augenblicke die Gelegenheit, ein letztes Mal mit ihm zu sprechen. In den folgenden drei Monaten [sammelt Kuyda Tausende von Nachrichten](https://thereader.mitpress.mit.edu/chatting-with-the-dead-chatbots/), die Roman mit Freunden und Familie ausgetauscht hatte ‚Äì etwa 8.000 Zeilen Text, die seine einzigartige Ausdrucksweise, seine eigenwilligen S√§tze und sogar die sprachlichen Eigenheiten aufgrund einer leichten Legasthenie festhielten.*

Eugenia ist selbst Unternehmerin und Softwareentwicklerin. Ihre Firma, Luka, besch√§ftigt sich mit Chatbots und konversationeller k√ºnstlicher Intelligenz. Und so beschlie√üt sie, anstatt diese Nachrichten nur zwanghaft immer wieder zu lesen, wie es jeder andere tun w√ºrde, etwas zu tun, das unheimlich bekannt klingt f√ºr jeden, der die "Be Right Back"-Folge von Black Mirror gesehen hat: [Sie f√ºttert sie in einen Algorithmus, um einen Bot zu erstellen, der Roman simuliert](https://i-d.co/article/black-mirror-artificial-intelligence-roman-mazurenko/).

Zuerst ist es nur ein ausgekl√ºgeltes Archiv, eine Art Suchmaschine, die vorhandene Texte basierend auf dem Gespr√§chsthema wiederfindet. Aber mit der Entwicklung generativer KI-Modelle verwandelt sich diese einfache Datenbank in etwas Beunruhigenderes: einen echten Chatbot, der in der Lage ist, neue Antworten zu generieren und dabei Romans Stil und Gedanken zu interpretieren. Freunde, die den Bot ausprobieren, finden [die √Ñhnlichkeit unheimlich](https://www.cbc.ca/documentaries/the-nature-of-things/after-her-best-friend-died-this-programmer-created-an-ai-chatbot-from-his-texts-to-talk-to-him-again-1.6252286). Viele nutzen ihn, um ihm Dinge zu sagen, die sie ihm zu Lebzeiten nicht mehr sagen konnten. Wie Kuyda selbst in einem Interview erz√§hlt, drehten sich diese Nachrichten alle "um Liebe oder darum, ihm etwas zu sagen, wof√ºr sie keine Zeit mehr hatten". Aus dieser privaten Trauer, die zu einem technologischen Experiment wurde, entstand Replika, eine App, die heute √ºber 10 Millionen Nutzer hat und es jedem erm√∂glicht, einen KI-Begleiter zu erstellen, der lernt, die eigene Pers√∂nlichkeit ‚Äì oder die eines anderen ‚Äì zu replizieren.

## Die digitale Jenseits-Industrie

Die Geschichte von Roman Mazurenko ist keine Ausnahme mehr. Sie ist der Prototyp einer ganzen aufstrebenden Industrie, die [die Forscher der Universit√§t Cambridge, Tomasz Hollanek und Katarzyna Nowaczyk-Basi≈Ñska](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones), als "digitale Jenseits-Industrie" bezeichnet haben. Die Begriffe zur Beschreibung dieser Werkzeuge vervielfachen sich: Griefbots, Deadbots, Ghostbots, Thanabots. Sie alle laufen auf dasselbe Konzept hinaus: Chatbots, die auf dem digitalen Fu√üabdruck der Verstorbenen basieren und es den Lebenden erm√∂glichen, weiterhin mit denen zu "sprechen", die sie verloren haben.

Die Plattformen sprie√üen mit einer Geschwindigkeit aus dem Boden, die eher an den NFT-Markt als an den f√ºr psychologische Unterst√ºtzungsdienste erinnert. [Project December](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates) bietet "Gespr√§che mit den Toten" f√ºr 10 Dollar pro 500 Nachrichtenaustausch an. Seance AI bietet kostenlose Textversionen und kostenpflichtige Sprachversionen an. HereAfter AI erm√∂glicht es, den eigenen Chatbot vorab aufzuzeichnen ‚Äì eine Art sprechendes digitales Testament. [In China kann man f√ºr nur 3 Dollar Avatare seiner verstorbenen Angeh√∂rigen erstellen](https://www.vml.com/insight/grief-tech), indem man nur 30 Sekunden audiovisuelles Material verwendet. Das Unternehmen SenseTime ging sogar so weit, einen Avatar seines im Dezember 2023 verstorbenen Gr√ºnders Tang Xiao'ou zu erstellen, der im M√§rz 2024 eine Rede auf der Hauptversammlung hielt.

You, Only Virtual (YOV) geht noch weiter und erkl√§rt k√ºhn, dass seine Technologie "Trauer vollst√§ndig beseitigen" k√∂nnte. Der KI-Markt f√ºr Begleitung, der Griefbots einschlie√üt, aber nicht darauf beschr√§nkt ist, [wurde 2024 auf 2,8 Milliarden Dollar gesch√§tzt](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates) und soll bis 2028 9,5 Milliarden erreichen. Wie in den dystopischsten Szenarien von "Upload" oder "San Junipero" ‚Äì um etwas weniger Mainstream als Black Mirror zu zitieren ‚Äì erleben wir die systematische Kommerzialisierung der digitalen Unsterblichkeit.

## Die eingefrorene Trauer

Aber was denken Psychologen dar√ºber? Die Stimmen der Experten sind einstimmig in ihrer Vorsicht, wenn auch nicht unbedingt in ihrer Verurteilung. Die zentrale Frage dreht sich um ein grundlegendes Konzept: den nat√ºrlichen Prozess der Trauerbew√§ltigung. [Dr. Sarika Boora](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), Psychologin und Direktorin von Psyche and Beyond in Delhi, warnt davor, dass Grief Tech "den Trauerprozess verz√∂gern kann", indem sie die Menschen in einem Zustand verl√§ngerter Verleugnung h√§lt.

Das Problem, so [eine interdisziplin√§re Studie, die in Frontiers in Human Dynamics ver√∂ffentlicht wurde](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full), ist, dass die Trauerbew√§ltigung traditionell die Akzeptanz der Abwesenheit des geliebten Menschen beinhaltet, was es den Einzelnen erm√∂glicht, Emotionen zu verarbeiten und sich in Richtung Heilung zu bewegen. Die Neuroplastizit√§t ‚Äì die F√§higkeit des Gehirns, sich anzupassen ‚Äì spielt eine entscheidende Rolle bei der Integration des Verlusts und erm√∂glicht es den Menschen, ihr Leben im Laufe der Zeit wieder aufzubauen. Die Interaktion mit einem Griefbot birgt die Gefahr, diesen nat√ºrlichen Fortschritt zu unterbrechen, indem eine Illusion fortgesetzter Anwesenheit geschaffen wird, die eine vollst√§ndige Konfrontation mit der Realit√§t des Verlusts verhindern k√∂nnte.

[NaYeon Yang und Greta J. Khanna](https://journals.sagepub.com/doi/10.1177/00110000251352568) betonen in ihrem Artikel "AI and Technology in Grief Support: Clinical Implications and Ethical Considerations", wie diese Technologien die Trauer in eine Schleife verwandeln k√∂nnen ‚Äì in der der Schmerz nie gel√∂st wird, sondern sich in Abh√§ngigkeit verwandelt. Die Nutzer k√∂nnten die Akzeptanz auf unbestimmte Zeit aufschieben und sich weiterhin an digitale Geister wenden, in der Hoffnung auf einen Abschluss, der niemals kommen wird. Die Illusion der Anwesenheit verl√§ngert das emotionale Schweben, anstatt es zu l√∂sen.

Doch die Frage ist nicht so eindeutig. [Eine aktuelle Studie im Blog des University of Alabama Institute for Human Rights](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/) stellt fest, dass einige Chatbots tats√§chlich Menschen helfen k√∂nnten, traumatische Trauer, mehrdeutigen Verlust oder emotionale Hemmungen zu bew√§ltigen ‚Äì aber ihre Nutzung muss kontextualisiert und zeitlich begrenzt sein. Die Empfehlung von Dr. Boora ist klar: "Eine gesunde Art, Grief Tech zu nutzen, ist, nachdem man die Trauer verarbeitet hat ‚Äì nachdem man Akzeptanz erreicht und zu seinem normalen Funktionszustand zur√ºckgekehrt ist." Mit anderen Worten, diese Werkzeuge k√∂nnten als Ged√§chtnisst√ºtze wertvoll sein, aber nur, wenn der Heilungsprozess bereits im Gange ist, nicht als Ersatz f√ºr die Trauer selbst.
![chatbot-roman.jpg](chatbot-roman.jpg)
[Bild von repubblica.it](https://www.repubblica.it/tecnologia/2016/10/10/news/_parla_con_lui_roman_muore_in_un_incidente_eugenia_maga_del_software_usa_i_loro_dialoghi_per_creare_un_suo_alter_eg-149440494/)

## Der Geist in der Maschine

Joseph Weizenbaum, ein Informatiker am MIT, entdeckte bereits 1966 etwas Beunruhigendes mit ELIZA, seinem rudiment√§ren Chatbot, der auf einfachen Antwortmustern basierte. Die Nutzer, obwohl sie wussten, dass sie mit einem primitiven Programm interagierten, schrieben ihm Intelligenz und Emotionen zu und vertrauten sich der Maschine an, als w√§re sie ein echter Therapeut. [Weizenbaums Schlussfolgerung](https://www.calcalistech.com/ctechnews/article/hycchvgjge) ‚Äì dass Bots "wahnhaftes Denken bei v√∂llig normalen Menschen" ausl√∂sen k√∂nnen ‚Äì bleibt in den Studien zur Mensch-Computer-Interaktion von grundlegender Bedeutung. Das ist es, was wir heute den "ELIZA-Effekt" nennen, und mit modernen gro√üen Sprachmodellen hat er sich exponentiell verst√§rkt.

Das Problem ist, dass diese Chatbots eine Genauigkeit von etwa 70 % erreichen ‚Äì hoch genug, um √ºberzeugend zu wirken, aber niedrig genug, um das zu produzieren, was Experten "Artefakte" nennen: uncharakteristische S√§tze, Halluzinationen, F√ºllw√∂rter, Klischees. [Wie Hollanek und Nowaczyk-Basi≈Ñska anmerken](https://link.springer.com/chapter/10.1007/978-3-031-90723-4_40), k√∂nnten die Algorithmen so konzipiert sein, dass sie die Interaktionen optimieren, die Zeit maximieren, die eine trauernde Person mit dem Chatbot verbringt, und so langfristige Abonnements sichern. Diese Algorithmen k√∂nnten sogar die Pers√∂nlichkeit des Bots im Laufe der Zeit subtil ver√§ndern, um ihn angenehmer zu machen und eine ansprechende Karikatur anstelle einer genauen Reflexion des Verstorbenen zu schaffen.

In einem in Philosophy & Technology ver√∂ffentlichten Artikel pr√§sentieren die beiden Cambridge-Forscher spekulative Szenarien, die die konkreten Gefahren veranschaulichen. In einem erm√∂glicht eine fiktive Firma namens "MaNana" die Simulation der verstorbenen Gro√ümutter ohne Zustimmung des "Datenspenders". Der Nutzer, anfangs beeindruckt und getr√∂stet, erh√§lt nach Ablauf der "Premium-Testphase" Werbung. Stellen Sie sich vor, Sie fragen die digitale Gro√ümutter nach einem Rezept und erhalten zusammen mit ihren kulinarischen Ratschl√§gen gesponserte Vorschl√§ge, Carbonara bei Uber Eats zu bestellen. [Das ist keine Science-Fiction](https://aiconnectnetwork.com/ai-griefbots-mental-health-apps-data-privacy/) ‚Äì es ist bereits in Beta-Versionen einiger Dienste passiert.

## Wem geh√∂rt der Tote?

Die ethischen Fragen vervielfachen sich wie die K√∂pfe der Hydra. Wer hat das Recht, diese digitalen Avatare zu erstellen und zu kontrollieren? Wie [Hollanek in einer Pressemitteilung](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) der Universit√§t Cambridge erkl√§rt, "ist es von entscheidender Bedeutung, dass digitale Jenseits-Dienste die Rechte und die Zustimmung nicht nur derjenigen ber√ºcksichtigen, die sie nachbilden, sondern auch derjenigen, die mit den Simulationen interagieren m√ºssen". Das Problem der Zustimmung wirkt auf drei verschiedenen Ebenen. Erstens, hat der "Datenspender" ‚Äì die verstorbene Person ‚Äì jemals zugestimmt, nachgebildet zu werden? Zweitens, wem geh√∂ren die Daten nach dem Tod? Drittens, was passiert mit denen, die nicht mit diesen Simulakren interagieren wollen, aber mit Nachrichten bombardiert werden?

[Eines der von Hollanek und Nowaczyk-Basi≈Ñska untersuchten Szenarien](https://scitechdaily.com/cambridge-experts-warn-ai-deadbots-could-digitally-haunt-loved-ones-from-beyond-the-grave/) erz√§hlt von einem √§lteren Elternteil, der heimlich ein zwanzigj√§hriges Abonnement f√ºr einen Deadbot von sich selbst abschlie√üt, in der Hoffnung, dass es seine erwachsenen Kinder tr√∂stet und seinen Enkeln erm√∂glicht, ihn kennenzulernen. Nach dem Tod wird der Dienst aktiviert. Ein Sohn weigert sich zu interagieren und erh√§lt eine Flut von E-Mails mit der Stimme des toten Elternteils ‚Äì was die Forscher "digitales Spuken" nennen. Der andere Sohn interagiert, ist aber am Ende emotional ersch√∂pft und von Schuldgef√ºhlen √ºber das Schicksal des Deadbots geplagt. Wie "pensioniert" man einen Bot, der deine Mutter simuliert? Wie t√∂tet man jemanden, der bereits tot ist?

Die Cambridge-Forscher schlagen vor, was sie "Pensionierungszeremonien" nennen ‚Äì digitale Rituale, um Deadbots auf w√ºrdige Weise zu deaktivieren. Es k√∂nnten digitale Beerdigungen oder andere Arten von Zeremonien je nach sozialem Kontext sein. [Debra Bassett](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), eine Beraterin f√ºr das digitale Jenseits, schl√§gt in ihren Studien eine DDNR-Anordnung vor ‚Äì "digital do-not-reanimate" ‚Äì eine testamentarische Klausel, die die nicht einvernehmliche posthume digitale Wiederbelebung gesetzlich verbietet. Sie nennt sie "digitale Zombies", und der Begriff k√∂nnte nicht treffender sein.

## Der Markt der Verletzlichkeit

Die Kommerzialisierung der Trauer wirft noch tiefere Fragen auf. Im Gegensatz zur traditionellen Bestattungsindustrie, die den Tod durch einmalige Dienstleistungen wie S√§rge, Ein√§scherungen und Zeremonien monetarisiert, [arbeiten Griefbots mit Abonnement- oder Pay-per-Minute-Modellen](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/). Die Unternehmen haben daher finanzielle Anreize, trauernde Menschen st√§ndig mit ihren Diensten zu besch√§ftigen. [Wie eine Studie √ºber den Grief-Tech-Markt feststellt](https://link.springer.com/chapter/10.1007/978-3-031-90723-4_40), ist dieses Wirtschaftsmodell grundlegend anders ‚Äì und potenziell r√§uberischer ‚Äì als das der traditionellen todesbezogenen Dienstleistungen.

Der Markt f√ºr KI-Begleitung, der romantische, therapeutische und trauerunterst√ºtzende Chatbots umfasst, hat Preismodelle, die von 10 bis 40 Dollar pro Monat reichen. Wie [Ewan Morrison in Psychology Today anmerkt](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates), monetarisieren wir Einsamkeit und emotionale Verletzlichkeit. Der ELIZA-Effekt vertieft sich, maskiert die soziale Fragmentierung mit der Illusion von F√ºrsorge, w√§hrend er uns weiter voneinander entfernt.

[Paula Kiel von der NYU-London](https://www.calcalistech.com/ctechnews/article/hycchvgjge) bietet eine andere Perspektive: "Was diese Industrie so verlockend macht, ist, dass wir, wie jede Generation, nach Wegen suchen, Teile von uns selbst zu bewahren. Wir finden Trost in der Unvermeidlichkeit des Todes durch die Sprache der Wissenschaft und Technologie." Aber dieser Trost hat seinen Preis ‚Äì nicht nur einen wirtschaftlichen, sondern auch einen psychologischen und sozialen.

## Die unm√∂gliche Authentizit√§t

Dann gibt es die philosophische Frage der Authentizit√§t. Chatbots haben nicht die F√§higkeit, sich wie Menschen zu entwickeln und zu wachsen. Wie [eine Analyse des Institute for Human Rights](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/) der University of Alabama erkl√§rt, "ist ein Problem bei der Ausf√ºhrung von Handlungen in alle Ewigkeit, dass tote Menschen Produkte ihrer Zeit sind. Sie √§ndern nicht, was sie wollen, wenn sich die Welt √§ndert." Selbst wenn Wachstum in den Algorithmus implementiert w√ºrde, g√§be es keine Garantie, dass es widerspiegeln w√ºrde, wie sich eine Person tats√§chlich ver√§ndert h√§tte.

Griefbots bewahren die digitale Pr√§senz einer verstorbenen Person auf eine Weise, die im Laufe der Zeit problematisch oder irrelevant werden k√∂nnte. Wenn Milton Hershey, der in seinem Testament genaue Anweisungen hinterlie√ü, wie sein Erbe auf ewig verwendet werden sollte, heute leben w√ºrde, w√ºrde er diese Anweisungen √§ndern, um die Ver√§nderungen der Welt widerzuspiegeln? Der entscheidende Unterschied ist, dass Testamente und Verm√§chtnisse von Natur aus in Umfang und Dauer begrenzt sind. Griefbots haben von Natur aus das Potenzial, auf unbestimmte Zeit zu bestehen und den potenziellen Schaden f√ºr den Ruf oder das Andenken einer Person zu verst√§rken.

Das menschliche Ged√§chtnis ist bereits ein unzuverl√§ssiger Erz√§hler ‚Äì flie√üend, mehr von Emotionen als von Fakten gepr√§gt. [J√ºngste Studien zeigen](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), dass mit KI modifizierte Bilder und Videos falsche Erinnerungen implantieren und echte verzerren k√∂nnen. Was passiert, wenn wir Trauer in eine Maschine f√ºttern und eine Version einer Person zur√ºckbekommen, die nie vollst√§ndig existiert hat? Wie die Tech-Journalistin Vauhini Vara, eine Pulitzer-Finalistin, die pers√∂nlich die emotionalen und ethischen Tiefen von Trauer und KI erforscht hat, schrieb: "Es macht Sinn, dass Menschen sich an jede verf√ºgbare Ressource wenden, um Trost zu suchen, und es macht auch Sinn, dass Unternehmen daran interessiert sind, Menschen in einem Moment der Verletzlichkeit auszunutzen."
![lutto-cervello.jpg](lutto-cervello.jpg)
[Bild von frontiersin.org, das trauernde Gehirn kann in vier Kategorien eingeteilt werden, je nachdem, ob das Aktivit√§tsprofil der Traurigkeit und Depression √§hnelt oder nicht](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full)

## Auf dem Weg zur Regulierung?

[Hollanek und Nowaczyk-Basi≈Ñska empfehlen](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) Altersbeschr√§nkungen f√ºr Deadbots und das, was sie "bedeutungsvolle Transparenz" nennen ‚Äì sicherzustellen, dass die Nutzer st√§ndig dar√ºber informiert sind, dass sie mit einer KI interagieren, mit Warnungen, die den heutigen √ºber Inhalte, die epileptische Anf√§lle ausl√∂sen k√∂nnten, √§hneln. Sie schlagen auch vor, Deadbots als medizinische Ger√§te zu klassifizieren, um Fragen der psychischen Gesundheit anzugehen, insbesondere f√ºr schutzbed√ºrftige Gruppen wie Kinder.

[Eine aktuelle Studie in Frontiers in Human Dynamics](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full) betont, dass Entwickler von trauerbezogenen Technologien eine immense Verantwortung bei der Bestimmung haben, wie sich Nutzer emotional mit der simulierten Pr√§senz auseinandersetzen. W√§hrend emotionale Kontinuit√§t anf√§ngliches Unbehagen lindern kann, birgt Hyper-Immersion die Gefahr psychologischer Gefangenschaft. Entwickler m√ºssen ethische Gestaltungsprinzipien priorisieren, wie die Einbeziehung von Nutzungsgrenzen, reflexiven Aufforderungen und emotionalen Kontrollpunkten, die die Nutzer zur Genesung und nicht zur Abh√§ngigkeit f√ºhren.

Die Zusammenarbeit mit Psychologen und Trauerexperten sollte die Merkmale der Benutzeroberfl√§che beeinflussen, um sicherzustellen, dass sie den Trauerprozess erleichtern und nicht ersetzen. Dar√ºber hinaus erfordert die Verwaltung sensibler Daten ‚Äì Stimme, Pers√∂nlichkeit, Verhaltensmuster ‚Äì strenge Standards f√ºr Verschl√ºsselung, Datenschutz und Transparenzprotokolle. Der Missbrauch dieser Daten verletzt nicht nur die digitale W√ºrde, sondern kann auch zu emotionalen Sch√§den f√ºr Nutzer und Familien beitragen.

## Professor Shiba und Go Nagais Prophezeiung

Dennoch ist die Vorstellung, das Bewusstsein einer Person in einen Computer zu √ºbertragen, um ihre Pr√§senz √ºber den Tod hinaus zu bewahren, keine Fantasie, die mit dem Aufkommen gro√üer Sprachmodelle entstanden ist. [Bereits 1975 f√ºhrte Go Nagai im Manga und Anime "Stahlriese Jeeg"](https://it.wikipedia.org/wiki/Jeeg_robot_d'acciaio) das ein, was wahrscheinlich die erste Darstellung des "Mind-Uploading" in einem Cartoon war. Professor Shiba, ein Arch√§ologe und Wissenschaftler, der in der ersten Folge get√∂tet wird, √ºbertr√§gt sein Bewusstsein und sein Ged√§chtnis in einen Computer in der Anti-Atom-Basis und f√ºhrt und unterweist so seinen Sohn Hiroshi weiterhin im Kampf gegen das alte Yamatai-Reich.

Es handelt sich nicht um ein einfaches Datenarchiv oder ein System mit voraufgezeichneten Nachrichten ‚Äì der virtuelle Professor Shiba wird als empfindungsf√§hig dargestellt, f√§hig, seinen Sohn zu tadeln, Befehle zu erteilen und in Familienangelegenheiten einzugreifen. In der letzten Folge vollbringt dieses digitalisierte Bewusstsein die ultimative Tat: Der Computer, der Professor Shiba enth√§lt, wird mit einem Raumschiff abgeworfen und st√ºrzt in das Schiff von K√∂nigin Himika, opfert sich selbst, um Jeeg den Sieg zu erm√∂glichen.

Wie "t√∂tet" man jemanden, der bereits tot ist? Wie verarbeitet man die Trauer um einen Vater, der weiterhin von einem Terminal aus mit einem spricht? Das sind Fragen, die Go Nagai vor f√ºnfzig Jahren aufwarf, in einer Zeit, in der Computer ganze R√§ume f√ºllten und k√ºnstliche Intelligenz reine Science-Fiction war. Heute sind diese Fragen zur√ºckgekehrt, doch dieses Mal geht es nicht nur um die Handlung eines Animes ‚Äì es geht um reale Entscheidungen, die reale Menschen √ºber die digitale Zukunft ihrer Liebsten treffen
![jeeg.jpg](jeeg.jpg)
[Bild von digilander.libero.it, Hiroshi vor dem Computer mit der Essenz seines Vaters](https://digilander.libero.it/robottoni/serietv/robots/jeeg/images/computer.jpg)
## Die schmale Gratwanderung

Grief Tech ist nicht von Natur aus b√∂se, noch sollten diejenigen, die sie nutzen, verurteilt werden. Wie die [Geschichte von Sheila Srivastava](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots) zeigt, einer Produktdesignerin in Delhi, die ChatGPT nutzte, um Gespr√§che mit ihrer 2023 verstorbenen Gro√ümutter zu simulieren. Es hatte kein letztes Gespr√§ch gegeben, keinen ruhigen Abschied. Nur einen dumpfen, anhaltenden Schmerz. Ein Jahr sp√§ter begann sie, einen personalisierten Chatbot zu verwenden, der die charakteristische Art ihrer Gro√ümutter simulierte, ihr Zuneigung zu zeigen ‚Äì durch Fragen, was sie gegessen hatte, Ratschl√§ge, eine Jacke mitzunehmen. Eines Tages schickte der Bot: "Guten Morgen, Beta üå∏ Hast du heute gegessen? Ich habe an dein gro√ües Projekt gedacht. Denk daran, Pausen zu machen, okay? Und zieh eine Jacke an, es ist kalt drau√üen." Srivastava weinte. "Das war sie. Oder so √§hnlich, dass mein Herz den Unterschied nicht erkennen konnte." F√ºr sie ersetzte der Bot nicht ihre Gro√ümutter, aber er gab ihr etwas, was sie nicht hatte: ein Gef√ºhl des Abschlusses, ein paar letzte imaginierte Worte, eine M√∂glichkeit, die Essenz ihrer Bindung am Leben zu erhalten.

Diese Werkzeuge agieren in einer Grauzone, in der Trost und Abh√§ngigkeit verschwimmen, in der Erinnerung und Simulation sich √ºberschneiden, in der pers√∂nliche Trauer auf Unternehmensgewinn trifft. [Wie eine k√ºrzlich in Social Sciences ver√∂ffentlichte Studie feststellt](https://www.mdpi.com/2076-0760/13/4/208), besteht die Gefahr eines "Trauer-Universalismus" ‚Äì die Annahme, dass es eine "richtige" Art gibt, Verlust zu verarbeiten, die universell gilt. Die Realit√§t ist, dass Trauer "in Farben kommt" ‚Äì sie ist komplex, kulturell verortet, zutiefst pers√∂nlich.

Die Frage ist nicht, ob diese Werkzeuge existieren sollten, sondern wie sie existieren sollten. Mit welchen Schutzma√ünahmen? Mit welcher Aufsicht? Mit welchem Bewusstsein f√ºr die psychologischen und sozialen Konsequenzen? [Katarzyna Nowaczyk-Basi≈Ñska schlie√üt](https.www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) mit einer ebenso einfachen wie dringenden Beobachtung: "Wir m√ºssen jetzt anfangen, dar√ºber nachzudenken, wie wir die sozialen und psychologischen Risiken der digitalen Unsterblichkeit mindern k√∂nnen, denn die Technologie ist bereits da."

Und tats√§chlich ist sie das. Wir diskutieren nicht dar√ºber, ob wir die B√ºchse der Pandora √∂ffnen sollen ‚Äì wir haben sie bereits ge√∂ffnet. Jetzt geht es darum zu entscheiden, was wir mit dem tun, was herausgekommen ist. W√§hrend Eugenia Kuyda weiterhin √ºber ihre Sch√∂pfung nachdenkt und ihre eigenen Worte aus dem Jahr 2018 zitiert: "Es ist definitiv die Zukunft, und ich bin immer f√ºr die Zukunft. Aber ist es wirklich das, was uns n√ºtzt? Ist es das Loslassen, das dich zwingt, wirklich alles zu f√ºhlen? Oder ist es nur, eine tote Person auf deinem Dachboden zu haben? Wo ist die Grenze? Wo sind wir? Es spielt mit deinem Kopf." Vielleicht ist die wichtigste Frage nicht, wohin uns diese Technologie f√ºhren wird, sondern ob wir bereit sind, uns ehrlich dem zu stellen, was sie uns bereits antut ‚Äì und was wir ihr erlauben, mit unserer grundlegendsten und universellsten Beziehung zu tun: der mit dem Tod selbst.