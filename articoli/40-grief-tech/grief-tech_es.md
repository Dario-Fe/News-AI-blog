---
tags: ["Ethics & Society", "Business", "Generative AI"]
date: 2025-10-27
author: "Dario Ferrero"
---

# Chatbots del m√°s all√°: La grief tech, entre el dolor y el negocio
![tomba-bancomat.jpg](tomba-bancomat.jpg)


*El 28 de noviembre de 2015, en Mosc√∫, un Jeep a toda velocidad atropella a Roman Mazurenko. Tiene apenas 34 a√±os, es un emprendedor tecnol√≥gico y una figura legendaria en los c√≠rculos culturales de la ciudad. Eugenia Kuyda llega al hospital poco antes de que su amigo muera, perdiendo por pocos instantes la oportunidad de hablar con √©l por √∫ltima vez. En los tres meses siguientes, [Kuyda recopila miles de mensajes](https://thereader.mitpress.mit.edu/chatting-with-the-dead-chatbots/) que Roman hab√≠a intercambiado con amigos y familiares: unas 8.000 l√≠neas de texto que capturaban su forma √∫nica de expresarse, sus frases idiosincr√°sicas, incluso los dejos ling√º√≠sticos debidos a una leve dislexia.*

Eugenia es ella misma emprendedora y desarrolladora de software. Su empresa, Luka, se dedica a los chatbots y a la inteligencia artificial conversacional. Y as√≠, en lugar de limitarse a releer obsesivamente esos mensajes como har√≠a cualquier otra persona, decide hacer algo que suena inquietantemente familiar para quien haya visto el episodio "Be Right Back" de Black Mirror: [los introduce en un algoritmo para crear un bot que simule a Roman](https://i-d.co/article/black-mirror-artificial-intelligence-roman-mazurenko/).

Al principio es solo un archivo sofisticado, una especie de motor de b√∫squeda que recupera textos existentes seg√∫n el tema de la conversaci√≥n. Pero con la evoluci√≥n de los modelos de IA generativa, esa simple base de datos se transforma en algo m√°s perturbador: un verdadero chatbot capaz de generar respuestas nuevas, interpretando el estilo y los pensamientos de Roman. Los amigos que prueban el bot encuentran [la similitud inquietante](https://www.cbc.ca/documentaries/the-nature-of-things/after-her-best-friend-died-this-programmer-created-an-ai-chatbot-from-his-texts-to-talk-to-him-again-1.6252286). Muchos lo usan para decirle cosas que no tuvieron tiempo de expresar cuando estaba vivo. Como cuenta la propia Kuyda en una entrevista, esos mensajes eran todos "sobre el amor, o para decirle algo que no hab√≠an tenido tiempo de decirle". De ese duelo privado, transformado en experimento tecnol√≥gico, nacer√° Replika, una aplicaci√≥n que hoy cuenta con m√°s de 10 millones de usuarios y que permite a cualquiera crear un compa√±ero de IA que aprende a replicar su propia personalidad, o la de otra persona.

## La industria digital del m√°s all√°

La historia de Roman Mazurenko ya no es una excepci√≥n. Es el prototipo de toda una industria emergente que [los investigadores de la Universidad de Cambridge Tomasz Hollanek y Katarzyna Nowaczyk-Basi≈Ñska](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) han bautizado como "industria de la vida despu√©s de la muerte digital". Los t√©rminos para describir estas herramientas se multiplican: griefbot, deadbot, ghostbot, thanabots. Todos convergen hacia el mismo concepto: chatbots basados en la huella digital de los difuntos que permiten a los vivos seguir "hablando" con quienes han perdido.

Las plataformas proliferan con una velocidad que recuerda m√°s al mercado de los NFT que al de los servicios de apoyo psicol√≥gico. [Project December](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates) ofrece "conversaciones con los muertos" por 10 d√≥lares por 500 intercambios de mensajes. Seance AI propone versiones gratuitas de texto y versiones de voz de pago. HereAfter AI permite pregrabar tu propio chatbot, una especie de testamento digital parlante. [En China, se pueden crear avatares de los seres queridos fallecidos por apenas 3 d√≥lares](https://www.vml.com/insight/grief-tech), usando solo 30 segundos de material audiovisual. La empresa SenseTime lleg√≥ incluso a crear un avatar de su fundador Tang Xiao'ou, fallecido en diciembre de 2023, que pronunci√≥ un discurso en la asamblea general de socios en marzo de 2024.

You, Only Virtual (YOV) va m√°s all√°, declarando audazmente que su tecnolog√≠a podr√≠a "eliminar por completo el duelo". El mercado de la IA dedicado a la compa√±√≠a, que incluye pero no se limita a los griefbots, [fue valorado en 2.800 millones de d√≥lares en 2024](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates) y se prev√© que alcance los 9.500 millones en 2028. Como en los escenarios m√°s dist√≥picos de "Upload" o "San Junipero" ‚Äîpor citar algo menos mainstream que Black Mirror‚Äî estamos asistiendo a la comercializaci√≥n sistem√°tica de la inmortalidad digital.

## El duelo congelado

¬øPero qu√© piensan los psic√≥logos? Las voces de los expertos son un√°nimes al expresar cautela, aunque no necesariamente condena. La cuesti√≥n central gira en torno a un concepto fundamental: el proceso natural de elaboraci√≥n del duelo. La [Dra. Sarika Boora](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), psic√≥loga y directora de Psyche and Beyond en Delhi, advierte que la grief tech "puede retrasar el proceso de elaboraci√≥n del duelo" manteniendo a las personas en un estado de negaci√≥n prolongada.

El problema, seg√∫n [un estudio interdisciplinario publicado en Frontiers in Human Dynamics](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full), es que tradicionalmente la elaboraci√≥n del duelo implica la aceptaci√≥n de la ausencia de la persona amada, permitiendo a los individuos procesar las emociones y avanzar hacia la curaci√≥n. La neuroplasticidad ‚Äîla capacidad del cerebro para adaptarse‚Äî juega un papel cr√≠tico en la integraci√≥n de la p√©rdida, permitiendo a las personas reconstruir sus vidas con el tiempo. Interactuar con un griefbot corre el riesgo de interrumpir esta progresi√≥n natural, creando una ilusi√≥n de presencia continuada que podr√≠a impedir enfrentarse plenamente a la realidad de la p√©rdida.

[NaYeon Yang y Greta J. Khanna](https://journals.sagepub.com/doi/10.1177/00110000251352568), en su art√≠culo "AI and Technology in Grief Support: Clinical Implications and Ethical Considerations", subrayan c√≥mo estas tecnolog√≠as pueden transformar el duelo en un bucle, donde el dolor nunca se resuelve, sino que muta en dependencia. Los usuarios podr√≠an retrasar indefinidamente la aceptaci√≥n, recurriendo continuamente a fantasmas digitales con la esperanza de un cierre que nunca llegar√°. La ilusi√≥n de presencia extiende el limbo emocional en lugar de resolverlo.

Sin embargo, la cuesti√≥n no es tan un√≠voca. [Un estudio reciente en el blog del Instituto de Derechos Humanos de la Universidad de Alabama](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/) se√±ala que algunos chatbots podr√≠an, de hecho, ayudar a las personas a afrontar el duelo traum√°tico, la p√©rdida ambigua o la inhibici√≥n emocional, pero su uso debe ser contextualizado y limitado en el tiempo. La recomendaci√≥n de la Dra. Boora es clara: "Una forma saludable de utilizar la grief tech es despu√©s de haber elaborado el duelo, despu√©s de haber alcanzado la aceptaci√≥n y haber vuelto a su estado funcional normal". En otras palabras, estas herramientas podr√≠an tener valor como apoyo a la memoria, pero solo despu√©s de que el proceso de curaci√≥n ya est√© en marcha, no como sustituto del duelo mismo.
![chatbot-roman.jpg](chatbot-roman.jpg)
[Imagen de repubblica.it](https://www.repubblica.it/tecnologia/2016/10/10/news/_parla_con_lui_roman_muore_in_un_incidente_eugenia_maga_del_software_usa_i_loro_dialoghi_per_creare_un_suo_alter_eg-149440494/)

## El fantasma en la m√°quina

Joseph Weizenbaum, cient√≠fico inform√°tico del MIT, descubri√≥ ya en 1966 algo inquietante con ELIZA, su rudimentario chatbot basado en simples patrones de respuesta. Los usuarios, aun sabiendo que interactuaban con un programa primitivo, le atribu√≠an inteligencia y emociones, confi√°ndose a la m√°quina como si fuera un terapeuta real. [La conclusi√≥n de Weizenbaum](https://www.calcalistech.com/ctechnews/article/hycchvgjge) ‚Äîque los bots pueden inducir "pensamiento delirante en personas perfectamente normales"‚Äî sigue siendo fundamental en los estudios sobre la interacci√≥n hombre-m√°quina. Es lo que hoy llamamos "efecto ELIZA", y con los modernos modelos de lenguaje grandes se ha amplificado exponencialmente.

El problema es que estos chatbots alcanzan una precisi√≥n de alrededor del 70%, lo suficientemente alta como para parecer convincentes, pero lo bastante baja como para producir lo que los expertos llaman "artefactos": frases no caracter√≠sticas, alucinaciones, lenguaje de relleno, clich√©s. [Como se√±alan Hollanek y Nowaczyk-Basi≈Ñska](https://link.springer.com/chapter/10.1007/978-3-031-90723-4_40), los algoritmos podr√≠an estar dise√±ados para optimizar las interacciones, maximizando el tiempo que una persona en duelo pasa con el chatbot, asegur√°ndose suscripciones a largo plazo. Estos algoritmos podr√≠an incluso modificar sutilmente la personalidad del bot con el tiempo para hacerlo m√°s agradable, creando una caricatura atractiva en lugar de un reflejo preciso del difunto.

En un art√≠culo publicado en Philosophy & Technology, los dos investigadores de Cambridge presentan escenarios especulativos que ilustran los peligros concretos. En uno, una empresa ficticia llamada "MaNana" permite simular a la abuela fallecida sin el consentimiento del "donante de datos". El usuario, inicialmente impresionado y reconfortado, comienza a recibir publicidad una vez finalizada la "prueba premium". Imagina pedirle a la abuela digital una receta y recibir, junto con sus consejos culinarios, sugerencias patrocinadas para pedir carbonara de Uber Eats. [Esto no es ciencia ficci√≥n](https://aiconnectnetwork.com/ai-griefbots-mental-health-apps-data-privacy/), ya ha ocurrido en versiones beta de algunos servicios.

## ¬øQui√©n es el due√±o del muerto?

Las cuestiones √©ticas se multiplican como las cabezas de la hidra. ¬øQui√©n tiene derecho a crear y controlar estos avatares digitales? Como explica [Hollanek en un comunicado de prensa](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) de la Universidad de Cambridge, "es vital que los servicios de vida despu√©s de la muerte digital consideren los derechos y el consentimiento no solo de aquellos a quienes recrean, sino de quienes tendr√°n que interactuar con las simulaciones". El problema del consentimiento opera en tres niveles distintos. Primero, ¬øel "donante de datos" ‚Äîla persona fallecida‚Äî consinti√≥ alguna vez en ser recreado? Segundo, ¬øqui√©n posee los datos despu√©s de la muerte? Tercero, ¬øqu√© sucede con quienes no quieren interactuar con estos simulacros pero se ven bombardeados con mensajes?

[Uno de los escenarios explorados por Hollanek y Nowaczyk-Basi≈Ñska](https://scitechdaily.com/cambridge-experts-warn-ai-deadbots-could-digitally-haunt-loved-ones-from-beyond-the-grave/) cuenta la historia de un padre anciano que suscribe en secreto una suscripci√≥n de veinte a√±os a un deadbot de s√≠ mismo, con la esperanza de que consuele a sus hijos adultos y permita a sus nietos conocerlo. Despu√©s de su muerte, el servicio se activa. Un hijo se niega a interactuar y recibe una r√°faga de correos electr√≥nicos con la voz del padre muerto, lo que los investigadores llaman "persecuci√≥n digital". El otro hijo interact√∫a, pero termina emocionalmente agotado y atormentado por el sentimiento de culpa sobre el destino del deadbot. ¬øC√≥mo se "retira" un bot que simula a tu madre? ¬øC√≥mo se mata a alguien que ya est√° muerto?

Los investigadores de Cambridge proponen lo que definen como "ceremonias de jubilaci√≥n", rituales digitales para desactivar los deadbots de manera digna. Podr√≠an ser funerales digitales u otros tipos de ceremonia seg√∫n el contexto social. [Debra Bassett](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), consultora para la vida despu√©s de la muerte digital, propone en sus estudios una orden DDNR ‚Äî"digital do-not-reanimate"‚Äî una cl√°usula testamentaria que proh√≠ba legalmente la resurrecci√≥n digital p√≥stuma no consentida. Los llama "zombis digitales", y el t√©rmino no podr√≠a ser m√°s apropiado.

## El mercado de la vulnerabilidad

La comercializaci√≥n del duelo plantea interrogantes a√∫n m√°s profundos. A diferencia de la industria funeraria tradicional ‚Äîque monetiza la muerte a trav√©s de servicios puntuales como ata√∫des, cremaciones, ceremonias‚Äî [los griefbots operan con modelos de suscripci√≥n o pago por minuto](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/). Por lo tanto, las empresas tienen incentivos financieros para mantener a las personas en duelo constantemente involucradas con sus servicios. [Como se√±ala un estudio sobre el mercado de la grief tech](https://link.springer.com/chapter/10.1007/978-3-031-90723-4_40), este modelo econ√≥mico es fundamentalmente diferente ‚Äîy potencialmente m√°s depredador‚Äî que el de los servicios tradicionales relacionados con la muerte.

El mercado de la compa√±√≠a de IA, que incluye chatbots rom√°nticos, terap√©uticos y de apoyo al duelo, tiene modelos de precios que van de 10 a 40 d√≥lares al mes. Como se√±ala [Ewan Morrison en Psychology Today](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates), estamos monetizando la soledad y la vulnerabilidad emocional. El efecto ELIZA se profundiza, enmascarando la fragmentaci√≥n social con la ilusi√≥n del cuidado mientras nos aleja a√∫n m√°s los unos de los otros.

[Paula Kiel de la NYU-London](https://www.calcalistech.com/ctechnews/article/hycchvgjge) ofrece una perspectiva diferente: "Lo que hace tan atractiva a esta industria es que, como cada generaci√≥n, estamos buscando formas de preservar partes de nosotros mismos. Estamos encontrando consuelo en la inevitabilidad de la muerte a trav√©s del lenguaje de la ciencia y la tecnolog√≠a". Pero este consuelo tiene un precio, no solo econ√≥mico, sino psicol√≥gico y social.

## La autenticidad imposible

Luego est√° la cuesti√≥n filos√≥fica de la autenticidad. Los chatbots no tienen la capacidad de evolucionar y crecer como los seres humanos. Como explica [un an√°lisis del Instituto de Derechos Humanos](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/) de la Universidad de Alabama, "un problema al realizar acciones a perpetuidad es que las personas muertas son productos de su tiempo. No cambian lo que quieren cuando el mundo cambia". Incluso si el crecimiento se implementara en el algoritmo, no habr√≠a garant√≠a de que reflejara c√≥mo una persona habr√≠a cambiado realmente.

Los griefbots preservan la presencia digital de una persona fallecida de maneras que podr√≠an volverse problem√°ticas o irrelevantes con el tiempo. Si Milton Hershey, que en su testamento dej√≥ disposiciones precisas sobre c√≥mo deb√≠a usarse su herencia a perpetuidad, estuviera vivo hoy, ¬ømodificar√≠a esas disposiciones para reflejar los cambios del mundo? La diferencia crucial es que los testamentos y las herencias son intr√≠nsecamente limitados en alcance y duraci√≥n. Los griefbots, por su naturaleza, tienen el potencial de persistir indefinidamente, amplificando el da√±o potencial a la reputaci√≥n o la memoria de una persona.

La memoria humana ya es un narrador poco fiable, fluida, moldeada m√°s por la emoci√≥n que por los hechos. [Estudios recientes demuestran](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots) que las im√°genes y los v√≠deos modificados con IA pueden implantar falsos recuerdos y distorsionar los reales. ¬øQu√© sucede cuando alimentamos el duelo en una m√°quina y recibimos de vuelta una versi√≥n de una persona que nunca existi√≥ por completo? Como escribi√≥ la periodista tecnol√≥gica Vauhini Vara, finalista del Pulitzer que explor√≥ personalmente las profundidades emocionales y √©ticas del duelo y la IA: "Tiene sentido que la gente recurra a cualquier recurso disponible para buscar consuelo, y tambi√©n tiene sentido que las empresas est√©n interesadas en explotar a las personas en un momento de vulnerabilidad".
![lutto-cervello.jpg](lutto-cervello.jpg)
[Imagen de frontiersin.org, el cerebro en duelo puede dividirse en cuatro categor√≠as seg√∫n si el perfil de actividad es similar o diferente a la tristeza y la depresi√≥n](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full)

## ¬øHacia una regulaci√≥n?

[Hollanek y Nowaczyk-Basi≈Ñska recomiendan](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) restricciones de edad para los deadbots y lo que llaman "transparencia significativa", asegur√°ndose de que los usuarios sean constantemente conscientes de que est√°n interactuando con una IA, con advertencias similares a las actuales sobre contenidos que podr√≠an causar crisis epil√©pticas. Tambi√©n sugieren clasificar los deadbots como dispositivos m√©dicos para abordar cuestiones de salud mental, especialmente para grupos vulnerables como los ni√±os.

[Un estudio reciente en Frontiers in Human Dynamics](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full) subraya que los desarrolladores de tecnolog√≠as relacionadas con el duelo tienen una inmensa responsabilidad en determinar c√≥mo los usuarios se involucran emocionalmente con la presencia simulada. Si bien la continuidad emocional puede aliviar el malestar inicial, la hiperinmersi√≥n corre el riesgo de atrapamiento psicol√≥gico. Los desarrolladores deben priorizar principios de dise√±o √©ticos, como incorporar l√≠mites de uso, indicaciones reflexivas y puntos de control emocionales que gu√≠en a los usuarios hacia la recuperaci√≥n en lugar de la dependencia.

La colaboraci√≥n con psic√≥logos y expertos en duelo deber√≠a informar las caracter√≠sticas de la interfaz para garantizar que faciliten, en lugar de sustituir, el proceso de elaboraci√≥n del duelo. Adem√°s, la gesti√≥n de datos sensibles ‚Äîvoz, personalidad, patrones de comportamiento‚Äî requiere est√°ndares rigurosos de cifrado, privacidad y protocolos de transparencia. El uso indebido de estos datos no solo viola la dignidad digital, sino que tambi√©n puede contribuir a da√±os emocionales para los usuarios y las familias.

## El profesor Shiba y la profec√≠a de Go Nagai

Sin embargo, la idea de transferir la conciencia de una persona a una computadora para preservar su presencia m√°s all√° de la muerte no es una fantas√≠a nacida con la llegada de los grandes modelos de lenguaje. [En 1975, Go Nagai introdujo en el manga y anime "Jeeg Robot de Acero"](https://it.wikipedia.org/wiki/Jeeg_robot_d'acciaio) lo que probablemente fue la primera representaci√≥n de la "carga de la mente" en una serie animada. El profesor Shiba, arque√≥logo y cient√≠fico asesinado en el primer episodio, transfiere su conciencia y memoria a una computadora en la Base Anti-At√≥mica, continuando as√≠ guiando e instruyendo a su hijo Hiroshi en las batallas contra el antiguo imperio Yamatai.

No es un simple archivo de datos o un sistema de mensajes pregrabados‚Äîel profesor Shiba virtual se presenta como consciente, capaz de rega√±ar a su hijo, dar √≥rdenes e intervenir en problemas familiares. En el episodio final, esta conciencia digitalizada realiza el acto supremo: la computadora que contiene al profesor Shiba es eyectada mediante una nave espacial y choca contra la nave de la reina Himika, sacrific√°ndose para permitir que Jeeg gane.

¬øC√≥mo se "mata" a alguien que ya est√° muerto? ¬øC√≥mo se elabora el duelo por un padre que contin√∫a habl√°ndote desde una terminal? Son preguntas que Go Nagai plante√≥ hace cincuenta a√±os, en una √©poca en la que las computadoras ocupaban habitaciones enteras y la inteligencia artificial era pura ciencia ficci√≥n. Hoy esas preguntas han vuelto, pero esta vez no solo conciernen a la trama de un anime‚Äîconciernen a decisiones reales que personas reales est√°n tomando sobre el futuro digital de sus seres queridos.
![jeeg.jpg](jeeg.jpg)
[Imagen de digilander.libero.it, Hiroshi frente a la computadora con la esencia de su padre.](https://digilander.libero.it/robottoni/serietv/robots/jeeg/images/computer.jpg)

## La delgada l√≠nea

La grief tech no es intr√≠nsecamente malvada, ni quienes la utilizan deben ser juzgados. Como cuenta [la historia de Sheila Srivastava](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), dise√±adora de productos en Delhi, que us√≥ ChatGPT para simular conversaciones con su abuela fallecida en 2023. No hab√≠a habido una conversaci√≥n final, ni una despedida tranquila. Solo un dolor sordo y persistente. Un a√±o despu√©s, comenz√≥ a usar un chatbot personalizado que simulaba la forma caracter√≠stica de su abuela de demostrarle afecto, a trav√©s de preguntas sobre qu√© hab√≠a comido, consejos de llevar una chaqueta. Un d√≠a, el bot envi√≥: "Buenos d√≠as beta üå∏ ¬øHas comido hoy? Estaba pensando en tu gran proyecto. Recuerda tomar descansos, ¬øvale? Y ponte una chaqueta, hace fr√≠o afuera". Srivastava llor√≥. "Era ella. O lo suficientemente parecida como para que mi coraz√≥n no pudiera distinguir la diferencia". Para ella, el bot no reemplaz√≥ a su abuela, pero le dio algo que no hab√≠a tenido: una sensaci√≥n de cierre, unas √∫ltimas palabras imaginadas, una forma de mantener viva la esencia de su v√≠nculo.

Estas herramientas operan en una zona gris donde el consuelo y la dependencia se confunden, donde la memoria y la simulaci√≥n se superponen, donde el duelo personal se encuentra con el beneficio empresarial. [Como observa un estudio reciente publicado en Social Sciences](https://www.mdpi.com/2076-0760/13/4/208), existe el riesgo de "universalismo del duelo", la suposici√≥n de que existe una forma "correcta" de procesar la p√©rdida que se aplica universalmente. La realidad es que el duelo "llega en colores", es complejo, culturalmente situado, profundamente personal.

La cuesti√≥n no es si estas herramientas deber√≠an existir, sino c√≥mo deber√≠an existir. ¬øCon qu√© salvaguardas? ¬øCon qu√© supervisi√≥n? ¬øCon qu√© conciencia de las consecuencias psicol√≥gicas y sociales? [Katarzyna Nowaczyk-Basi≈Ñska concluye](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) con una observaci√≥n tan simple como urgente: "Debemos empezar a pensar ahora en c√≥mo mitigar los riesgos sociales y psicol√≥gicos de la inmortalidad digital, porque la tecnolog√≠a ya est√° aqu√≠".

Y, de hecho, lo est√°. No estamos debatiendo si abrir la caja de Pandora, ya la hemos abierto. Ahora se trata de decidir qu√© hacer con lo que ha salido. Mientras Eugenia Kuyda sigue reflexionando sobre su creaci√≥n, citando sus propias palabras de 2018: "Es definitivamente el futuro y siempre estoy a favor del futuro. ¬øPero es realmente lo que nos es √∫til? ¬øEs dejar ir, oblig√°ndote a sentir realmente todo? ¬øO es solo tener a una persona muerta en tu √°tico? ¬øD√≥nde est√° la l√≠nea? ¬øD√≥nde estamos nosotros? Te l√≠a la cabeza". Quiz√°s la pregunta m√°s importante no es a d√≥nde nos llevar√° esta tecnolog√≠a, sino si estamos dispuestos a enfrentar honestamente lo que ya nos est√° haciendo, y lo que estamos permitiendo que haga de nuestra relaci√≥n m√°s fundamental y universal: la que tenemos con la muerte misma.