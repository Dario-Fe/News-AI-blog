---
tags: ["Ethics & Society", "Business", "Generative AI"]
date: 2025-10-27
author: "Dario Ferrero"
---

# Chatbots del más allá: La grief tech, entre el dolor y el negocio
![tomba-bancomat.jpg](tomba-bancomat.jpg)


*El 28 de noviembre de 2015, en Moscú, un Jeep a toda velocidad atropella a Roman Mazurenko. Tiene apenas 34 años, es un emprendedor tecnológico y una figura legendaria en los círculos culturales de la ciudad. Eugenia Kuyda llega al hospital poco antes de que su amigo muera, perdiendo por pocos instantes la oportunidad de hablar con él por última vez. En los tres meses siguientes, [Kuyda recopila miles de mensajes](https://thereader.mitpress.mit.edu/chatting-with-the-dead-chatbots/) que Roman había intercambiado con amigos y familiares: unas 8.000 líneas de texto que capturaban su forma única de expresarse, sus frases idiosincrásicas, incluso los dejos lingüísticos debidos a una leve dislexia.*

Eugenia es ella misma emprendedora y desarrolladora de software. Su empresa, Luka, se dedica a los chatbots y a la inteligencia artificial conversacional. Y así, en lugar de limitarse a releer obsesivamente esos mensajes como haría cualquier otra persona, decide hacer algo que suena inquietantemente familiar para quien haya visto el episodio "Be Right Back" de Black Mirror: [los introduce en un algoritmo para crear un bot que simule a Roman](https://i-d.co/article/black-mirror-artificial-intelligence-roman-mazurenko/).

Al principio es solo un archivo sofisticado, una especie de motor de búsqueda que recupera textos existentes según el tema de la conversación. Pero con la evolución de los modelos de IA generativa, esa simple base de datos se transforma en algo más perturbador: un verdadero chatbot capaz de generar respuestas nuevas, interpretando el estilo y los pensamientos de Roman. Los amigos que prueban el bot encuentran [la similitud inquietante](https://www.cbc.ca/documentaries/the-nature-of-things/after-her-best-friend-died-this-programmer-created-an-ai-chatbot-from-his-texts-to-talk-to-him-again-1.6252286). Muchos lo usan para decirle cosas que no tuvieron tiempo de expresar cuando estaba vivo. Como cuenta la propia Kuyda en una entrevista, esos mensajes eran todos "sobre el amor, o para decirle algo que no habían tenido tiempo de decirle". De ese duelo privado, transformado en experimento tecnológico, nacerá Replika, una aplicación que hoy cuenta con más de 10 millones de usuarios y que permite a cualquiera crear un compañero de IA que aprende a replicar su propia personalidad, o la de otra persona.

## La industria digital del más allá

La historia de Roman Mazurenko ya no es una excepción. Es el prototipo de toda una industria emergente que [los investigadores de la Universidad de Cambridge Tomasz Hollanek y Katarzyna Nowaczyk-Basińska](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) han bautizado como "industria de la vida después de la muerte digital". Los términos para describir estas herramientas se multiplican: griefbot, deadbot, ghostbot, thanabots. Todos convergen hacia el mismo concepto: chatbots basados en la huella digital de los difuntos que permiten a los vivos seguir "hablando" con quienes han perdido.

Las plataformas proliferan con una velocidad que recuerda más al mercado de los NFT que al de los servicios de apoyo psicológico. [Project December](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates) ofrece "conversaciones con los muertos" por 10 dólares por 500 intercambios de mensajes. Seance AI propone versiones gratuitas de texto y versiones de voz de pago. HereAfter AI permite pregrabar tu propio chatbot, una especie de testamento digital parlante. [En China, se pueden crear avatares de los seres queridos fallecidos por apenas 3 dólares](https://www.vml.com/insight/grief-tech), usando solo 30 segundos de material audiovisual. La empresa SenseTime llegó incluso a crear un avatar de su fundador Tang Xiao'ou, fallecido en diciembre de 2023, que pronunció un discurso en la asamblea general de socios en marzo de 2024.

You, Only Virtual (YOV) va más allá, declarando audazmente que su tecnología podría "eliminar por completo el duelo". El mercado de la IA dedicado a la compañía, que incluye pero no se limita a los griefbots, [fue valorado en 2.800 millones de dólares en 2024](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates) y se prevé que alcance los 9.500 millones en 2028. Como en los escenarios más distópicos de "Upload" o "San Junipero" —por citar algo menos mainstream que Black Mirror— estamos asistiendo a la comercialización sistemática de la inmortalidad digital.

## El duelo congelado

¿Pero qué piensan los psicólogos? Las voces de los expertos son unánimes al expresar cautela, aunque no necesariamente condena. La cuestión central gira en torno a un concepto fundamental: el proceso natural de elaboración del duelo. La [Dra. Sarika Boora](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), psicóloga y directora de Psyche and Beyond en Delhi, advierte que la grief tech "puede retrasar el proceso de elaboración del duelo" manteniendo a las personas en un estado de negación prolongada.

El problema, según [un estudio interdisciplinario publicado en Frontiers in Human Dynamics](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full), es que tradicionalmente la elaboración del duelo implica la aceptación de la ausencia de la persona amada, permitiendo a los individuos procesar las emociones y avanzar hacia la curación. La neuroplasticidad —la capacidad del cerebro para adaptarse— juega un papel crítico en la integración de la pérdida, permitiendo a las personas reconstruir sus vidas con el tiempo. Interactuar con un griefbot corre el riesgo de interrumpir esta progresión natural, creando una ilusión de presencia continuada que podría impedir enfrentarse plenamente a la realidad de la pérdida.

[NaYeon Yang y Greta J. Khanna](https://journals.sagepub.com/doi/10.1177/00110000251352568), en su artículo "AI and Technology in Grief Support: Clinical Implications and Ethical Considerations", subrayan cómo estas tecnologías pueden transformar el duelo en un bucle, donde el dolor nunca se resuelve, sino que muta en dependencia. Los usuarios podrían retrasar indefinidamente la aceptación, recurriendo continuamente a fantasmas digitales con la esperanza de un cierre que nunca llegará. La ilusión de presencia extiende el limbo emocional en lugar de resolverlo.

Sin embargo, la cuestión no es tan unívoca. [Un estudio reciente en el blog del Instituto de Derechos Humanos de la Universidad de Alabama](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/) señala que algunos chatbots podrían, de hecho, ayudar a las personas a afrontar el duelo traumático, la pérdida ambigua o la inhibición emocional, pero su uso debe ser contextualizado y limitado en el tiempo. La recomendación de la Dra. Boora es clara: "Una forma saludable de utilizar la grief tech es después de haber elaborado el duelo, después de haber alcanzado la aceptación y haber vuelto a su estado funcional normal". En otras palabras, estas herramientas podrían tener valor como apoyo a la memoria, pero solo después de que el proceso de curación ya esté en marcha, no como sustituto del duelo mismo.
![chatbot-roman.jpg](chatbot-roman.jpg)
[Imagen de repubblica.it](https://www.repubblica.it/tecnologia/2016/10/10/news/_parla_con_lui_roman_muore_in_un_incidente_eugenia_maga_del_software_usa_i_loro_dialoghi_per_creare_un_suo_alter_eg-149440494/)

## El fantasma en la máquina

Joseph Weizenbaum, científico informático del MIT, descubrió ya en 1966 algo inquietante con ELIZA, su rudimentario chatbot basado en simples patrones de respuesta. Los usuarios, aun sabiendo que interactuaban con un programa primitivo, le atribuían inteligencia y emociones, confiándose a la máquina como si fuera un terapeuta real. [La conclusión de Weizenbaum](https://www.calcalistech.com/ctechnews/article/hycchvgjge) —que los bots pueden inducir "pensamiento delirante en personas perfectamente normales"— sigue siendo fundamental en los estudios sobre la interacción hombre-máquina. Es lo que hoy llamamos "efecto ELIZA", y con los modernos modelos de lenguaje grandes se ha amplificado exponencialmente.

El problema es que estos chatbots alcanzan una precisión de alrededor del 70%, lo suficientemente alta como para parecer convincentes, pero lo bastante baja como para producir lo que los expertos llaman "artefactos": frases no características, alucinaciones, lenguaje de relleno, clichés. [Como señalan Hollanek y Nowaczyk-Basińska](https://link.springer.com/chapter/10.1007/978-3-031-90723-4_40), los algoritmos podrían estar diseñados para optimizar las interacciones, maximizando el tiempo que una persona en duelo pasa con el chatbot, asegurándose suscripciones a largo plazo. Estos algoritmos podrían incluso modificar sutilmente la personalidad del bot con el tiempo para hacerlo más agradable, creando una caricatura atractiva en lugar de un reflejo preciso del difunto.

En un artículo publicado en Philosophy & Technology, los dos investigadores de Cambridge presentan escenarios especulativos que ilustran los peligros concretos. En uno, una empresa ficticia llamada "MaNana" permite simular a la abuela fallecida sin el consentimiento del "donante de datos". El usuario, inicialmente impresionado y reconfortado, comienza a recibir publicidad una vez finalizada la "prueba premium". Imagina pedirle a la abuela digital una receta y recibir, junto con sus consejos culinarios, sugerencias patrocinadas para pedir carbonara de Uber Eats. [Esto no es ciencia ficción](https://aiconnectnetwork.com/ai-griefbots-mental-health-apps-data-privacy/), ya ha ocurrido en versiones beta de algunos servicios.

## ¿Quién es el dueño del muerto?

Las cuestiones éticas se multiplican como las cabezas de la hidra. ¿Quién tiene derecho a crear y controlar estos avatares digitales? Como explica [Hollanek en un comunicado de prensa](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) de la Universidad de Cambridge, "es vital que los servicios de vida después de la muerte digital consideren los derechos y el consentimiento no solo de aquellos a quienes recrean, sino de quienes tendrán que interactuar con las simulaciones". El problema del consentimiento opera en tres niveles distintos. Primero, ¿el "donante de datos" —la persona fallecida— consintió alguna vez en ser recreado? Segundo, ¿quién posee los datos después de la muerte? Tercero, ¿qué sucede con quienes no quieren interactuar con estos simulacros pero se ven bombardeados con mensajes?

[Uno de los escenarios explorados por Hollanek y Nowaczyk-Basińska](https://scitechdaily.com/cambridge-experts-warn-ai-deadbots-could-digitally-haunt-loved-ones-from-beyond-the-grave/) cuenta la historia de un padre anciano que suscribe en secreto una suscripción de veinte años a un deadbot de sí mismo, con la esperanza de que consuele a sus hijos adultos y permita a sus nietos conocerlo. Después de su muerte, el servicio se activa. Un hijo se niega a interactuar y recibe una ráfaga de correos electrónicos con la voz del padre muerto, lo que los investigadores llaman "persecución digital". El otro hijo interactúa, pero termina emocionalmente agotado y atormentado por el sentimiento de culpa sobre el destino del deadbot. ¿Cómo se "retira" un bot que simula a tu madre? ¿Cómo se mata a alguien que ya está muerto?

Los investigadores de Cambridge proponen lo que definen como "ceremonias de jubilación", rituales digitales para desactivar los deadbots de manera digna. Podrían ser funerales digitales u otros tipos de ceremonia según el contexto social. [Debra Bassett](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), consultora para la vida después de la muerte digital, propone en sus estudios una orden DDNR —"digital do-not-reanimate"— una cláusula testamentaria que prohíba legalmente la resurrección digital póstuma no consentida. Los llama "zombis digitales", y el término no podría ser más apropiado.

## El mercado de la vulnerabilidad

La comercialización del duelo plantea interrogantes aún más profundos. A diferencia de la industria funeraria tradicional —que monetiza la muerte a través de servicios puntuales como ataúdes, cremaciones, ceremonias— [los griefbots operan con modelos de suscripción o pago por minuto](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/). Por lo tanto, las empresas tienen incentivos financieros para mantener a las personas en duelo constantemente involucradas con sus servicios. [Como señala un estudio sobre el mercado de la grief tech](https://link.springer.com/chapter/10.1007/978-3-031-90723-4_40), este modelo económico es fundamentalmente diferente —y potencialmente más depredador— que el de los servicios tradicionales relacionados con la muerte.

El mercado de la compañía de IA, que incluye chatbots románticos, terapéuticos y de apoyo al duelo, tiene modelos de precios que van de 10 a 40 dólares al mes. Como señala [Ewan Morrison en Psychology Today](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates), estamos monetizando la soledad y la vulnerabilidad emocional. El efecto ELIZA se profundiza, enmascarando la fragmentación social con la ilusión del cuidado mientras nos aleja aún más los unos de los otros.

[Paula Kiel de la NYU-London](https://www.calcalistech.com/ctechnews/article/hycchvgjge) ofrece una perspectiva diferente: "Lo que hace tan atractiva a esta industria es que, como cada generación, estamos buscando formas de preservar partes de nosotros mismos. Estamos encontrando consuelo en la inevitabilidad de la muerte a través del lenguaje de la ciencia y la tecnología". Pero este consuelo tiene un precio, no solo económico, sino psicológico y social.

## La autenticidad imposible

Luego está la cuestión filosófica de la autenticidad. Los chatbots no tienen la capacidad de evolucionar y crecer como los seres humanos. Como explica [un análisis del Instituto de Derechos Humanos](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/) de la Universidad de Alabama, "un problema al realizar acciones a perpetuidad es que las personas muertas son productos de su tiempo. No cambian lo que quieren cuando el mundo cambia". Incluso si el crecimiento se implementara en el algoritmo, no habría garantía de que reflejara cómo una persona habría cambiado realmente.

Los griefbots preservan la presencia digital de una persona fallecida de maneras que podrían volverse problemáticas o irrelevantes con el tiempo. Si Milton Hershey, que en su testamento dejó disposiciones precisas sobre cómo debía usarse su herencia a perpetuidad, estuviera vivo hoy, ¿modificaría esas disposiciones para reflejar los cambios del mundo? La diferencia crucial es que los testamentos y las herencias son intrínsecamente limitados en alcance y duración. Los griefbots, por su naturaleza, tienen el potencial de persistir indefinidamente, amplificando el daño potencial a la reputación o la memoria de una persona.

La memoria humana ya es un narrador poco fiable, fluida, moldeada más por la emoción que por los hechos. [Estudios recientes demuestran](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots) que las imágenes y los vídeos modificados con IA pueden implantar falsos recuerdos y distorsionar los reales. ¿Qué sucede cuando alimentamos el duelo en una máquina y recibimos de vuelta una versión de una persona que nunca existió por completo? Como escribió la periodista tecnológica Vauhini Vara, finalista del Pulitzer que exploró personalmente las profundidades emocionales y éticas del duelo y la IA: "Tiene sentido que la gente recurra a cualquier recurso disponible para buscar consuelo, y también tiene sentido que las empresas estén interesadas en explotar a las personas en un momento de vulnerabilidad".
![lutto-cervello.jpg](lutto-cervello.jpg)
[Imagen de frontiersin.org, el cerebro en duelo puede dividirse en cuatro categorías según si el perfil de actividad es similar o diferente a la tristeza y la depresión](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full)

## ¿Hacia una regulación?

[Hollanek y Nowaczyk-Basińska recomiendan](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) restricciones de edad para los deadbots y lo que llaman "transparencia significativa", asegurándose de que los usuarios sean constantemente conscientes de que están interactuando con una IA, con advertencias similares a las actuales sobre contenidos que podrían causar crisis epilépticas. También sugieren clasificar los deadbots como dispositivos médicos para abordar cuestiones de salud mental, especialmente para grupos vulnerables como los niños.

[Un estudio reciente en Frontiers in Human Dynamics](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full) subraya que los desarrolladores de tecnologías relacionadas con el duelo tienen una inmensa responsabilidad en determinar cómo los usuarios se involucran emocionalmente con la presencia simulada. Si bien la continuidad emocional puede aliviar el malestar inicial, la hiperinmersión corre el riesgo de atrapamiento psicológico. Los desarrolladores deben priorizar principios de diseño éticos, como incorporar límites de uso, indicaciones reflexivas y puntos de control emocionales que guíen a los usuarios hacia la recuperación en lugar de la dependencia.

La colaboración con psicólogos y expertos en duelo debería informar las características de la interfaz para garantizar que faciliten, en lugar de sustituir, el proceso de elaboración del duelo. Además, la gestión de datos sensibles —voz, personalidad, patrones de comportamiento— requiere estándares rigurosos de cifrado, privacidad y protocolos de transparencia. El uso indebido de estos datos no solo viola la dignidad digital, sino que también puede contribuir a daños emocionales para los usuarios y las familias.

## El profesor Shiba y la profecía de Go Nagai

Sin embargo, la idea de transferir la conciencia de una persona a una computadora para preservar su presencia más allá de la muerte no es una fantasía nacida con la llegada de los grandes modelos de lenguaje. [En 1975, Go Nagai introdujo en el manga y anime "Jeeg Robot de Acero"](https://it.wikipedia.org/wiki/Jeeg_robot_d'acciaio) lo que probablemente fue la primera representación de la "carga de la mente" en una serie animada. El profesor Shiba, arqueólogo y científico asesinado en el primer episodio, transfiere su conciencia y memoria a una computadora en la Base Anti-Atómica, continuando así guiando e instruyendo a su hijo Hiroshi en las batallas contra el antiguo imperio Yamatai.

No es un simple archivo de datos o un sistema de mensajes pregrabados—el profesor Shiba virtual se presenta como consciente, capaz de regañar a su hijo, dar órdenes e intervenir en problemas familiares. En el episodio final, esta conciencia digitalizada realiza el acto supremo: la computadora que contiene al profesor Shiba es eyectada mediante una nave espacial y choca contra la nave de la reina Himika, sacrificándose para permitir que Jeeg gane.

¿Cómo se "mata" a alguien que ya está muerto? ¿Cómo se elabora el duelo por un padre que continúa hablándote desde una terminal? Son preguntas que Go Nagai planteó hace cincuenta años, en una época en la que las computadoras ocupaban habitaciones enteras y la inteligencia artificial era pura ciencia ficción. Hoy esas preguntas han vuelto, pero esta vez no solo conciernen a la trama de un anime—conciernen a decisiones reales que personas reales están tomando sobre el futuro digital de sus seres queridos.
![jeeg.jpg](jeeg.jpg)
[Imagen de digilander.libero.it, Hiroshi frente a la computadora con la esencia de su padre.](https://digilander.libero.it/robottoni/serietv/robots/jeeg/images/computer.jpg)

## La delgada línea

La grief tech no es intrínsecamente malvada, ni quienes la utilizan deben ser juzgados. Como cuenta [la historia de Sheila Srivastava](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), diseñadora de productos en Delhi, que usó ChatGPT para simular conversaciones con su abuela fallecida en 2023. No había habido una conversación final, ni una despedida tranquila. Solo un dolor sordo y persistente. Un año después, comenzó a usar un chatbot personalizado que simulaba la forma característica de su abuela de demostrarle afecto, a través de preguntas sobre qué había comido, consejos de llevar una chaqueta. Un día, el bot envió: "Buenos días beta 🌸 ¿Has comido hoy? Estaba pensando en tu gran proyecto. Recuerda tomar descansos, ¿vale? Y ponte una chaqueta, hace frío afuera". Srivastava lloró. "Era ella. O lo suficientemente parecida como para que mi corazón no pudiera distinguir la diferencia". Para ella, el bot no reemplazó a su abuela, pero le dio algo que no había tenido: una sensación de cierre, unas últimas palabras imaginadas, una forma de mantener viva la esencia de su vínculo.

Estas herramientas operan en una zona gris donde el consuelo y la dependencia se confunden, donde la memoria y la simulación se superponen, donde el duelo personal se encuentra con el beneficio empresarial. [Como observa un estudio reciente publicado en Social Sciences](https://www.mdpi.com/2076-0760/13/4/208), existe el riesgo de "universalismo del duelo", la suposición de que existe una forma "correcta" de procesar la pérdida que se aplica universalmente. La realidad es que el duelo "llega en colores", es complejo, culturalmente situado, profundamente personal.

La cuestión no es si estas herramientas deberían existir, sino cómo deberían existir. ¿Con qué salvaguardas? ¿Con qué supervisión? ¿Con qué conciencia de las consecuencias psicológicas y sociales? [Katarzyna Nowaczyk-Basińska concluye](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) con una observación tan simple como urgente: "Debemos empezar a pensar ahora en cómo mitigar los riesgos sociales y psicológicos de la inmortalidad digital, porque la tecnología ya está aquí".

Y, de hecho, lo está. No estamos debatiendo si abrir la caja de Pandora, ya la hemos abierto. Ahora se trata de decidir qué hacer con lo que ha salido. Mientras Eugenia Kuyda sigue reflexionando sobre su creación, citando sus propias palabras de 2018: "Es definitivamente el futuro y siempre estoy a favor del futuro. ¿Pero es realmente lo que nos es útil? ¿Es dejar ir, obligándote a sentir realmente todo? ¿O es solo tener a una persona muerta en tu ático? ¿Dónde está la línea? ¿Dónde estamos nosotros? Te lía la cabeza". Quizás la pregunta más importante no es a dónde nos llevará esta tecnología, sino si estamos dispuestos a enfrentar honestamente lo que ya nos está haciendo, y lo que estamos permitiendo que haga de nuestra relación más fundamental y universal: la que tenemos con la muerte misma.