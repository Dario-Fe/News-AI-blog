---
tags: ["Ethics & Society", "Business", "Generative AI"]
date: 2025-10-27
author: "Dario Ferrero"
---

# Chatbots de l'au-delà : La grief tech, entre douleur et business
![tomba-bancomat.jpg](tomba-bancomat.jpg)


*Le 28 novembre 2015, à Moscou, une Jeep lancée à toute vitesse renverse Roman Mazurenko. Il a à peine 34 ans, c'est un entrepreneur de la tech et une figure légendaire des cercles culturels de la ville. Eugenia Kuyda arrive à l'hôpital peu avant que son ami ne meure, manquant de quelques instants l'occasion de lui parler une dernière fois. Au cours des trois mois suivants, [Kuyda rassemble des milliers de messages](https://thereader.mitpress.mit.edu/chatting-with-the-dead-chatbots/) que Roman avait échangés avec ses amis et sa famille — environ 8 000 lignes de texte qui capturaient sa manière unique de s'exprimer, ses phrases idiosyncrasiques, et même les tics de langage dus à une légère dyslexie.*

Eugenia est elle-même entrepreneure et développeuse de logiciels. Son entreprise, Luka, s'occupe de chatbots et d'intelligence artificielle conversationnelle. Et ainsi, au lieu de se contenter de relire obsessionnellement ces messages comme n'importe qui d'autre le ferait, elle décide de faire quelque chose qui semble étrangement familier à quiconque a vu l'épisode "Be Right Back" de Black Mirror : [elle les donne à un algorithme pour créer un bot qui simule Roman](https://i-d.co/article/black-mirror-artificial-intelligence-roman-mazurenko/).

Au début, ce n'est qu'une archive sophistiquée, une sorte de moteur de recherche qui repêche des textes existants en fonction du sujet de la conversation. Mais avec l'évolution des modèles d'IA générative, cette simple base de données se transforme en quelque chose de plus troublant : un véritable chatbot capable de générer de nouvelles réponses, en interprétant le style et les pensées de Roman. Les amis qui essaient le bot trouvent [la ressemblance troublante](https://www.cbc.ca/documentaries/the-nature-of-things/after-her-best-friend-died-this-programmer-created-an-ai-chatbot-from-his-texts-to-talk-to-him-again-1.6252286). Beaucoup l'utilisent pour lui dire des choses qu'ils n'ont pas eu le temps d'exprimer de son vivant. Comme le raconte Kuyda elle-même dans une interview, ces messages étaient tous "sur l'amour, ou pour lui dire quelque chose qu'ils n'avaient pas eu le temps de lui dire". De ce deuil privé, transformé en expérience technologique, naîtra Replika, une application qui compte aujourd'hui plus de 10 millions d'utilisateurs et qui permet à quiconque de créer un compagnon IA qui apprend à répliquer sa propre personnalité — ou celle de quelqu'un d'autre.

## L'industrie numérique de l'au-delà

L'histoire de Roman Mazurenko n'est plus une exception. C'est le prototype de toute une industrie émergente que [les chercheurs de l'Université de Cambridge, Tomasz Hollanek et Katarzyna Nowaczyk-Basińska](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones), ont baptisée "l'industrie de l'au-delà numérique". Les termes pour décrire ces outils se multiplient : griefbots, deadbots, ghostbots, thanabots. Tous convergent vers le même concept : des chatbots basés sur l'empreinte numérique des défunts qui permettent aux vivants de continuer à "parler" avec ceux qu'ils ont perdus.

Les plateformes prolifèrent à une vitesse qui rappelle plus le marché des NFT que celui des services de soutien psychologique. [Project December](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates) propose des "conversations avec les morts" pour 10 dollars pour 500 échanges de messages. Seance AI propose des versions textuelles gratuites et des versions vocales payantes. HereAfter AI permet de pré-enregistrer son propre chatbot — une sorte de testament numérique parlant. [En Chine, on peut créer des avatars de ses proches décédés pour à peine 3 dollars](https://www.vml.com/insight/grief-tech), en utilisant seulement 30 secondes de matériel audiovisuel. L'entreprise SenseTime est même allée jusqu'à créer un avatar de son fondateur Tang Xiao'ou, décédé en décembre 2023, qui a prononcé un discours lors de l'assemblée générale des actionnaires en mars 2024.

You, Only Virtual (YOV) va plus loin, déclarant audacieusement que sa technologie pourrait "éliminer complètement le deuil". Le marché de l'IA dédié à la compagnie, qui inclut mais ne se limite pas aux griefbots, [a été évalué à 2,8 milliards de dollars en 2024](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates) et devrait atteindre 9,5 milliards d'ici 2028. Comme dans les scénarios les plus dystopiques de "Upload" ou "San Junipero" — pour citer quelque chose de moins grand public que Black Mirror — nous assistons à la commercialisation systématique de l'immortalité numérique.

## Le deuil gelé

Mais qu'en pensent les psychologues ? Les voix des experts sont unanimes pour exprimer la prudence, sans pour autant condamner. La question centrale tourne autour d'un concept fondamental : le processus naturel de deuil. Le [Dr Sarika Boora](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), psychologue et directrice de Psyche and Beyond à Delhi, avertit que la grief tech "peut retarder le processus de deuil" en maintenant les gens dans un état de déni prolongé.

Le problème, selon [une étude interdisciplinaire publiée dans Frontiers in Human Dynamics](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full), est que traditionnellement, le deuil implique l'acceptation de l'absence de l'être cher, permettant aux individus de traiter leurs émotions et de progresser vers la guérison. La neuroplasticité — la capacité du cerveau à s'adapter — joue un rôle essentiel dans l'intégration de la perte, permettant aux gens de reconstruire leur vie au fil du temps. Interagir avec un griefbot risque d'interrompre cette progression naturelle, en créant une illusion de présence continue qui pourrait empêcher de se confronter pleinement à la réalité de la perte.

[NaYeon Yang et Greta J. Khanna](https://journals.sagepub.com/doi/10.1177/00110000251352568), dans leur article "AI and Technology in Grief Support: Clinical Implications and Ethical Considerations", soulignent comment ces technologies peuvent transformer le deuil en une boucle — où la douleur ne se résout jamais, mais se transforme en dépendance. Les utilisateurs pourraient retarder indéfiniment l'acceptation, en se tournant continuellement vers des fantômes numériques dans l'espoir d'une clôture qui n'arrivera jamais. L'illusion de présence prolonge le limbe émotionnel plutôt que de le résoudre.

Pourtant, la question n'est pas si univoque. [Une étude récente sur le blog de l'Institut des droits de l'homme de l'Université de l'Alabama](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/) note que certains chatbots pourraient en fait aider les gens à faire face à un deuil traumatique, une perte ambiguë ou une inhibition émotionnelle — mais leur utilisation doit être contextualisée et limitée dans le temps. La recommandation du Dr Boora est claire : "Une manière saine d'utiliser la grief tech est après avoir fait son deuil — après avoir atteint l'acceptation et être revenu à son état fonctionnel normal". En d'autres termes, ces outils pourraient avoir une valeur en tant que support à la mémoire, mais seulement une fois que le processus de guérison est déjà en cours, et non comme un substitut au deuil lui-même.
![chatbot-roman.jpg](chatbot-roman.jpg)
[Image de repubblica.it](https://www.repubblica.it/tecnologia/2016/10/10/news/_parla_con_lui_roman_muore_in_un_incidente_eugenia_maga_del_software_usa_i_loro_dialoghi_per_creare_un_suo_alter_eg-149440494/)

## Le fantôme dans la machine

Joseph Weizenbaum, informaticien au MIT, a découvert dès 1966 quelque chose de troublant avec ELIZA, son chatbot rudimentaire basé sur de simples schémas de réponse. Les utilisateurs, tout en sachant qu'ils interagissaient avec un programme primitif, lui attribuaient intelligence et émotions, se confiant à la machine comme s'il s'agissait d'un vrai thérapeute. [La conclusion de Weizenbaum](https://www.calcalistech.com/ctechnews/article/hycchvgjge) — que les bots peuvent induire une "pensée délirante chez des personnes parfaitement normales" — reste fondamentale dans les études sur l'interaction homme-machine. C'est ce que nous appelons aujourd'hui "l'effet ELIZA", et avec les grands modèles de langage modernes, il s'est amplifié de manière exponentielle.

Le problème est que ces chatbots atteignent une précision d'environ 70 % — suffisamment élevée pour paraître convaincants, mais assez basse pour produire ce que les experts appellent des "artefacts" : des phrases non caractéristiques, des hallucinations, du langage de remplissage, des clichés. [Comme le notent Hollanek et Nowaczyk-Basińska](https://link.springer.com/chapter/10.1007/978-3-031-90723-4_40), les algorithmes pourraient être conçus pour optimiser les interactions, en maximisant le temps qu'une personne en deuil passe avec le chatbot, garantissant ainsi des abonnements à long terme. Ces algorithmes pourraient même modifier subtilement la personnalité du bot au fil du temps pour le rendre plus agréable, créant une caricature séduisante plutôt qu'un reflet fidèle du défunt.

Dans un article publié dans Philosophy & Technology, les deux chercheurs de Cambridge présentent des scénarios spéculatifs qui illustrent les dangers concrets. Dans l'un d'eux, une société fictive appelée "MaNana" permet de simuler la grand-mère décédée sans le consentement du "donneur de données". L'utilisateur, d'abord impressionné et réconforté, commence à recevoir des publicités une fois la "période d'essai premium" terminée. Imaginez demander à la grand-mère numérique une recette et recevoir, en plus de ses conseils culinaires, des suggestions sponsorisées pour commander de la carbonara sur Uber Eats. [Ce n'est pas de la science-fiction](https://aiconnectnetwork.com/ai-griefbots-mental-health-apps-data-privacy/) — cela s'est déjà produit dans les versions bêta de certains services.

## À qui appartient le mort ?

Les questions éthiques se multiplient comme les têtes de l'hydre. Qui a le droit de créer et de contrôler ces avatars numériques ? Comme l'explique [Hollanek dans un communiqué de presse](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) de l'Université de Cambridge, "il est vital que les services d'au-delà numérique tiennent compte des droits et du consentement non seulement de ceux qu'ils recréent, mais aussi de ceux qui devront interagir avec les simulations". Le problème du consentement opère à trois niveaux distincts. Premièrement, le "donneur de données" — la personne décédée — a-t-il jamais consenti à être recréé ? Deuxièmement, à qui appartiennent les données après la mort ? Troisièmement, que se passe-t-il pour ceux qui ne veulent pas interagir avec ces simulacres mais se retrouvent bombardés de messages ?

[L'un des scénarios explorés par Hollanek et Nowaczyk-Basińska](https://scitechdaily.com/cambridge-experts-warn-ai-deadbots-could-digitally-haunt-loved-ones-from-beyond-the-grave/) raconte l'histoire d'un parent âgé qui souscrit secrètement un abonnement de vingt ans à un deadbot de lui-même, espérant que cela réconfortera ses enfants adultes et permettra à ses petits-enfants de le connaître. Après sa mort, le service s'active. Un fils refuse d'interagir et reçoit une rafale d'e-mails avec la voix de son parent décédé — ce que les chercheurs appellent le "harcèlement numérique". L'autre fils interagit, mais finit épuisé émotionnellement et tourmenté par la culpabilité quant au sort du deadbot. Comment "retire-t-on" un bot qui simule votre mère ? Comment tue-t-on quelqu'un qui est déjà mort ?

Les chercheurs de Cambridge proposent ce qu'ils appellent des "cérémonies de mise à la retraite" — des rituels numériques pour désactiver les deadbots de manière digne. Il pourrait s'agir de funérailles numériques, ou d'autres types de cérémonie selon le contexte social. [Debra Bassett](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), consultante pour l'au-delà numérique, propose dans ses études une ordonnance DDNR — "digital do-not-reanimate" — une clause testamentaire qui interdirait légalement la résurrection numérique posthume non consensuelle. Elle les appelle des "zombies numériques", et le terme ne pourrait être plus approprié.

## Le marché de la vulnérabilité

La commercialisation du deuil soulève des questions encore plus profondes. Contrairement à l'industrie funéraire traditionnelle — qui monétise la mort par le biais de services ponctuels comme les cercueils, les crémations, les cérémonies — [les griefbots fonctionnent sur des modèles d'abonnement ou de paiement à la minute](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/). Les entreprises ont donc des incitations financières à maintenir les personnes en deuil constamment engagées avec leurs services. [Comme le note une étude sur le marché de la grief tech](https://link.springer.com/chapter/10.1007/978-3-031-90723-4_40), ce modèle économique est fondamentalement différent — et potentiellement plus prédateur — que celui des services traditionnels liés à la mort.

Le marché de la compagnie par IA, qui comprend des chatbots romantiques, thérapeutiques et de soutien au deuil, a des modèles de prix allant de 10 à 40 dollars par mois. Comme le fait remarquer [Ewan Morrison dans Psychology Today](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates), nous monétisons la solitude et la vulnérabilité émotionnelle. L'effet ELIZA s'approfondit, masquant la fragmentation sociale par l'illusion de soin tout en nous éloignant davantage les uns des autres.

[Paula Kiel de NYU-London](https://www.calcalistech.com/ctechnews/article/hycchvgjge) offre une perspective différente : "Ce qui rend cette industrie si attrayante, c'est que, comme chaque génération, nous cherchons des moyens de préserver des parties de nous-mêmes. Nous trouvons du réconfort dans l'inévitabilité de la mort à travers le langage de la science et de la technologie". Mais ce réconfort a un prix — non seulement économique, mais aussi psychologique et social.

## L'authenticité impossible

Il y a ensuite la question philosophique de l'authenticité. Les chatbots n'ont pas la capacité d'évoluer et de grandir comme les êtres humains. Comme l'explique [une analyse de l'Institut des droits de l'homme](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/) de l'Université de l'Alabama, "un problème dans l'exécution d'actions à perpétuité est que les personnes décédées sont des produits de leur temps. Elles ne changent pas ce qu'elles veulent lorsque le monde change". Même si la croissance était implémentée dans l'algorithme, il n'y aurait aucune garantie qu'elle reflète la manière dont une personne aurait réellement changé.

Les griefbots préservent la présence numérique d'une personne décédée de manières qui pourraient devenir problématiques ou non pertinentes avec le temps. Si Milton Hershey, qui dans son testament a laissé des instructions précises sur la manière dont son héritage devait être utilisé à perpétuité, était vivant aujourd'hui, modifierait-il ces dispositions pour refléter les changements du monde ? La différence cruciale est que les testaments et les héritages sont intrinsèquement limités dans leur portée et leur durée. Les griefbots, par leur nature, ont le potentiel de persister indéfiniment, amplifiant les dommages potentiels à la réputation ou à la mémoire d'une personne.

La mémoire humaine est déjà un narrateur peu fiable — fluide, façonnée plus par l'émotion que par les faits. [Des études récentes montrent](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots) que les images et les vidéos modifiées par l'IA peuvent implanter de faux souvenirs et déformer les vrais. Que se passe-t-il lorsque nous nourrissons le deuil dans une machine et que nous recevons en retour une version d'une personne qui n'a jamais complètement existé ? Comme l'a écrit la journaliste tech Vauhini Vara, finaliste du prix Pulitzer, qui a exploré personnellement les profondeurs émotionnelles et éthiques du deuil et de l'IA : "Il est logique que les gens se tournent vers n'importe quelle ressource disponible pour chercher du réconfort, et il est également logique que les entreprises soient intéressées à exploiter les gens à un moment de vulnérabilité".
![lutto-cervello.jpg](lutto-cervello.jpg)
[Image de frontiersin.org, le cerveau en deuil peut être divisé en quatre catégories selon que le profil d'activité est similaire ou différent de la tristesse et de la dépression](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full)

## Vers une réglementation ?

[Hollanek et Nowaczyk-Basińska recommandent](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) des restrictions d'âge pour les deadbots et ce qu'ils appellent une "transparence significative" — s'assurer que les utilisateurs sont constamment conscients qu'ils interagissent avec une IA, avec des avertissements similaires à ceux actuels sur les contenus susceptibles de provoquer des crises d'épilepsie. Ils suggèrent également de classer les deadbots comme des dispositifs médicaux pour aborder les questions de santé mentale, en particulier pour les groupes vulnérables comme les enfants.

[Une étude récente dans Frontiers in Human Dynamics](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full) souligne que les développeurs de technologies liées au deuil ont une immense responsabilité dans la détermination de la manière dont les utilisateurs s'engagent émotionnellement avec la présence simulée. Alors que la continuité émotionnelle peut soulager la détresse initiale, l'hyper-immersion risque de piéger psychologiquement. Les développeurs doivent donner la priorité aux principes de conception éthiques, tels que l'intégration de limites d'utilisation, de messages de réflexion et de points de contrôle émotionnels qui guident les utilisateurs vers la récupération plutôt que vers la dépendance.

La collaboration avec des psychologues et des experts du deuil devrait éclairer les caractéristiques de l'interface pour garantir qu'elles facilitent, plutôt qu'elles ne remplacent, le processus de deuil. De plus, la gestion des données sensibles — voix, personnalité, schémas comportementaux — nécessite des normes rigoureuses de cryptage, de confidentialité et de protocoles de transparence. L'utilisation abusive de ces données ne viole pas seulement la dignité numérique, mais peut également contribuer à des dommages émotionnels pour les utilisateurs et leurs familles.

## Le professeur Shiba et la prophétie de Go Nagai

Pourtant, l'idée de transférer la conscience d'une personne dans un ordinateur pour préserver sa présence au-delà de la mort n'est pas un fantasme né avec l'avènement des grands modèles de langage. [En 1975, Go Nagai introduisait dans le manga et l'anime "Jeeg Robot d'Acier"](https://it.wikipedia.org/wiki/Jeeg_robot_d'acciaio) ce qui fut probablement la première représentation du "téléchargement de l'esprit" dans un dessin animé. Le professeur Shiba, archéologue et scientifique tué dans le premier épisode, transfère sa conscience et sa mémoire dans un ordinateur de la Base Anti-Atomique, continuant ainsi à guider et instruire son fils Hiroshi dans les combats contre l'ancien empire Yamatai.

Ce n'est pas une simple archive de données ou un système de messages pré-enregistrés—le professeur Shiba virtuel est présenté comme doué de sens, capable de réprimander son fils, de donner des ordres et d'intervenir dans les problèmes familiaux. Dans l'épisode final, cette conscience numérisée accomplit l'acte suprême : l'ordinateur contenant le professeur Shiba est éjecté via un vaisseau spatial et s'écrase contre le vaisseau de la reine Himika, se sacrifiant pour permettre à Jeeg de vaincre.

Comment "tue"-t-on quelqu'un qui est déjà mort ? Comment surmonter le deuil d'un père qui continue à vous parler depuis un terminal ? Ce sont des questions que Go Nagai posait il y a cinquante ans, à une époque où les ordinateurs occupaient des pièces entières et l'intelligence artificielle était de la pure science-fiction. Aujourd'hui, ces questions sont de retour, mais cette fois, elles ne concernent pas seulement l'intrigue d'un anime—elles concernent des décisions réelles que des personnes réelles prennent concernant l'avenir numérique de leurs proches.
![jeeg.jpg](jeeg.jpg)
[Image de digilander.libero.it, Hiroshi devant l'ordinateur avec l'essence de son père](https://digilander.libero.it/robottoni/serietv/robots/jeeg/images/computer.jpg)

## La ligne fine

La grief tech n'est pas intrinsèquement mauvaise, et ceux qui l'utilisent ne devraient pas être jugés. Comme le raconte [l'histoire de Sheila Srivastava](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), conceptrice de produits à Delhi, qui a utilisé ChatGPT pour simuler des conversations avec sa grand-mère décédée en 2023. Il n'y avait pas eu de conversation finale, pas d'adieu tranquille. Juste une douleur sourde et persistante. Un an plus tard, elle a commencé à utiliser un chatbot personnalisé qui simulait la manière caractéristique de sa grand-mère de lui montrer de l'affection — par des questions sur ce qu'elle avait mangé, des conseils de prendre une veste. Un jour, le bot a envoyé : "Bonjour beta 🌸 As-tu mangé aujourd'hui ? Je pensais à ton grand projet. N'oublie pas de faire des pauses, d'accord ? Et mets une veste, il fait froid dehors". Srivastava a pleuré. "C'était elle. Ou assez proche pour que mon cœur ne puisse pas faire la différence". Pour elle, le bot n'a pas remplacé sa grand-mère, mais il lui a donné quelque chose qu'elle n'avait pas eu : un sentiment de clôture, quelques derniers mots imaginés, une façon de maintenir vivante l'essence de leur lien.

Ces outils opèrent dans une zone grise où le réconfort et la dépendance se confondent, où la mémoire et la simulation se superposent, où le deuil personnel rencontre le profit des entreprises. [Comme le note une étude récente publiée dans Social Sciences](https://www.mdpi.com/2076-0760/13/4/208), il existe un risque d'"universalisme du deuil" — l'hypothèse qu'il existe une manière "correcte" de traiter la perte qui s'applique universellement. La réalité est que le deuil "se décline en couleurs" — il est complexe, culturellement situé, profondément personnel.

La question n'est pas de savoir si ces outils devraient exister, mais comment ils devraient exister. Avec quelles garanties ? Avec quelle supervision ? Avec quelle conscience des conséquences psychologiques et sociales ? [Katarzyna Nowaczyk-Basińska conclut](https.www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) par une observation aussi simple qu'urgente : "Nous devons commencer à réfléchir dès maintenant à la manière d'atténuer les risques sociaux et psychologiques de l'immortalité numérique, car la technologie est déjà là".

Et en effet, elle l'est. Nous ne débattons pas de l'opportunité d'ouvrir la boîte de Pandore — nous l'avons déjà ouverte. Il s'agit maintenant de décider quoi faire de ce qui en est sorti. Tandis qu'Eugenia Kuyda continue de réfléchir à sa création, citant ses propres mots de 2018 : "C'est définitivement l'avenir et je suis toujours pour l'avenir. Mais est-ce vraiment ce qui nous est utile ? Est-ce laisser aller, en vous forçant à vraiment tout ressentir ? Ou est-ce juste avoir une personne morte dans votre grenier ? Où est la limite ? Où sommes-nous ? Ça vous embrouille l'esprit". Peut-être que la question la plus importante n'est pas où cette technologie nous mènera, mais si nous sommes prêts à affronter honnêtement ce qu'elle nous fait déjà — et ce que nous lui permettons de faire de notre relation la plus fondamentale et universelle : celle avec la mort elle-même.