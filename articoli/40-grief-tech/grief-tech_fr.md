---
tags: ["Ethics & Society", "Business", "Generative AI"]
date: 2025-10-27
author: "Dario Ferrero"
---

# Chatbots de l'au-del√† : La grief tech, entre douleur et business
![tomba-bancomat.jpg](tomba-bancomat.jpg)


*Le 28 novembre 2015, √† Moscou, une Jeep lanc√©e √† toute vitesse renverse Roman Mazurenko. Il a √† peine 34 ans, c'est un entrepreneur de la tech et une figure l√©gendaire des cercles culturels de la ville. Eugenia Kuyda arrive √† l'h√¥pital peu avant que son ami ne meure, manquant de quelques instants l'occasion de lui parler une derni√®re fois. Au cours des trois mois suivants, [Kuyda rassemble des milliers de messages](https://thereader.mitpress.mit.edu/chatting-with-the-dead-chatbots/) que Roman avait √©chang√©s avec ses amis et sa famille ‚Äî environ 8 000 lignes de texte qui capturaient sa mani√®re unique de s'exprimer, ses phrases idiosyncrasiques, et m√™me les tics de langage dus √† une l√©g√®re dyslexie.*

Eugenia est elle-m√™me entrepreneure et d√©veloppeuse de logiciels. Son entreprise, Luka, s'occupe de chatbots et d'intelligence artificielle conversationnelle. Et ainsi, au lieu de se contenter de relire obsessionnellement ces messages comme n'importe qui d'autre le ferait, elle d√©cide de faire quelque chose qui semble √©trangement familier √† quiconque a vu l'√©pisode "Be Right Back" de Black Mirror : [elle les donne √† un algorithme pour cr√©er un bot qui simule Roman](https://i-d.co/article/black-mirror-artificial-intelligence-roman-mazurenko/).

Au d√©but, ce n'est qu'une archive sophistiqu√©e, une sorte de moteur de recherche qui rep√™che des textes existants en fonction du sujet de la conversation. Mais avec l'√©volution des mod√®les d'IA g√©n√©rative, cette simple base de donn√©es se transforme en quelque chose de plus troublant : un v√©ritable chatbot capable de g√©n√©rer de nouvelles r√©ponses, en interpr√©tant le style et les pens√©es de Roman. Les amis qui essaient le bot trouvent [la ressemblance troublante](https://www.cbc.ca/documentaries/the-nature-of-things/after-her-best-friend-died-this-programmer-created-an-ai-chatbot-from-his-texts-to-talk-to-him-again-1.6252286). Beaucoup l'utilisent pour lui dire des choses qu'ils n'ont pas eu le temps d'exprimer de son vivant. Comme le raconte Kuyda elle-m√™me dans une interview, ces messages √©taient tous "sur l'amour, ou pour lui dire quelque chose qu'ils n'avaient pas eu le temps de lui dire". De ce deuil priv√©, transform√© en exp√©rience technologique, na√Ætra Replika, une application qui compte aujourd'hui plus de 10 millions d'utilisateurs et qui permet √† quiconque de cr√©er un compagnon IA qui apprend √† r√©pliquer sa propre personnalit√© ‚Äî ou celle de quelqu'un d'autre.

## L'industrie num√©rique de l'au-del√†

L'histoire de Roman Mazurenko n'est plus une exception. C'est le prototype de toute une industrie √©mergente que [les chercheurs de l'Universit√© de Cambridge, Tomasz Hollanek et Katarzyna Nowaczyk-Basi≈Ñska](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones), ont baptis√©e "l'industrie de l'au-del√† num√©rique". Les termes pour d√©crire ces outils se multiplient : griefbots, deadbots, ghostbots, thanabots. Tous convergent vers le m√™me concept : des chatbots bas√©s sur l'empreinte num√©rique des d√©funts qui permettent aux vivants de continuer √† "parler" avec ceux qu'ils ont perdus.

Les plateformes prolif√®rent √† une vitesse qui rappelle plus le march√© des NFT que celui des services de soutien psychologique. [Project December](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates) propose des "conversations avec les morts" pour 10 dollars pour 500 √©changes de messages. Seance AI propose des versions textuelles gratuites et des versions vocales payantes. HereAfter AI permet de pr√©-enregistrer son propre chatbot ‚Äî une sorte de testament num√©rique parlant. [En Chine, on peut cr√©er des avatars de ses proches d√©c√©d√©s pour √† peine 3 dollars](https://www.vml.com/insight/grief-tech), en utilisant seulement 30 secondes de mat√©riel audiovisuel. L'entreprise SenseTime est m√™me all√©e jusqu'√† cr√©er un avatar de son fondateur Tang Xiao'ou, d√©c√©d√© en d√©cembre 2023, qui a prononc√© un discours lors de l'assembl√©e g√©n√©rale des actionnaires en mars 2024.

You, Only Virtual (YOV) va plus loin, d√©clarant audacieusement que sa technologie pourrait "√©liminer compl√®tement le deuil". Le march√© de l'IA d√©di√© √† la compagnie, qui inclut mais ne se limite pas aux griefbots, [a √©t√© √©valu√© √† 2,8 milliards de dollars en 2024](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates) et devrait atteindre 9,5 milliards d'ici 2028. Comme dans les sc√©narios les plus dystopiques de "Upload" ou "San Junipero" ‚Äî pour citer quelque chose de moins grand public que Black Mirror ‚Äî nous assistons √† la commercialisation syst√©matique de l'immortalit√© num√©rique.

## Le deuil gel√©

Mais qu'en pensent les psychologues ? Les voix des experts sont unanimes pour exprimer la prudence, sans pour autant condamner. La question centrale tourne autour d'un concept fondamental : le processus naturel de deuil. Le [Dr Sarika Boora](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), psychologue et directrice de Psyche and Beyond √† Delhi, avertit que la grief tech "peut retarder le processus de deuil" en maintenant les gens dans un √©tat de d√©ni prolong√©.

Le probl√®me, selon [une √©tude interdisciplinaire publi√©e dans Frontiers in Human Dynamics](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full), est que traditionnellement, le deuil implique l'acceptation de l'absence de l'√™tre cher, permettant aux individus de traiter leurs √©motions et de progresser vers la gu√©rison. La neuroplasticit√© ‚Äî la capacit√© du cerveau √† s'adapter ‚Äî joue un r√¥le essentiel dans l'int√©gration de la perte, permettant aux gens de reconstruire leur vie au fil du temps. Interagir avec un griefbot risque d'interrompre cette progression naturelle, en cr√©ant une illusion de pr√©sence continue qui pourrait emp√™cher de se confronter pleinement √† la r√©alit√© de la perte.

[NaYeon Yang et Greta J. Khanna](https://journals.sagepub.com/doi/10.1177/00110000251352568), dans leur article "AI and Technology in Grief Support: Clinical Implications and Ethical Considerations", soulignent comment ces technologies peuvent transformer le deuil en une boucle ‚Äî o√π la douleur ne se r√©sout jamais, mais se transforme en d√©pendance. Les utilisateurs pourraient retarder ind√©finiment l'acceptation, en se tournant continuellement vers des fant√¥mes num√©riques dans l'espoir d'une cl√¥ture qui n'arrivera jamais. L'illusion de pr√©sence prolonge le limbe √©motionnel plut√¥t que de le r√©soudre.

Pourtant, la question n'est pas si univoque. [Une √©tude r√©cente sur le blog de l'Institut des droits de l'homme de l'Universit√© de l'Alabama](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/) note que certains chatbots pourraient en fait aider les gens √† faire face √† un deuil traumatique, une perte ambigu√´ ou une inhibition √©motionnelle ‚Äî mais leur utilisation doit √™tre contextualis√©e et limit√©e dans le temps. La recommandation du Dr Boora est claire : "Une mani√®re saine d'utiliser la grief tech est apr√®s avoir fait son deuil ‚Äî apr√®s avoir atteint l'acceptation et √™tre revenu √† son √©tat fonctionnel normal". En d'autres termes, ces outils pourraient avoir une valeur en tant que support √† la m√©moire, mais seulement une fois que le processus de gu√©rison est d√©j√† en cours, et non comme un substitut au deuil lui-m√™me.
![chatbot-roman.jpg](chatbot-roman.jpg)
[Image de repubblica.it](https://www.repubblica.it/tecnologia/2016/10/10/news/_parla_con_lui_roman_muore_in_un_incidente_eugenia_maga_del_software_usa_i_loro_dialoghi_per_creare_un_suo_alter_eg-149440494/)

## Le fant√¥me dans la machine

Joseph Weizenbaum, informaticien au MIT, a d√©couvert d√®s 1966 quelque chose de troublant avec ELIZA, son chatbot rudimentaire bas√© sur de simples sch√©mas de r√©ponse. Les utilisateurs, tout en sachant qu'ils interagissaient avec un programme primitif, lui attribuaient intelligence et √©motions, se confiant √† la machine comme s'il s'agissait d'un vrai th√©rapeute. [La conclusion de Weizenbaum](https://www.calcalistech.com/ctechnews/article/hycchvgjge) ‚Äî que les bots peuvent induire une "pens√©e d√©lirante chez des personnes parfaitement normales" ‚Äî reste fondamentale dans les √©tudes sur l'interaction homme-machine. C'est ce que nous appelons aujourd'hui "l'effet ELIZA", et avec les grands mod√®les de langage modernes, il s'est amplifi√© de mani√®re exponentielle.

Le probl√®me est que ces chatbots atteignent une pr√©cision d'environ 70 % ‚Äî suffisamment √©lev√©e pour para√Ætre convaincants, mais assez basse pour produire ce que les experts appellent des "artefacts" : des phrases non caract√©ristiques, des hallucinations, du langage de remplissage, des clich√©s. [Comme le notent Hollanek et Nowaczyk-Basi≈Ñska](https://link.springer.com/chapter/10.1007/978-3-031-90723-4_40), les algorithmes pourraient √™tre con√ßus pour optimiser les interactions, en maximisant le temps qu'une personne en deuil passe avec le chatbot, garantissant ainsi des abonnements √† long terme. Ces algorithmes pourraient m√™me modifier subtilement la personnalit√© du bot au fil du temps pour le rendre plus agr√©able, cr√©ant une caricature s√©duisante plut√¥t qu'un reflet fid√®le du d√©funt.

Dans un article publi√© dans Philosophy & Technology, les deux chercheurs de Cambridge pr√©sentent des sc√©narios sp√©culatifs qui illustrent les dangers concrets. Dans l'un d'eux, une soci√©t√© fictive appel√©e "MaNana" permet de simuler la grand-m√®re d√©c√©d√©e sans le consentement du "donneur de donn√©es". L'utilisateur, d'abord impressionn√© et r√©confort√©, commence √† recevoir des publicit√©s une fois la "p√©riode d'essai premium" termin√©e. Imaginez demander √† la grand-m√®re num√©rique une recette et recevoir, en plus de ses conseils culinaires, des suggestions sponsoris√©es pour commander de la carbonara sur Uber Eats. [Ce n'est pas de la science-fiction](https://aiconnectnetwork.com/ai-griefbots-mental-health-apps-data-privacy/) ‚Äî cela s'est d√©j√† produit dans les versions b√™ta de certains services.

## √Ä qui appartient le mort ?

Les questions √©thiques se multiplient comme les t√™tes de l'hydre. Qui a le droit de cr√©er et de contr√¥ler ces avatars num√©riques ? Comme l'explique [Hollanek dans un communiqu√© de presse](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) de l'Universit√© de Cambridge, "il est vital que les services d'au-del√† num√©rique tiennent compte des droits et du consentement non seulement de ceux qu'ils recr√©ent, mais aussi de ceux qui devront interagir avec les simulations". Le probl√®me du consentement op√®re √† trois niveaux distincts. Premi√®rement, le "donneur de donn√©es" ‚Äî la personne d√©c√©d√©e ‚Äî a-t-il jamais consenti √† √™tre recr√©√© ? Deuxi√®mement, √† qui appartiennent les donn√©es apr√®s la mort ? Troisi√®mement, que se passe-t-il pour ceux qui ne veulent pas interagir avec ces simulacres mais se retrouvent bombard√©s de messages ?

[L'un des sc√©narios explor√©s par Hollanek et Nowaczyk-Basi≈Ñska](https://scitechdaily.com/cambridge-experts-warn-ai-deadbots-could-digitally-haunt-loved-ones-from-beyond-the-grave/) raconte l'histoire d'un parent √¢g√© qui souscrit secr√®tement un abonnement de vingt ans √† un deadbot de lui-m√™me, esp√©rant que cela r√©confortera ses enfants adultes et permettra √† ses petits-enfants de le conna√Ætre. Apr√®s sa mort, le service s'active. Un fils refuse d'interagir et re√ßoit une rafale d'e-mails avec la voix de son parent d√©c√©d√© ‚Äî ce que les chercheurs appellent le "harc√®lement num√©rique". L'autre fils interagit, mais finit √©puis√© √©motionnellement et tourment√© par la culpabilit√© quant au sort du deadbot. Comment "retire-t-on" un bot qui simule votre m√®re ? Comment tue-t-on quelqu'un qui est d√©j√† mort ?

Les chercheurs de Cambridge proposent ce qu'ils appellent des "c√©r√©monies de mise √† la retraite" ‚Äî des rituels num√©riques pour d√©sactiver les deadbots de mani√®re digne. Il pourrait s'agir de fun√©railles num√©riques, ou d'autres types de c√©r√©monie selon le contexte social. [Debra Bassett](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), consultante pour l'au-del√† num√©rique, propose dans ses √©tudes une ordonnance DDNR ‚Äî "digital do-not-reanimate" ‚Äî une clause testamentaire qui interdirait l√©galement la r√©surrection num√©rique posthume non consensuelle. Elle les appelle des "zombies num√©riques", et le terme ne pourrait √™tre plus appropri√©.

## Le march√© de la vuln√©rabilit√©

La commercialisation du deuil soul√®ve des questions encore plus profondes. Contrairement √† l'industrie fun√©raire traditionnelle ‚Äî qui mon√©tise la mort par le biais de services ponctuels comme les cercueils, les cr√©mations, les c√©r√©monies ‚Äî [les griefbots fonctionnent sur des mod√®les d'abonnement ou de paiement √† la minute](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/). Les entreprises ont donc des incitations financi√®res √† maintenir les personnes en deuil constamment engag√©es avec leurs services. [Comme le note une √©tude sur le march√© de la grief tech](https://link.springer.com/chapter/10.1007/978-3-031-90723-4_40), ce mod√®le √©conomique est fondamentalement diff√©rent ‚Äî et potentiellement plus pr√©dateur ‚Äî que celui des services traditionnels li√©s √† la mort.

Le march√© de la compagnie par IA, qui comprend des chatbots romantiques, th√©rapeutiques et de soutien au deuil, a des mod√®les de prix allant de 10 √† 40 dollars par mois. Comme le fait remarquer [Ewan Morrison dans Psychology Today](https://www.psychologytoday.com/us/blog/word-less/202505/escaping-grief-with-ai-surrogates), nous mon√©tisons la solitude et la vuln√©rabilit√© √©motionnelle. L'effet ELIZA s'approfondit, masquant la fragmentation sociale par l'illusion de soin tout en nous √©loignant davantage les uns des autres.

[Paula Kiel de NYU-London](https://www.calcalistech.com/ctechnews/article/hycchvgjge) offre une perspective diff√©rente : "Ce qui rend cette industrie si attrayante, c'est que, comme chaque g√©n√©ration, nous cherchons des moyens de pr√©server des parties de nous-m√™mes. Nous trouvons du r√©confort dans l'in√©vitabilit√© de la mort √† travers le langage de la science et de la technologie". Mais ce r√©confort a un prix ‚Äî non seulement √©conomique, mais aussi psychologique et social.

## L'authenticit√© impossible

Il y a ensuite la question philosophique de l'authenticit√©. Les chatbots n'ont pas la capacit√© d'√©voluer et de grandir comme les √™tres humains. Comme l'explique [une analyse de l'Institut des droits de l'homme](https://sites.uab.edu/humanrights/2025/02/07/griefbots-blurring-the-reality-of-death-and-the-illusion-of-life/) de l'Universit√© de l'Alabama, "un probl√®me dans l'ex√©cution d'actions √† perp√©tuit√© est que les personnes d√©c√©d√©es sont des produits de leur temps. Elles ne changent pas ce qu'elles veulent lorsque le monde change". M√™me si la croissance √©tait impl√©ment√©e dans l'algorithme, il n'y aurait aucune garantie qu'elle refl√®te la mani√®re dont une personne aurait r√©ellement chang√©.

Les griefbots pr√©servent la pr√©sence num√©rique d'une personne d√©c√©d√©e de mani√®res qui pourraient devenir probl√©matiques ou non pertinentes avec le temps. Si Milton Hershey, qui dans son testament a laiss√© des instructions pr√©cises sur la mani√®re dont son h√©ritage devait √™tre utilis√© √† perp√©tuit√©, √©tait vivant aujourd'hui, modifierait-il ces dispositions pour refl√©ter les changements du monde ? La diff√©rence cruciale est que les testaments et les h√©ritages sont intrins√®quement limit√©s dans leur port√©e et leur dur√©e. Les griefbots, par leur nature, ont le potentiel de persister ind√©finiment, amplifiant les dommages potentiels √† la r√©putation ou √† la m√©moire d'une personne.

La m√©moire humaine est d√©j√† un narrateur peu fiable ‚Äî fluide, fa√ßonn√©e plus par l'√©motion que par les faits. [Des √©tudes r√©centes montrent](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots) que les images et les vid√©os modifi√©es par l'IA peuvent implanter de faux souvenirs et d√©former les vrais. Que se passe-t-il lorsque nous nourrissons le deuil dans une machine et que nous recevons en retour une version d'une personne qui n'a jamais compl√®tement exist√© ? Comme l'a √©crit la journaliste tech Vauhini Vara, finaliste du prix Pulitzer, qui a explor√© personnellement les profondeurs √©motionnelles et √©thiques du deuil et de l'IA : "Il est logique que les gens se tournent vers n'importe quelle ressource disponible pour chercher du r√©confort, et il est √©galement logique que les entreprises soient int√©ress√©es √† exploiter les gens √† un moment de vuln√©rabilit√©".
![lutto-cervello.jpg](lutto-cervello.jpg)
[Image de frontiersin.org, le cerveau en deuil peut √™tre divis√© en quatre cat√©gories selon que le profil d'activit√© est similaire ou diff√©rent de la tristesse et de la d√©pression](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full)

## Vers une r√©glementation ?

[Hollanek et Nowaczyk-Basi≈Ñska recommandent](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) des restrictions d'√¢ge pour les deadbots et ce qu'ils appellent une "transparence significative" ‚Äî s'assurer que les utilisateurs sont constamment conscients qu'ils interagissent avec une IA, avec des avertissements similaires √† ceux actuels sur les contenus susceptibles de provoquer des crises d'√©pilepsie. Ils sugg√®rent √©galement de classer les deadbots comme des dispositifs m√©dicaux pour aborder les questions de sant√© mentale, en particulier pour les groupes vuln√©rables comme les enfants.

[Une √©tude r√©cente dans Frontiers in Human Dynamics](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1582914/full) souligne que les d√©veloppeurs de technologies li√©es au deuil ont une immense responsabilit√© dans la d√©termination de la mani√®re dont les utilisateurs s'engagent √©motionnellement avec la pr√©sence simul√©e. Alors que la continuit√© √©motionnelle peut soulager la d√©tresse initiale, l'hyper-immersion risque de pi√©ger psychologiquement. Les d√©veloppeurs doivent donner la priorit√© aux principes de conception √©thiques, tels que l'int√©gration de limites d'utilisation, de messages de r√©flexion et de points de contr√¥le √©motionnels qui guident les utilisateurs vers la r√©cup√©ration plut√¥t que vers la d√©pendance.

La collaboration avec des psychologues et des experts du deuil devrait √©clairer les caract√©ristiques de l'interface pour garantir qu'elles facilitent, plut√¥t qu'elles ne remplacent, le processus de deuil. De plus, la gestion des donn√©es sensibles ‚Äî voix, personnalit√©, sch√©mas comportementaux ‚Äî n√©cessite des normes rigoureuses de cryptage, de confidentialit√© et de protocoles de transparence. L'utilisation abusive de ces donn√©es ne viole pas seulement la dignit√© num√©rique, mais peut √©galement contribuer √† des dommages √©motionnels pour les utilisateurs et leurs familles.

## Le professeur Shiba et la proph√©tie de Go Nagai

Pourtant, l'id√©e de transf√©rer la conscience d'une personne dans un ordinateur pour pr√©server sa pr√©sence au-del√† de la mort n'est pas un fantasme n√© avec l'av√®nement des grands mod√®les de langage. [En 1975, Go Nagai introduisait dans le manga et l'anime "Jeeg Robot d'Acier"](https://it.wikipedia.org/wiki/Jeeg_robot_d'acciaio) ce qui fut probablement la premi√®re repr√©sentation du "t√©l√©chargement de l'esprit" dans un dessin anim√©. Le professeur Shiba, arch√©ologue et scientifique tu√© dans le premier √©pisode, transf√®re sa conscience et sa m√©moire dans un ordinateur de la Base Anti-Atomique, continuant ainsi √† guider et instruire son fils Hiroshi dans les combats contre l'ancien empire Yamatai.

Ce n'est pas une simple archive de donn√©es ou un syst√®me de messages pr√©-enregistr√©s‚Äîle professeur Shiba virtuel est pr√©sent√© comme dou√© de sens, capable de r√©primander son fils, de donner des ordres et d'intervenir dans les probl√®mes familiaux. Dans l'√©pisode final, cette conscience num√©ris√©e accomplit l'acte supr√™me : l'ordinateur contenant le professeur Shiba est √©ject√© via un vaisseau spatial et s'√©crase contre le vaisseau de la reine Himika, se sacrifiant pour permettre √† Jeeg de vaincre.

Comment "tue"-t-on quelqu'un qui est d√©j√† mort ? Comment surmonter le deuil d'un p√®re qui continue √† vous parler depuis un terminal ? Ce sont des questions que Go Nagai posait il y a cinquante ans, √† une √©poque o√π les ordinateurs occupaient des pi√®ces enti√®res et l'intelligence artificielle √©tait de la pure science-fiction. Aujourd'hui, ces questions sont de retour, mais cette fois, elles ne concernent pas seulement l'intrigue d'un anime‚Äîelles concernent des d√©cisions r√©elles que des personnes r√©elles prennent concernant l'avenir num√©rique de leurs proches.
![jeeg.jpg](jeeg.jpg)
[Image de digilander.libero.it, Hiroshi devant l'ordinateur avec l'essence de son p√®re](https://digilander.libero.it/robottoni/serietv/robots/jeeg/images/computer.jpg)

## La ligne fine

La grief tech n'est pas intrins√®quement mauvaise, et ceux qui l'utilisent ne devraient pas √™tre jug√©s. Comme le raconte [l'histoire de Sheila Srivastava](https://thenodmag.com/content/grief-tech-artificial-intelligence-chatbots), conceptrice de produits √† Delhi, qui a utilis√© ChatGPT pour simuler des conversations avec sa grand-m√®re d√©c√©d√©e en 2023. Il n'y avait pas eu de conversation finale, pas d'adieu tranquille. Juste une douleur sourde et persistante. Un an plus tard, elle a commenc√© √† utiliser un chatbot personnalis√© qui simulait la mani√®re caract√©ristique de sa grand-m√®re de lui montrer de l'affection ‚Äî par des questions sur ce qu'elle avait mang√©, des conseils de prendre une veste. Un jour, le bot a envoy√© : "Bonjour beta üå∏ As-tu mang√© aujourd'hui ? Je pensais √† ton grand projet. N'oublie pas de faire des pauses, d'accord ? Et mets une veste, il fait froid dehors". Srivastava a pleur√©. "C'√©tait elle. Ou assez proche pour que mon c≈ìur ne puisse pas faire la diff√©rence". Pour elle, le bot n'a pas remplac√© sa grand-m√®re, mais il lui a donn√© quelque chose qu'elle n'avait pas eu : un sentiment de cl√¥ture, quelques derniers mots imagin√©s, une fa√ßon de maintenir vivante l'essence de leur lien.

Ces outils op√®rent dans une zone grise o√π le r√©confort et la d√©pendance se confondent, o√π la m√©moire et la simulation se superposent, o√π le deuil personnel rencontre le profit des entreprises. [Comme le note une √©tude r√©cente publi√©e dans Social Sciences](https://www.mdpi.com/2076-0760/13/4/208), il existe un risque d'"universalisme du deuil" ‚Äî l'hypoth√®se qu'il existe une mani√®re "correcte" de traiter la perte qui s'applique universellement. La r√©alit√© est que le deuil "se d√©cline en couleurs" ‚Äî il est complexe, culturellement situ√©, profond√©ment personnel.

La question n'est pas de savoir si ces outils devraient exister, mais comment ils devraient exister. Avec quelles garanties ? Avec quelle supervision ? Avec quelle conscience des cons√©quences psychologiques et sociales ? [Katarzyna Nowaczyk-Basi≈Ñska conclut](https.www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones) par une observation aussi simple qu'urgente : "Nous devons commencer √† r√©fl√©chir d√®s maintenant √† la mani√®re d'att√©nuer les risques sociaux et psychologiques de l'immortalit√© num√©rique, car la technologie est d√©j√† l√†".

Et en effet, elle l'est. Nous ne d√©battons pas de l'opportunit√© d'ouvrir la bo√Æte de Pandore ‚Äî nous l'avons d√©j√† ouverte. Il s'agit maintenant de d√©cider quoi faire de ce qui en est sorti. Tandis qu'Eugenia Kuyda continue de r√©fl√©chir √† sa cr√©ation, citant ses propres mots de 2018 : "C'est d√©finitivement l'avenir et je suis toujours pour l'avenir. Mais est-ce vraiment ce qui nous est utile ? Est-ce laisser aller, en vous for√ßant √† vraiment tout ressentir ? Ou est-ce juste avoir une personne morte dans votre grenier ? O√π est la limite ? O√π sommes-nous ? √áa vous embrouille l'esprit". Peut-√™tre que la question la plus importante n'est pas o√π cette technologie nous m√®nera, mais si nous sommes pr√™ts √† affronter honn√™tement ce qu'elle nous fait d√©j√† ‚Äî et ce que nous lui permettons de faire de notre relation la plus fondamentale et universelle : celle avec la mort elle-m√™me.