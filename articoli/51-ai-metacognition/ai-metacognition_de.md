---
tags: ["Research", "Generative AI", "Ethics & Society"]
date: 2025-11-21
author: "Dario Ferrero"
---

# KI macht dich fähiger, nicht weiser. Überschätzung des eigenen Wissens
![ai-metacognition.jpg](ai-metacognition.jpg)

*Stellen Sie sich vor, Sie absolvieren den amerikanischen Zulassungstest für die juristische Fakultät mit ChatGPT an Ihrer Seite. Die Ergebnisse verbessern sich merklich: drei Punkte mehr als bei denen, die die Prüfung allein ablegen. Doch wenn Sie gebeten werden, Ihre eigene Leistung zu bewerten, liegen Sie um vier Punkte zu hoch. Und nicht nur das: Je mehr Sie technisch über die Funktionsweise der künstlichen Intelligenz wissen, desto mehr verstärkt sich diese Illusion der Kompetenz. Willkommen im Paradoxon der erweiterten Kognition, wo Besserwerden gleichzeitig bedeutet, die Fähigkeit zu verlieren, zu verstehen, wie gut wir wirklich sind.*

## Das Paradoxon der gesteigerten Leistung

Die von der Aalto-Universität durchgeführte und in [Computers in Human Behavior](https://www.sciencedirect.com/science/article/pii/S0747563225002262) veröffentlichte Forschung unterzog 246 Teilnehmer zwanzig logischen Denkaufgaben aus dem LSAT, dem standardisierten Test für den Zugang zu amerikanischen Rechtsschulen. Die Hälfte der Stichprobe konnte ChatGPT-4o während der Lösung frei konsultieren, die andere Hälfte ging autonom vor. Die Zahlen erzählen eine ambivalente Geschichte: Diejenigen, die KI nutzten, erzielten Durchschnittswerte von 12,98 von 20, im Vergleich zu 9,45 bei denen, die ohne Unterstützung arbeiteten. Eine Verbesserung um 37 %, die auf dem Papier die Versprechen der erweiterten Intelligenz bestätigen würde.

Aber es gibt ein unbequemes Detail. Als die Teilnehmer der KI-Gruppe gebeten wurden, abzuschätzen, wie viele Antworten sie richtig gegeben hatten, lag der Durchschnitt ihrer Vorhersagen bei 16,50 von 20. Eine systematische Überschätzung von etwa vier Punkten, doppelt so hoch wie die der Kontrollgruppe. Als ob der Zugang zu einem leistungsstarken Werkzeug nicht nur ihre tatsächlichen Fähigkeiten verbessert hätte, sondern gleichzeitig auch ihre Wahrnehmung dieser Fähigkeiten in noch größerem Maße verzerrt hätte.

Das Phänomen erinnert an das Gefühl, das wir alle beim Scrollen durch Wikipedia um drei Uhr morgens erlebt haben: Nachdem wir drei Artikel über das Exoskelett von Krebstieren gelesen haben, fühlen wir uns plötzlich als Experten für Meeresbiologie. Psychologen nennen diesen Mechanismus "Illusion der Erklärungstiefe", und er funktioniert so: Wenn eine Information leicht zugänglich ist, verwechselt unser Gehirn sie mit besessener Information. Im Fall von KI wird der Effekt jedoch durch die konversationelle Natur der Interaktion verstärkt: ChatGPT gibt nicht nur Daten zurück, sondern konstruiert eine artikulierte Erzählung, die den kognitiven Prozess selbst nachahmt.

Daniela Fernandes, die Erstautorin der Studie, bemerkte, dass die Mehrheit der Teilnehmer überraschend oberflächlich mit ChatGPT interagierte. 46 % sendeten nur eine einzige Eingabeaufforderung pro Problem, kopierten die Frage in die Benutzeroberfläche und akzeptierten die Antwort ohne weitere Vertiefung. Das ist so, als würde man sich auf das erste Ergebnis einer Google-Suche verlassen, ohne die Quellen zu überprüfen, aber mit subtileren Konsequenzen: Man delegiert nicht nur die Suche nach Informationen, man lagert den Denkprozess selbst aus.
![performance1 .jpg](performance1 .jpg)
[Bild aus dem offiziellen Papier. Vergleich der Leistungsbewertungen zwischen Teilnehmern, die mit ChatGPT interagieren, und einem Datensatz ohne KI](https://www.sciencedirect.com/science/article/pii/S0747563225002262)

## Das Verschwinden des Dunning-Kruger-Effekts

Seit Jahrzehnten ist eines der am besten untersuchten Phänomene in der kognitiven Psychologie der Dunning-Kruger-Effekt: jene systematische Tendenz von weniger kompetenten Personen, ihre eigenen Fähigkeiten drastisch zu überschätzen, während die besser Vorbereiteten zu einer bescheidenen Unterschätzung neigen. Es ist das Prinzip, das erklärt, warum der Kollege, der kaum eine Excel-Tabelle öffnen kann, sich selbstbewusst anbietet, die Unternehmensdatenbank zu verwalten, während der Informatiker zögert, bevor er eine Meinung abgibt.

Bei logischen Denktests, die ohne KI durchgeführt werden, tritt dieses Muster mit mathematischer Regelmäßigkeit auf: Wer sich im untersten Leistungsprofil befindet, neigt dazu zu glauben, zur mittleren bis oberen Schicht zu gehören, wer herausragend ist, positioniert sich oft tiefer, als er es verdient hätte. Es ist ein Kalibrierungseffekt, der umgekehrt proportional zur tatsächlichen Kompetenz ist und von dem angetrieben wird, was die Forscher "metakognitives Rauschen" nennen: das Hintergrundrauschen, das unsere Fähigkeit zur Selbsteinschätzung stört.

Aber als die Forscher von Aalto ein bayesianisches Computermodell auf die Daten der Gruppe anwendeten, die ChatGPT verwendete, entdeckten sie etwas Unerwartetes. Der Dunning-Kruger-Effekt verschwand einfach. Er wurde nicht reduziert oder abgeschwächt: Er hörte auf, als statistisch relevantes Muster zu existieren. Alle Teilnehmer, unabhängig von ihrer tatsächlichen Leistung, zeigten ähnliche Grade der Überschätzung. Als ob die KI nicht nur die Kompetenzen, sondern auch die Unfähigkeit, sie genau zu bewerten, nivelliert hätte.

Der zugrunde liegende Mechanismus ist kontraintuitiv, aber logisch. Der Dunning-Kruger-Effekt ergibt sich aus der Korrelation zwischen der Fähigkeit in der Aufgabe und der metakognitiven Fähigkeit, d. h. der Fähigkeit, die Qualität des eigenen Denkens zu überwachen. Wer gut darin ist, logische Probleme zu lösen, ist auch gut darin zu erkennen, wann seine Lösung stichhaltig ist oder zusammenbricht. Aber wenn ChatGPT ins Spiel kommt, bricht diese Korrelation zusammen: Die KI liefert allen relativ gleichwertige Ergebnisse, unabhängig von der Fähigkeit des einzelnen Benutzers, deren Gültigkeit zu beurteilen. Das Ergebnis ist eine nach oben nivellierte Leistung und eine nach unten nivellierte Metakognition. Alle verbessern sich, aber niemand weiß wirklich, um wie viel.

Robin Welsch, Kognitionspsychologe und Leiter der Forschung, bemerkte, dass dies eine Umkehrung der in den 1960er Jahren von Doug Engelbart formulierten Hypothese der kognitiven Augmentation darstellt. Die ursprüngliche Idee war, dass Technologien den menschlichen Intellekt verstärken und dabei das kritische Bewusstsein erhalten oder verbessern könnten. Stattdessen erleben wir eine asymmetrische Verstärkung: Die Fähigkeiten wachsen, die Weisheit stagniert.

## Technologische Alphabetisierung als Bumerang

Wenn es ein Ergebnis der Studie gibt, das die gängigen Annahmen über das Verhältnis von Kompetenz und bewusstem Umgang mit Technologie auf den Kopf stellt, dann ist es die Korrelation zwischen KI-Alphabetisierung und metakognitiver Genauigkeit. Die Forscher maßen die KI-Alphabetisierung der Teilnehmer mit der SNAIL-Skala (Scale for the assessment of non-experts' AI literacy), die drei Dimensionen bewertet: technisches Verständnis der Funktionsweise von Systemen, Fähigkeit zur kritischen Bewertung von Ergebnissen und Geschicklichkeit in der praktischen Anwendung.

Was man erwarten würde, ist eine positive Beziehung: Wer KI besser kennt, sollte sie bewusster nutzen, ihre Grenzen erkennen und das Vertrauen in ihre Vorschläge besser kalibrieren. Stattdessen zeigen die Daten das Gegenteil. Teilnehmer mit höheren Werten in der Dimension "technisches Verständnis" waren systematisch ungenauer in ihren Selbsteinschätzungen. Je mehr sie über Prompt-Engineering, Modelltemperatur und Transformer-Architekturen wussten, desto mehr überschätzten sie die Qualität ihrer Arbeit mit KI.

Es ist ein Effekt, der an jene Szene aus *Primer*, dem Kultfilm von Shane Carruth über Zeitreisen, erinnert, in der die Protagonisten so sehr in die technische Komplexität ihrer Maschine vertieft sind, dass sie die Implikationen dessen, was sie bauen, völlig aus den Augen verlieren. Die Vertrautheit mit dem Mechanismus garantiert nicht die Weisheit im Umgang damit. Im Gegenteil, sie kann ein falsches Gefühl der Kontrolle erzeugen: "Ich weiß, wie dieses Ding funktioniert, also kann ich den Ergebnissen vertrauen, die es mir liefert."

Thomas Kosch, Experte für Mensch-Maschine-Interaktion und Mitautor der Studie, schlägt vor, dass dieses Paradoxon aus einem grundlegenden Missverständnis darüber resultiert, was es bedeutet, ein KI-System zu "kennen". Ein großes Sprachmodell technisch zu verstehen bedeutet zu wissen, dass es Text über Aufmerksamkeitsschichten verarbeitet, dass es das nächste Token durch Wahrscheinlichkeitsmaximierung vorhersagt, dass es auf spezifische Datensätze feinabgestimmt werden kann. Aber dieses prozedurale Wissen führt nicht automatisch zu epistemischem Bewusstsein: zu wissen, wann das Modell eine zuverlässige Antwort produziert und wann es einfach nur plausiblen Text generiert.

Die Forschungsgruppe fand eine positive Korrelation zwischen allen drei Faktoren der SNAIL-Skala und dem durchschnittlichen Vertrauen der Teilnehmer. Wer mehr wusste, fühlte sich sicherer. Aber dieses Vertrauen führte nicht zu einer besseren Fähigkeit, zwischen richtigen und falschen Antworten zu unterscheiden: Der Bereich unter der ROC-Kurve, der misst, wie gut Vertrauensurteile die tatsächliche Genauigkeit vorhersagen, blieb stabil oder nahm sogar mit zunehmender KI-Alphabetisierung ab. Je mehr man weiß, desto mehr vertraut man, aber man urteilt nicht unbedingt besser.
![performance2.jpg](performance2.jpg)
[Bild aus dem offiziellen Papier. Vergleich der Leistungsbewertungen zwischen der Stichprobe der Teilnehmer, die mit ChatGPT interagieren, und der Stichprobe der Teilnehmer ohne KI.](https://www.sciencedirect.com/science/article/pii/S0747563225002262)

## Der Zusammenbruch der Metakognition

Um zu verstehen, wie tief das durch die Nutzung von KI verursachte metakognitive Defizit ist, analysierten die Forscher nicht nur die globalen Leistungsschätzungen (Wie viele Fragen glauben Sie, richtig gelöst zu haben?), sondern auch die schrittweisen Vertrauensurteile: Nach jeder einzelnen Aufgabe bewerteten die Teilnehmer auf einer Skala von 0 bis 100, wie sicher sie sich ihrer Antwort waren.

Unter normalen Bedingungen sollten diese Urteile prädiktiv sein: hohes Vertrauen bei richtigen Antworten, niedriges bei falschen. Es ist das Signal, dass das metakognitive System funktioniert, dass es eine Rückkopplungsschleife zwischen dem Entscheidungsprozess und der Überwachung der Qualität dieses Prozesses gibt. Aber in der Gruppe, die ChatGPT verwendete, war diese Korrelation deutlich abgeschwächt. Das durchschnittliche Vertrauen bei richtigen Antworten war nur geringfügig höher als bei falschen Antworten, mit einem durchschnittlichen AUC von 0,62, kaum über dem Zufallsniveau und weit unter der Schwelle von 0,70, die Psychologen als Indikator für "mäßige metakognitive Sensitivität" betrachten.

In der Praxis fühlten sich die Teilnehmer mehr oder weniger immer sicher, unabhängig davon, ob sie die richtige oder die falsche Antwort gaben. Es ist, als würde man mit abgedecktem Armaturenbrett fahren: Das Fahrzeug funktioniert, aber man hat keine Ahnung, ob man mit dreißig oder hundertdreißig Stundenkilometern unterwegs ist.

Dieser Zusammenbruch der metakognitiven Sensitivität hat Implikationen, die weit über Logiktests hinausgehen. Denken Sie an einen Arzt, der ein diagnostisches KI-System verwendet: Wenn seine Fähigkeit, zwischen zuverlässigen und zweifelhaften Diagnosen zu unterscheiden, durch die Leichtigkeit beeinträchtigt wird, mit der die KI formal überzeugende Ergebnisse liefert, ist das Ergebnis keine Verbesserung der klinischen Praxis, sondern eine potenzielle Risikoerhöhung. Oder an einen Studenten, der sich mit ChatGPT auf eine Prüfung vorbereitet, um Probleme zu lösen: Wenn der sofortige Zugang zu Lösungen ihn davon überzeugt, den Stoff verstanden zu haben, wird er mit einem illusorischen Gefühl der Vorbereitung in die Prüfung gehen.

Das Phänomen wird durch das verschärft, was Stephen Fleming, ein kognitiver Neurowissenschaftler am University College London, als "Flüssigkeitsheuristik" bezeichnet: Wenn eine Information schnell und mühelos verarbeitet wird, interpretiert das Gehirn dies als ein Zeichen von Kompetenz. ChatGPT liefert aufwändige, artikulierte, tadellos formatierte und fast sofortige Antworten. Jeder Aspekt dieser Erfahrung schreit unserem intuitiven System "das ist richtig" zu und umgeht die bewusste Analyse, die wir normalerweise bei unsicheren Informationen aktivieren.

## Intelligente Schnittstellen neu denken

Die Frage lautet also: Wenn KI die Leistung tatsächlich verbessert, aber das kritische Bewusstsein verschlechtert, wie gestalten wir Systeme, die die Vorteile erhalten und den Schaden minimieren? Die Antwort kann sich nicht auf "weniger nutzen" oder "weniger vertrauen" beschränken. Der Geist ist aus der Flasche, und Dutzende von Millionen Menschen nutzen täglich Werkzeuge wie ChatGPT, Claude und Gemini, um zu argumentieren, zu schreiben und Probleme zu lösen.

Die Forscher schlagen vor, was sie "Explain-back-Mikroaufgabe" nennen: Bevor eine von der KI generierte Antwort akzeptiert wird, muss der Benutzer sie mit eigenen Worten neu formulieren und die zugrunde liegende Logik explizit machen. Es ist eine Technik, die den Übergang von einem oberflächlichen Lernen, das auf der Erkennung von Mustern basiert, zu einem tiefen Lernen, das auf strukturellem Verständnis basiert, erzwingt. Wenn Sie ChatGPT bitten, ein Physikproblem zu lösen, und dann mit eigenen Worten erklären müssen, warum diese Lösung funktioniert, zwingt Sie das System zu einer direkten Konfrontation mit Ihrem tatsächlichen Verständnis.

Eine weitere vielversprechende Richtung betrifft die explizite Kalibrierung des Vertrauens. Anstatt den Benutzer die Zuverlässigkeit einer Antwort implizit aus dem selbstbewussten Ton des Modells ableiten zu lassen, könnte die Benutzeroberfläche Unsicherheitsmetriken anzeigen: Wie "sicher" das Modell der Antwort in Bezug auf die Wahrscheinlichkeitsverteilung ist, ob es plausible alternative Antworten gibt, wie sehr die Frage vom Trainingsdatensatz abweicht. Es ist keine perfekte Lösung, da Benutzer dazu neigen, diese Signale zu ignorieren, wenn die Ausgabe überzeugend erscheint, aber es führt zumindest ein Element der kognitiven Reibung ein.

Experten für Mensch-KI-Interaktion erforschen auch radikalere Architekturen wie metakognitive Agenten: sekundäre KI-Systeme, deren Aufgabe es nicht ist, Ihnen bei der Lösung des Problems zu helfen, sondern zu überwachen, wie Sie das Problem mit Hilfe der ersten KI lösen. Eine Art kognitiver Supervisor, der eingreift, wenn er Muster von übermäßigem Vertrauen oder unkritischer Akzeptanz erkennt. Das klingt nach Science-Fiction, aber einige vorläufige Experimente zeigen eine signifikante Reduzierung der Leistungsüberschätzung, wenn diese Art von metakognitivem Gerüst implementiert wird.

Falk Lieder, Forscher am Max-Planck-Institut, hat einen noch ehrgeizigeren Rahmen vorgeschlagen: Schnittstellen, die sich dynamisch an das metakognitive Kompetenzniveau des Benutzers anpassen. Wenn das System erkennt, dass Sie den Ergebnissen zu blind vertrauen, erhöht es den Grad der Mehrdeutigkeit der Antworten, fügt deutlichere Unsicherheitssignale ein und fordert Sie auf, Ihre Entscheidungen zu rechtfertigen. Wenn Sie hingegen kritische Fähigkeiten zeigen, zieht es sich zurück und lässt mehr Autonomie. Es ist ein Modell, das die aktuelle Logik umkehrt, bei der die KI versucht, immer flüssiger und unterstützender zu sein, um stattdessen eine Strategie der "kalibrierten Unterstützung" einzuführen.

Das Risiko besteht natürlich darin, dass diese Eingriffe eine solche Reibung erzeugen, dass die Werkzeuge weniger attraktiv werden. Wenn ich jedes Mal, wenn ich ChatGPT um Hilfe bei einem Problem bitte, zuerst einen metakognitiven Test durchlaufen muss, bevor ich die Antwort sehe, werde ich es wahrscheinlich nicht mehr benutzen. Aber vielleicht ist das genau der Punkt: Das Ziel sollte nicht sein, das Engagement mit der KI zu maximieren, sondern die kognitive Augmentation im vollsten Sinne zu optimieren, einschließlich der metakognitiven Dimension.

Janet Rafner, die die soziotechnischen Implikationen von KI am Kopenhagener Institut untersucht, schlägt vor, dass das Problem Lösungen auf mehreren Ebenen erfordert. Kurzfristig bessere Schnittstellendesigns. Mittelfristig Bildungsprogramme, die nicht nur technische KI-Alphabetisierung, sondern auch metakognitive Alphabetisierung vermitteln. Langfristig eine kulturelle Neubewertung dessen, was Kompetenz in einer Welt bedeutet, in der der Zugang zu externen kognitiven Fähigkeiten allgegenwärtig ist. Es ist eine Herausforderung, die an die erinnert, mit der die Schrift selbst vor Jahrtausenden konfrontiert war: Wenn Ihnen das externe Gedächtnis des geschriebenen Wortes zur Verfügung steht, was passiert dann mit dem internen Gedächtnis? Und vor allem, was passiert mit Ihrem Bewusstsein dafür, was Sie wirklich erinnern und was Sie nur abgeschrieben haben?

Die Forschung der Aalto-Universität bietet keine endgültigen Lösungen, aber sie identifiziert präzise ein Problem, das Gefahr lief, unter der Begeisterung für Leistungssteigerungen unterzugehen. KI macht uns im rechnerischen Sinne intelligenter: Wir lösen mehr Probleme, schneller, mit weniger Fehlern. Aber sie macht uns nicht im metakognitiven Sinne weiser: Wir verbessern nicht unsere Fähigkeit, unsere eigenen kognitiven Prozesse kritisch zu bewerten, zu erkennen, wann wir wirklich verstehen und wann wir nur Verständnis simulieren.

Es ist der Unterschied zwischen Deckard und den Replikanten in *Blade Runner*: Sie alle besitzen Erinnerungen, aber nur einige wissen, welche authentisch ihre eigenen sind. In unserem Fall erhalten wir alle bessere Antworten mit KI, aber wir laufen Gefahr, die Fähigkeit zu verlieren, zwischen echtem Verständnis und geliehener Kompetenz zu unterscheiden. Und in einer Welt, in der immer mehr wichtige Entscheidungen mit Hilfe intelligenter Systeme getroffen werden, könnte dieser Unterschied den entscheidenden Unterschied ausmachen.
