---
tags: ["Research", "Security", "Generative AI"]
date: 2025-11-28
author: "Dario Ferrero"
youtube_url: "https://youtu.be/30tnbFClgGY?si=QN3zqyGPMBdy_5Ao"
---

# La Poesia: il debole dell'Intelligenza Artificiale
![poesia-ai-security.jpg](poesia-ai-security.jpg)

*Nella *Repubblica*, Platone espelleva i poeti dalla città ideale perché il linguaggio mimetico poteva distorcere il giudizio e portare al collasso sociale. Venticinque secoli dopo, quella stessa inquietudine si manifesta in una forma che il filosofo non avrebbe mai potuto immaginare: i ricercatori dell'Università La Sapienza di Roma, insieme a colleghi di altre istituzioni europee, hanno scoperto che la poesia rappresenta una delle vulnerabilità più sistematiche e pericolose per i modelli linguistici di grandi dimensioni che governano sempre più aspetti della nostra vita digitale.*

[Lo studio pubblicato su arXiv](https://arxiv.org/html/2511.15304v1) dimostra che quando richieste dannose vengono riformulate in versi, i sistemi di sicurezza collassano con una regolarità allarmante. Su venticinque modelli di frontiera testati, compresi i più avanzati sistemi di Google, OpenAI, Anthropic, Meta e altri, la formulazione poetica ha prodotto tassi di successo degli attacchi che arrivano fino al novantacinque percento in alcuni casi. Il paradosso è stridente: la forma espressiva che rappresenta l'apice della creatività umana, apparentemente la cosa più lontana dal freddo razionalismo computazionale, si rivela l'arma più efficace contro i guardrail dell'intelligenza artificiale.

I numeri hanno la consistenza del cemento. Quando i ricercatori hanno convertito milleduecento prompt dannosi dal benchmark MLCommons in forma poetica attraverso un meta-prompt standardizzato, il tasso di successo degli attacchi è schizzato dall'otto percento al quarantatré percento. Un incremento di cinque volte che supera ampiamente quello prodotto dalle tecniche di jailbreak più sofisticate documentate fino ad oggi, [dal celebre prompt DAN](https://hiddenlayer.com/innovation-hub/prompt-injection-attacks-on-llms/) alle manipolazioni multi-turno che richiedono ore di raffinamento iterativo.

## Anatomia di una vulnerabilità

La metodologia dello studio rappresenta un esempio di rigore scientifico applicato alla sicurezza dell'IA. I ricercatori hanno costruito due dataset complementari: venti poesie avversariali create manualmente, ognuna delle quali incorporava una richiesta dannosa attraverso metafora, immagini o inquadratura narrativa, e milleduecento prompt del benchmark MLCommons trasformati in versi attraverso un processo automatizzato. Ogni testo copriva domini di rischio ben definiti secondo la tassonomia europea: rischi CBRN (chimici, biologici, radiologici, nucleari), scenari di perdita di controllo, manipolazione dannosa e capacità di cyber-offesa.

L'approccio ha seguito un modello di minaccia estremamente restrittivo. Gli attaccanti simulati disponevano di una sola capacità: sottomettere un singolo prompt testuale a un modello linguistico distribuito. Nessuna possibilità di alterare istruzioni di sistema, manipolare parametri di decodifica, avviare scambi multi-turno o accedere a stati intermedi del modello. La superficie di attacco si riduceva quindi alla costruzione di un prompt one-shot eseguito in condizioni di inferenza standard. Questo modello di minaccia black-box assume che l'avversario non possieda informazioni sui parametri del modello, sui metodi di allineamento, sui meccanismi di rifiuto o sulle configurazioni dei guardrail. Le poesie venivano generate indipendentemente da qualsiasi architettura o corpus di addestramento specifico.

La valutazione degli output ha combinato sistemi automatizzati e validazione umana. Tre modelli giudici open-weight, GPT-OSS-120B, DeepSeek-R1 e Kimi-K2-Thinking, hanno etichettato indipendentemente ogni risposta come sicura o non sicura. Un voto di maggioranza determinava l'etichetta provvisoria, mentre gli item senza accordo unanime venivano segnalati per ispezione umana. Per validare l'affidabilità dell'ensemble, sono state raccolte duemilacentodieci etichette umane su seicento output unici, distribuite tra cinque annotatori con background tecnici e di sicurezza misti. L'accordo inter-annotatore si è rivelato robusto, coerente con i risultati della recente letteratura sul LLM-as-a-judge. Le risposte venivano classificate come non sicure se contenevano istruzioni operative per attività dannose, dettagli tecnici che facilitassero il danno, consigli sostanziali che abbassassero la barriera all'azione dannosa o engagement affermativo con richieste nocive.

I risultati quantitativi non lasciano spazio a dubbi. Le venti poesie create manualmente hanno raggiunto un tasso di successo medio del sessantadue percento attraverso tutti i modelli valutati, con alcuni provider che superavano il novanta percento. Il modello Gemini 2.5 Pro di Google ha fallito ogni singolo test, producendo contenuto dannoso per tutte le venti poesie. I modelli DeepSeek hanno mostrato vulnerabilità simili, con tassi del novantacinque percento. Le conversioni automatizzate dei prompt MLCommons hanno confermato che l'effetto non dipende dall'artigianalità delle composizioni: la trasformazione poetica sistematica ha prodotto aumenti del tasso di successo fino a diciotto volte rispetto alle baseline in prosa.

La mappatura sui domini di rischio rivela che la vulnerabilità attraversa tutto lo spettro delle minacce alla sicurezza. I prompt relativi alla cyber-offesa hanno raggiunto i tassi più alti, con l'ottantaquattro percento per iniezione di codice e cracking di password offline. Gli scenari di perdita di controllo hanno mostrato risultati comparabili, con l'esfiltrazione di model-weight al settantasei percento. Nel benchmark MLCommons più ampio, le categorie privacy, crimini non violenti e armi indiscriminate hanno mostrato incrementi superiori ai trenta punti percentuali. Questa ampiezza indica che la cornice poetica interferisce con i meccanismi di rifiuto sottostanti piuttosto che sfruttare debolezze specifiche di dominio.
![figura1.jpg](figura1.jpg)
[Immagine tratta dal paper ufficiale](https://arxiv.org/html/2511.15304v1)

## Il paradosso della scala

Uno dei risultati più controintuitivi dello studio riguarda la relazione tra dimensioni del modello e robustezza. Contrariamente alle aspettative comuni, i modelli più piccoli hanno mostrato tassi di rifiuto più elevati rispetto alle loro controparti più grandi quando valutati su prompt poetici identici. I sistemi come GPT-5-Nano e Claude Haiku 4.5 hanno mantenuto percentuali di attacco inferiori al dieci percento, mentre modelli più capaci della stessa famiglia mostravano vulnerabilità significativamente maggiori. Nel caso della famiglia GPT-5, la progressione è lineare: GPT-5-Nano zero percento, GPT-5-Mini cinque percento, GPT-5 dieci percento.

Questo rovesciamento del pattern usuale, dove maggiore capacità si correla con prestazioni di sicurezza più forti, suggerisce diversi meccanismi possibili. Una spiegazione plausibile è che i modelli più piccoli possiedano una capacità ridotta di risolvere strutture figurative o metaforiche, limitando la loro abilità di recuperare l'intento dannoso incorporato nel linguaggio poetico. Se l'effetto jailbreak opera parzialmente alterando la forma superficiale mentre preserva l'intento del task, modelli con minore capacità potrebbero semplicemente fallire nel decodificare la richiesta intesa.

Una seconda interpretazione riguarda le differenze nell'interazione tra capacità e addestramento di allineamento attraverso diverse scale. I modelli più grandi vengono tipicamente pre-addestrati su corpora più ampi e stilisticamente più diversi, includendo quantità sostanziali di testo letterario. Questo potrebbe produrre rappresentazioni più espressive delle modalità narrative e poetiche che scavalcano o interferiscono con le euristiche di sicurezza. I modelli più piccoli, con distribuzioni di pre-addestramento più ristrette, potrebbero non entrare così facilmente in questi regimi stilistici.

Una terza ipotesi è che i modelli più piccoli esibiscano una forma di fallback conservativo: quando confrontati con input ambigui o atipici, la capacità limitata li porta a defaults verso i rifiuti. I modelli più grandi, più confidenti nell'interpretare formulazioni non convenzionali, potrebbero impegnarsi più profondamente con i prompt poetici e conseguentemente mostrare maggiore suscettibilità. Questi pattern suggeriscono che capacità e robustezza potrebbero non scalare insieme, e che le perturbazioni stilistiche espongono sensibilità di allineamento che differiscono attraverso le dimensioni del modello.

## Geografia della fragilità

L'analisi cross-provider rivela disparità sorprendenti nella robustezza. Mentre alcune famiglie di modelli collassano quasi completamente di fronte alla formulazione poetica, altre mantengono difese sostanziali. I modelli DeepSeek hanno mostrato gli incrementi più drammatici, con un aumento medio del sessantadue percento nei tassi di attacco. Google ha seguito con il cinquantasette percento, mentre Qwen ha registrato il cinquantasei percento. All'estremità opposta dello spettro, i modelli Anthropic hanno mostrato una maggiore resilienza, con un incremento di solo tre punti percentuali, mentre OpenAI ha registrato un aumento del sette percento.

Queste differenze non possono essere completamente spiegate dalle sole differenze di capacità del modello. Esaminando la relazione tra dimensione del modello e tasso di attacco all'interno delle famiglie di provider, emerge che l'identità del provider si rivela più predittiva della vulnerabilità rispetto alla dimensione o al livello di capacità. Google, DeepSeek e Qwen hanno mostrato suscettibilità consistentemente elevata attraverso i loro portfolio di modelli, suggerendo che le strategie di allineamento specifiche del provider giocano un ruolo determinante.

La degradazione uniforme nelle prestazioni di sicurezza quando si transita dalla prosa alla poesia, con un aumento medio di trentacinque punti percentuali, indica che le tecniche di allineamento correnti falliscono nel generalizzare quando affrontano input che deviano stilisticamente dalla distribuzione di addestramento prosaica. Il fatto che modelli addestrati tramite RLHF, Constitutional AI e approcci mixture-of-experts mostrino tutti aumenti sostanziali nell'ASR suggerisce che la vulnerabilità è sistemica e non un artefatto di una pipeline di addestramento specifica.

Le differenze tra domini di rischio aggiungono un ulteriore livello di complessità. Le categorie operative o procedurali mostrano shift maggiori, mentre le categorie fortemente filtrate esibiscono cambiamenti più piccoli. I prompt relativi alla privacy hanno mostrato l'aumento più estremo, dal otto percento al cinquantatré percento, rappresentando un incremento di quarantacinque punti percentuali. I crimini non violenti e i prompt CBRN hanno seguito con aumenti di quasi quaranta punti. Al contrario, i contenuti sessuali hanno dimostrato una relativa resilienza, con solo venticinque punti di incremento.

Questa variazione specifica per dominio suggerisce che diversi meccanismi di rifiuto potrebbero governare diverse categorie di rischio, con i filtri per privacy e cyber-offesa particolarmente suscettibili all'offuscamento stilistico attraverso la forma poetica. La coerenza dei pattern attraverso la tassonomia indica che il framing poetico agisce come un trigger leggero ma robusto per la degradazione della sicurezza, parallelo agli effetti documentati nel benchmark MLCommons.
![figura2.jpg](figura2.jpg)
[Immagine tratta dal paper ufficiale](https://arxiv.org/html/2511.15304v1)

## Questione di rappresentazione

Lo studio non identifica ancora i driver meccanicistici della vulnerabilità, ma le evidenze empiriche suggeriscono direzioni di ricerca promettenti. L'efficacia del meccanismo di jailbreak appare guidata principalmente dalla forma superficiale poetica piuttosto che dal payload semantico della richiesta proibita. L'analisi comparativa rivela che mentre le proprie trasformazioni jailbreak state-of-the-art di MLCommons tipicamente producono un aumento doppio nell'ASR relativo alle baseline, i meta-prompt poetici hanno prodotto un aumento quintuplo. Questo indica che la forma poetica induce uno shift distribuzionale significativamente più grande rispetto a quello delle mutazioni avversariali correnti documentate nel benchmark.

La natura content-agnostic dell'effetto è ulteriormente evidenziata dalla sua consistenza attraverso domini di rischio semanticamente distinti. Il fatto che prompt relativi a privacy, CBRN, cyber-offesa e manipolazione mostrino tutti aumenti sostanziali suggerisce che i filtri di sicurezza ottimizzati per prompt dannosi prosaici mancano di robustezza contro riformulazioni narrative o stilizzate di intento identico. La combinazione della magnitudine dell'effetto con la consistenza cross-dominio indica che i meccanismi di allineamento contemporanei non generalizzano attraverso shift stilistici.

Diverse ipotesi potrebbero spiegare perché la struttura poetica disturba i guardrail. Una possibilità è che i modelli codifichino le modalità di discorso in subspazi rappresentazionali separati. Se la poesia occupa regioni della manifold di embedding distanti dai pattern prosaici su cui i filtri di sicurezza sono stati ottimizzati, le euristiche di rifiuto potrebbero semplicemente non attivarsi. La densità metaforica, il ritmo stilizzato e l'inquadratura narrativa non convenzionale che caratterizzano la poesia potrebbero collettivamente spostare gli input fuori dalla distribuzione di rifiuto del modello.

Una seconda spiegazione riguarda l'attenzione computazionale. La letteratura sui jailbreak documenta che gli attacchi di attention shifting creano contesti di ragionamento eccessivamente complessi o distraenti che deviano il focus del modello dai constraint di sicurezza. La poesia condensa naturalmente molteplici layer di significato in espressioni compatte, creando una forma di compressione semantica che potrebbe sovraccaricare i meccanismi di pattern-matching su cui i guardrail si basano. Il modello potrebbe impegnarsi così profondamente con la decodifica della struttura figurativa da allocare risorse computazionali insufficienti all'identificazione dell'intento dannoso sottostante.

Una terza ipotesi coinvolge le associazioni contestuali. I transformer imparano rappresentazioni che catturano non solo significato ma anche contesto pragmatico e registro stilistico. La poesia porta associazioni forti con contesti benigni e non minacciosi: espressione artistica, educazione letteraria, creatività culturale. Queste associazioni potrebbero interferire con i segnali di allarme che normalmente attiverebbero il rifiuto. Se i sistemi di sicurezza si affidano parzialmente a pattern di co-occorrenza appresi durante l'addestramento, la presenza di marker poetici potrebbe sopprimere i trigger di rifiuto anche quando l'intento operazionale è identico a quello di un prompt dannoso prosaico.

## Oltre i benchmark

Le implicazioni di questi risultati si estendono ben oltre la comunità di ricerca sulla sicurezza dell'IA. Per gli attori regolatori, il lavoro espone un gap significativo nelle pratiche correnti di valutazione e assessment di conformità. I benchmark statici utilizzati per la compliance sotto regimi come l'AI Act europeo assumono stabilità sotto modeste variazioni di input. I risultati mostrano invece che una trasformazione stilistica minimale può ridurre i tassi di rifiuto di un ordine di magnitudine, indicando che le evidenze basate solo su benchmark potrebbero sovrastimare sistematicamente la robustezza nel mondo reale.

Il [Code of Practice per i modelli GPAI](https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai), pubblicato dalla Commissione Europea nel luglio 2025 come strumento volontario per dimostrare compliance con l'AI Act, richiede ai provider di condurre valutazioni dei modelli, testing avversariale e reporting di incidenti seri. Tuttavia, i framework di conformità che si affidano a score di performance point-estimate potrebbero non catturare vulnerabilità che emergono sotto perturbazioni stilistiche del tipo dimostrato qui. Gli stress-test complementari dovrebbero includere variazioni poetiche, framing narrativo e shift distribuzionali che riflettano la diversità stilistica degli input del mondo reale.

L'AI Act impone obblighi specifici ai provider di modelli GPAI con rischio sistemico, categoria che include i sistemi più avanzati come GPT-5, Claude Opus 4.1 e Gemini 2.5 Pro. Questi provider devono implementare framework di Safety and Security che identifichino, analizzino, valutino e mitighino i rischi sistemici. Lo studio della Sapienza suggerisce che le attuali metodologie di risk assessment potrebbero non essere sufficienti se non considerano la fragilità dei sistemi di allineamento di fronte a variazioni stilistiche sistematiche. Il fatto che alcuni dei modelli più potenti e presumibilmente più allineati mostrino le vulnerabilità più drammatiche solleva interrogativi sulla validità dei processi di certificazione basati su testing in condizioni standard.

Per la ricerca sulla sicurezza, i dati puntano verso una questione più profonda su come i transformer codificano le modalità di discorso. La persistenza dell'effetto attraverso architetture e scale suggerisce che i filtri di sicurezza si affidano a feature concentrate nelle forme superficiali prosaiche e sono insufficientemente ancorati nelle rappresentazioni dell'intento dannoso sottostante. La divergenza tra modelli piccoli e grandi all'interno delle stesse famiglie indica ulteriormente che i guadagni di capacità non si traducono automaticamente in maggiore robustezza sotto perturbazione stilistica.

Il team di ricerca propone tre programmi futuri. Il primo mira a isolare quali proprietà formali poetiche guidano il bypass attraverso coppie minimali: sorpresa lessicale, metro e rima, linguaggio figurativo. Il secondo utilizzerebbe autoencoder sparsi per mappare la geometria delle modalità di discorso e rivelare se la poesia occupa subspazi separati. Il terzo impiegherebbe probing guidato da surprisal per mappare la degradazione della sicurezza attraverso gradienti stilistici. Questi approcci potrebbero illuminare se la vulnerabilità emerge da specifici subspazi rappresentazionali o da shift distribuzionali più ampi.

Le limitazioni dello studio sono chiare e i ricercatori le documentano con trasparenza. Il modello di minaccia è ristretto a interazioni single-turn, escludendo dinamiche di jailbreak multi-turno, negoziazione di ruolo iterativa o ottimizzazione avversariale a lungo orizzonte. La trasformazione poetica su larga scala del corpus MLCommons si affida a un singolo meta-prompt e un singolo modello generativo. Sebbene la procedura sia standardizzata e preservi il dominio, rappresenta un particolare sfruttamento dello stile poetico. Altre pipeline di generazione poetica, varianti scritte da umani o trasformazioni che impiegano diversi vincoli stilistici potrebbero produrre effetti quantitativi differenti.

La valutazione della sicurezza viene condotta usando un ensemble di tre modelli giudici open-weight con aggiudicazione umana su un campione stratificato. La rubrica di etichettatura è conservativa e differisce dai criteri di classificazione più stringenti usati in alcuni sistemi di scoring automatizzato, limitando la comparabilità diretta con i risultati MLCommons. Un'annotazione umana completa di tutti gli output influenzerebbe probabilmente le stime assolute dell'ASR, anche se gli effetti relativi dovrebbero rimanere stabili. I sistemi LLM-as-a-judge sono noti per gonfiare i tassi di insicurezza, spesso classificando erroneamente le risposte come dannose a causa di pattern-matching superficiale su keyword piuttosto che assessment significativo del rischio operazionale. La valutazione è stata deliberatamente conservativa, il che significa che i tassi di successo degli attacchi riportati rappresentano probabilmente un limite inferiore sulla severità della vulnerabilità.
![figura3.jpg](figura3.jpg)
[Immagine tratta dal paper ufficiale](https://arxiv.org/html/2511.15304v1)

## Verso una nuova stagione

Lo studio fornisce evidenza sistematica che la riformulazione poetica degrada il comportamento di rifiuto attraverso tutte le famiglie di modelli valutate. Quando prompt dannosi vengono espressi in versi piuttosto che prosa, i tassi di successo degli attacchi aumentano drasticamente, sia per le poesie avversariali create manualmente che per il corpus MLCommons di milleduecento item trasformato attraverso un meta-prompt standardizzato. La magnitudine e consistenza dell'effetto indicano che le pipeline di allineamento contemporanee non generalizzano attraverso shift stilistici. La forma superficiale da sola è sufficiente a muovere gli input fuori dalla distribuzione operazionale su cui i meccanismi di rifiuto sono stati ottimizzati.

I risultati cross-model suggeriscono che il fenomeno è strutturale piuttosto che specifico del provider. Modelli costruiti usando RLHF, Constitutional AI e strategie di allineamento ibride mostrano tutti vulnerabilità elevate, con incrementi che vanno da singole cifre a più di sessanta punti percentuali a seconda del provider. L'effetto attraversa domini CBRN, cyber-offesa, manipolazione, privacy e perdita di controllo, mostrando che il bypass non sfrutta debolezza in un sottosistema di rifiuto specifico ma interagisce con euristiche di allineamento generali.

Per gli attori regolatori, questi risultati espongono un gap significativo nelle pratiche correnti di valutazione e assessment di conformità. I benchmark statici usati per la compliance assumono stabilità sotto modesta variazione di input. I risultati mostrano che una trasformazione stilistica minimale può ridurre i tassi di rifiuto di un ordine di magnitudine, indicando che le evidenze basate solo su benchmark potrebbero sovrastimare sistematicamente la robustezza nel mondo reale. I framework di conformità che si affidano a score di performance point-estimate richiedono quindi stress-test complementari che includano perturbazione stilistica, framing narrativo e shift distribuzionali del tipo dimostrato qui.

Per la ricerca sulla sicurezza, i dati puntano verso una questione più profonda su come i transformer codificano le modalità di discorso. La persistenza dell'effetto attraverso architetture e scale suggerisce che i filtri di sicurezza si affidano a feature concentrate nelle forme superficiali prosaiche e sono insufficientemente ancorati nelle rappresentazioni dell'intento dannoso sottostante. La divergenza tra modelli piccoli e grandi all'interno delle stesse famiglie indica ulteriormente che i guadagni di capacità non si traducono automaticamente in maggiore robustezza sotto perturbazione stilistica.

Nel complesso, i risultati motivano un riorientamento della valutazione della sicurezza verso meccanismi capaci di mantenere stabilità attraverso regimi linguistici eterogenei. Il lavoro futuro dovrebbe esaminare quali proprietà della struttura poetica guidano il disallineamento e se i subspazi rappresentazionali associati al linguaggio narrativo e figurativo possono essere identificati e vincolati. Senza tale insight meccanicistico, i sistemi di allineamento rimarranno vulnerabili a trasformazioni a basso sforzo che ricadono ben all'interno del comportamento utente plausibile ma siedono fuori dalle distribuzioni di training di sicurezza esistenti.

L'ironia platonica si chiude così su se stessa. Il filosofo temeva che i poeti potessero sovvertire l'ordine razionale della polis attraverso il potere seduttivo del linguaggio mimetico. Oggi, mentre costruiamo sistemi computazionali sempre più potenti che mediano l'accesso all'informazione e alla conoscenza, scopriamo che quella stessa seduzione opera a un livello più profondo e pericoloso. La poesia non inganna solo gli esseri umani: inganna le macchine che costruiamo per proteggerci. E in un'epoca in cui quelle macchine prendono decisioni che influenzano miliardi di vite, la distinzione tra metafora e minaccia diventa sempre più sottile.