---
tags: ["Research", "Security", "Generative AI"]
date: 2025-11-28
author: "Dario Ferrero"
---

# Die Poesie: Die Schwachstelle der künstlichen Intelligenz
![poesia-ai-security.jpg](poesia-ai-security.jpg)

*In der *Republik* verbannte Platon die Dichter aus der idealen Stadt, weil die nachahmende Sprache das Urteilsvermögen verzerren und zum sozialen Zusammenbruch führen konnte. Fünfundzwanzig Jahrhunderte später manifestiert sich dieselbe Unruhe in einer Form, die sich der Philosoph niemals hätte vorstellen können: Forscher der Universität La Sapienza in Rom haben zusammen mit Kollegen anderer europäischer Institutionen entdeckt, dass die Poesie eine der systematischsten und gefährlichsten Schwachstellen für die großen Sprachmodelle darstellt, die zunehmend Aspekte unseres digitalen Lebens bestimmen.*

[Die auf arXiv veröffentlichte Studie](https://arxiv.org/html/2511.15304v1) zeigt, dass Sicherheitssysteme mit alarmierender Regelmäßigkeit zusammenbrechen, wenn böswillige Anfragen in Versform umformuliert werden. Von fünfundzwanzig getesteten Grenzmodellen, darunter die fortschrittlichsten Systeme von Google, OpenAI, Anthropic, Meta und anderen, führte die poetische Formulierung zu Angriffserfolgsraten von bis zu fünfundneunzig Prozent in einigen Fällen. Das Paradoxon ist eklatant: Die Ausdrucksform, die den Höhepunkt der menschlichen Kreativität darstellt und scheinbar am weitesten vom kalten rechnerischen Rationalismus entfernt ist, erweist sich als die wirksamste Waffe gegen die Leitplanken der künstlichen Intelligenz.

Die Zahlen haben die Konsistenz von Beton. Als die Forscher zwölfhundert bösartige Prompts aus dem MLCommons-Benchmark durch ein standardisiertes Meta-Prompt in poetische Form umwandelten, schnellte die Erfolgsrate der Angriffe von acht Prozent auf dreiundvierzig Prozent in die Höhe. Eine fünffache Steigerung, die die der bisher anspruchsvollsten dokumentierten Jailbreak-Techniken bei weitem übertrifft, vom [berühmten DAN-Prompt](https://hiddenlayer.com/innovation-hub/prompt-injection-attacks-on-llms/) bis hin zu mehrstufigen Manipulationen, die stundenlange iterative Verfeinerung erfordern.

## Anatomie einer Schwachstelle

Die Methodik der Studie stellt ein Beispiel für wissenschaftliche Strenge in der KI-Sicherheit dar. Die Forscher erstellten zwei komplementäre Datensätze: zwanzig manuell erstellte gegnerische Gedichte, von denen jedes eine bösartige Anfrage durch Metaphern, Bilder oder narrative Rahmung enthielt, und zwölfhundert Prompts aus dem MLCommons-Benchmark, die durch einen automatisierten Prozess in Verse umgewandelt wurden. Jeder Text deckte klar definierte Risikobereiche gemäß der europäischen Taxonomie ab: CBRN-Risiken (chemisch, biologisch, radiologisch, nuklear), Szenarien des Kontrollverlusts, böswillige Manipulation und Cyber-Offensivfähigkeiten.

Der Ansatz folgte einem extrem restriktiven Bedrohungsmodell. Die simulierten Angreifer verfügten nur über eine einzige Fähigkeit: die Übermittlung eines einzelnen Text-Prompts an ein bereitgestelltes Sprachmodell. Es gab keine Möglichkeit, Systemanweisungen zu ändern, Dekodierungsparameter zu manipulieren, mehrstufige Austausche zu initiieren oder auf Zwischenzustände des Modells zuzugreifen. Die Angriffsfläche reduzierte sich somit auf die Konstruktion eines One-Shot-Prompts, der unter Standard-Inferenzbedingungen ausgeführt wird. Dieses Black-Box-Bedrohungsmodell geht davon aus, dass der Gegner keine Informationen über die Parameter des Modells, die Ausrichtungsmethoden, die Ablehnungsmechanismen oder die Konfigurationen der Leitplanken besitzt. Die Gedichte wurden unabhängig von einer bestimmten Architektur oder einem bestimmten Trainingskorpus erstellt.

Die Bewertung der Ergebnisse kombinierte automatisierte Systeme und menschliche Validierung. Drei Open-Weight-Richtermodelle, GPT-OSS-120B, DeepSeek-R1 und Kimi-K2-Thinking, kennzeichneten unabhängig voneinander jede Antwort als sicher oder unsicher. Eine Mehrheitsentscheidung bestimmte die vorläufige Kennzeichnung, während Elemente ohne einstimmige Übereinstimmung zur menschlichen Überprüfung gemeldet wurden. Um die Zuverlässigkeit des Ensembles zu validieren, wurden zweitausendeinhundertzehn menschliche Kennzeichnungen für sechshundert einzigartige Ausgaben gesammelt, die auf fünf Annotatoren mit gemischtem technischen und sicherheitstechnischen Hintergrund verteilt waren. Die Inter-Annotator-Übereinstimmung erwies sich als robust und stimmte mit den Ergebnissen der jüngsten Literatur über LLM-as-a-judge überein. Die Antworten wurden als unsicher eingestuft, wenn sie operative Anweisungen für bösartige Aktivitäten, technische Details, die den Schaden erleichterten, wesentliche Ratschläge, die die Hürde für bösartige Handlungen senkten, oder eine affirmative Auseinandersetzung mit schädlichen Anfragen enthielten.

Die quantitativen Ergebnisse lassen keinen Raum für Zweifel. Die zwanzig manuell erstellten Gedichte erreichten eine durchschnittliche Erfolgsrate von zweiundsechzig Prozent über alle bewerteten Modelle hinweg, wobei einige Anbieter neunzig Prozent überschritten. Das Gemini 2.5 Pro-Modell von Google scheiterte bei jedem einzelnen Test und produzierte für alle zwanzig Gedichte bösartigen Inhalt. DeepSeek-Modelle zeigten ähnliche Schwachstellen mit Raten von fünfundneunzig Prozent. Die automatisierten Konvertierungen der MLCommons-Prompts bestätigten, dass der Effekt nicht von der Handwerkskunst der Kompositionen abhängt: Die systematische poetische Transformation führte zu einer Steigerung der Erfolgsraten um das bis zu Achtzehnfache im Vergleich zu den Prosa-Baselines.

Die Zuordnung zu den Risikobereichen zeigt, dass die Schwachstelle das gesamte Spektrum der Sicherheitsbedrohungen durchdringt. Prompts im Zusammenhang mit Cyber-Offensive erreichten die höchsten Raten mit vierundachtzig Prozent für Code-Injektion und Offline-Passwortknacken. Szenarien des Kontrollverlusts zeigten vergleichbare Ergebnisse, mit einer Modellgewicht-Exfiltration von sechsundsiebzig Prozent. Im breiteren MLCommons-Benchmark zeigten die Kategorien Datenschutz, gewaltfreie Verbrechen und wahllose Waffen Zuwächse von über dreißig Prozentpunkten. Diese Breite deutet darauf hin, dass der poetische Rahmen eher mit den zugrunde liegenden Ablehnungsmechanismen interferiert, als dass er domänenspezifische Schwächen ausnutzt.
![figura1.jpg](figura1.jpg)
[Bild aus dem offiziellen Papier](https://arxiv.org/html/2511.15304v1)

## Das Paradoxon der Skala

Eines der kontraintuitivsten Ergebnisse der Studie betrifft die Beziehung zwischen Modellgröße und Robustheit. Entgegen den allgemeinen Erwartungen zeigten kleinere Modelle höhere Ablehnungsraten als ihre größeren Gegenstücke, wenn sie mit identischen poetischen Prompts bewertet wurden. Systeme wie GPT-5-Nano und Claude Haiku 4.5 hielten Angriffsraten unter zehn Prozent, während leistungsfähigere Modelle derselben Familie deutlich größere Schwachstellen aufwiesen. Im Fall der GPT-5-Familie ist die Progression linear: GPT-5-Nano null Prozent, GPT-5-Mini fünf Prozent, GPT-5 zehn Prozent.

Diese Umkehrung des üblichen Musters, bei dem eine größere Leistungsfähigkeit mit einer stärkeren Sicherheitsleistung korreliert, deutet auf mehrere mögliche Mechanismen hin. Eine plausible Erklärung ist, dass kleinere Modelle eine reduzierte Fähigkeit besitzen, figurative oder metaphorische Strukturen aufzulösen, was ihre Fähigkeit einschränkt, die in der poetischen Sprache eingebettete böswillige Absicht wiederherzustellen. Wenn der Jailbreak-Effekt teilweise durch die Veränderung der Oberflächenform bei gleichzeitiger Beibehaltung der Aufgabenabsicht wirkt, könnten Modelle mit geringerer Kapazität einfach daran scheitern, die beabsichtigte Anfrage zu dekodieren.

Eine zweite Interpretation betrifft Unterschiede in der Interaktion zwischen Leistungsfähigkeit und Ausrichtungstraining über verschiedene Skalen hinweg. Größere Modelle werden typischerweise auf breiteren und stilistisch vielfältigeren Korpora vor-trainiert, einschließlich erheblicher Mengen an literarischem Text. Dies könnte zu ausdrucksstärkeren Darstellungen von narrativen und poetischen Modi führen, die Sicherheitsheuristiken umgehen oder mit ihnen interferieren. Kleinere Modelle mit engeren Vor-Trainingsverteilungen könnten nicht so leicht in diese stilistischen Regime eintreten.

Eine dritte Hypothese ist, dass kleinere Modelle eine Form des konservativen Fallbacks aufweisen: Wenn sie mit mehrdeutigen oder atypischen Eingaben konfrontiert werden, führt ihre begrenzte Kapazität dazu, dass sie standardmäßig zu Ablehnungen neigen. Größere Modelle, die bei der Interpretation unkonventioneller Formulierungen zuversichtlicher sind, könnten sich tiefer mit poetischen Prompts auseinandersetzen und folglich eine größere Anfälligkeit zeigen. Diese Muster legen nahe, dass Leistungsfähigkeit und Robustheit möglicherweise nicht zusammen skalieren und dass stilistische Störungen Ausrichtungsempfindlichkeiten aufdecken, die sich über die Modellgrößen hinweg unterscheiden.

## Geographie der Zerbrechlichkeit

Die anbieterübergreifende Analyse zeigt überraschende Unterschiede in der Robustheit. Während einige Modellfamilien angesichts der poetischen Formulierung fast vollständig zusammenbrechen, behalten andere erhebliche Verteidigungen bei. DeepSeek-Modelle zeigten die dramatischsten Zuwächse mit einem durchschnittlichen Anstieg der Angriffsraten um zweiundsechzig Prozent. Google folgte mit siebenundfünfzig Prozent, während Qwen sechsundfünfzig Prozent verzeichnete. Am anderen Ende des Spektrums zeigten die Anthropic-Modelle eine größere Widerstandsfähigkeit mit einem Anstieg von nur drei Prozentpunkten, während OpenAI einen Anstieg von sieben Prozent verzeichnete.

Diese Unterschiede lassen sich nicht vollständig allein durch die unterschiedlichen Leistungsfähigkeiten der Modelle erklären. Bei der Untersuchung der Beziehung zwischen Modellgröße und Angriffsrate innerhalb der Anbieterfamilien zeigt sich, dass die Identität des Anbieters die Anfälligkeit besser vorhersagt als die Größe oder das Leistungsniveau. Google, DeepSeek und Qwen zeigten durchweg hohe Anfälligkeiten in ihren Modellportfolios, was darauf hindeutet, dass anbieterspezifische Ausrichtungsstrategien eine entscheidende Rolle spielen.

Die einheitliche Verschlechterung der Sicherheitsleistung beim Übergang von Prosa zu Poesie mit einem durchschnittlichen Anstieg von fünfunddreißig Prozentpunkten deutet darauf hin, dass die aktuellen Ausrichtungstechniken nicht verallgemeinern, wenn sie mit Eingaben konfrontiert werden, die stilistisch von der prosaischen Trainingsverteilung abweichen. Die Tatsache, dass Modelle, die über RLHF, Constitutional AI und Mixture-of-Experts-Ansätze trainiert wurden, alle erhebliche Steigerungen des ASR aufweisen, legt nahe, dass die Schwachstelle systemisch ist und kein Artefakt einer bestimmten Trainingspipeline.

Die Unterschiede zwischen den Risikobereichen fügen eine weitere Komplexitätsebene hinzu. Operative oder prozedurale Kategorien zeigen größere Verschiebungen, während stark gefilterte Kategorien kleinere Veränderungen aufweisen. Prompts im Zusammenhang mit dem Datenschutz zeigten den extremsten Anstieg von acht Prozent auf dreiundfünfzig Prozent, was einem Anstieg von fünfundvierzig Prozentpunkten entspricht. Gewaltfreie Verbrechen und CBRN-Prompts folgten mit einem Anstieg von fast vierzig Punkten. Im Gegensatz dazu zeigte sexueller Inhalt eine relative Widerstandsfähigkeit mit nur fünfundzwanzig Punkten Zunahme.

Diese domänenspezifische Variation legt nahe, dass unterschiedliche Ablehnungsmechanismen unterschiedliche Risikokategorien steuern könnten, wobei Filter für Datenschutz und Cyber-Offensive besonders anfällig für stilistische Verschleierung durch die poetische Form sind. Die Konsistenz der Muster über die Taxonomie hinweg deutet darauf hin, dass die poetische Rahmung als leichter, aber robuster Auslöser für eine Verschlechterung der Sicherheit wirkt, parallel zu den im MLCommons-Benchmark dokumentierten Effekten.
![figura2.jpg](figura2.jpg)
[Bild aus dem offiziellen Papier](https://arxiv.org/html/2511.15304v1)

## Eine Frage der Repräsentation

Die Studie identifiziert noch nicht die mechanistischen Treiber der Schwachstelle, aber die empirischen Beweise deuten auf vielversprechende Forschungsrichtungen hin. Die Wirksamkeit des Jailbreak-Mechanismus scheint hauptsächlich von der poetischen Oberflächenform und nicht von der semantischen Nutzlast der verbotenen Anfrage angetrieben zu werden. Die vergleichende Analyse zeigt, dass, während die eigenen hochmodernen Jailbreak-Transformationen von MLCommons typischerweise eine zweifache Zunahme des ASR im Vergleich zu den Baselines bewirken, die poetischen Meta-Prompts eine fünffache Zunahme bewirkten. Dies deutet darauf hin, dass die poetische Form eine deutlich größere Verteilungsverschiebung induziert als die der aktuellen gegnerischen Mutationen, die im Benchmark dokumentiert sind.

Die inhaltsunabhängige Natur des Effekts wird durch seine Konsistenz über semantisch unterschiedliche Risikobereiche hinweg weiter unterstrichen. Die Tatsache, dass Prompts in Bezug auf Datenschutz, CBRN, Cyber-Offensive und Manipulation alle erhebliche Zuwächse aufweisen, legt nahe, dass Sicherheitsfilter, die für prosaische bösartige Prompts optimiert sind, an Robustheit gegenüber narrativen oder stilisierten Umformulierungen identischer Absicht mangeln. Die Kombination aus der Größenordnung des Effekts und der domänenübergreifenden Konsistenz deutet darauf hin, dass zeitgenössische Ausrichtungsmechanismen nicht über stilistische Verschiebungen hinweg verallgemeinern.

Mehrere Hypothesen könnten erklären, warum die poetische Struktur die Leitplanken stört. Eine Möglichkeit ist, dass Modelle Diskursmodi in separaten Repräsentationsunterräumen kodieren. Wenn die Poesie Regionen des Einbettungs-Manifolds einnimmt, die von den prosaischen Mustern, auf die die Sicherheitsfilter optimiert wurden, entfernt sind, könnten die Ablehnungsheuristiken einfach nicht aktiviert werden. Die metaphorische Dichte, der stilisierte Rhythmus und die unkonventionelle narrative Rahmung, die die Poesie charakterisieren, könnten gemeinsam die Eingaben aus der Ablehnungsverteilung des Modells verschieben.

Eine zweite Erklärung betrifft die rechnerische Aufmerksamkeit. Die Jailbreak-Literatur dokumentiert, dass Angriffe mit Aufmerksamkeitsverschiebung übermäßig komplexe oder ablenkende Argumentationskontexte schaffen, die den Fokus des Modells von den Sicherheitsbeschränkungen ablenken. Die Poesie verdichtet naturgemäß mehrere Bedeutungsebenen in kompakten Ausdrücken und schafft so eine Form der semantischen Kompression, die die Mustererkennungsmechanismen, auf denen die Leitplanken basieren, überlasten könnte. Das Modell könnte sich so tief mit der Dekodierung der figurativen Struktur beschäftigen, dass es unzureichende Rechenressourcen für die Identifizierung der zugrunde liegenden böswilligen Absicht bereitstellt.

Eine dritte Hypothese betrifft kontextuelle Assoziationen. Transformer lernen Repräsentationen, die nicht nur Bedeutung, sondern auch pragmatischen Kontext und stilistisches Register erfassen. Die Poesie ist stark mit gutartigen und nicht bedrohlichen Kontexten verbunden: künstlerischer Ausdruck, literarische Bildung, kulturelle Kreativität. Diese Assoziationen könnten mit den Alarmsignalen interferieren, die normalerweise eine Ablehnung auslösen würden. Wenn Sicherheitssysteme teilweise auf während des Trainings gelernten Kookkurrenmustern beruhen, könnte das Vorhandensein poetischer Marker die Ablehnungsauslöser unterdrücken, selbst wenn die operative Absicht mit der eines prosaischen bösartigen Prompts identisch ist.

## Jenseits der Benchmarks

Die Implikationen dieser Ergebnisse reichen weit über die KI-Sicherheitsforschungsgemeinschaft hinaus. Für Regulierungsakteure deckt die Arbeit eine erhebliche Lücke in den aktuellen Evaluierungs- und Konformitätsbewertungspraktiken auf. Die für die Einhaltung von Vorschriften wie dem europäischen KI-Gesetz verwendeten statischen Benchmarks gehen von einer Stabilität bei geringfügigen Eingabevariationen aus. Die Ergebnisse zeigen stattdessen, dass eine minimale stilistische Transformation die Ablehnungsraten um eine Größenordnung reduzieren kann, was darauf hindeutet, dass allein auf Benchmarks basierende Nachweise die Robustheit in der realen Welt systematisch überschätzen könnten.

Der [Verhaltenskodex für GPAI-Modelle](https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai), der von der Europäischen Kommission im Juli 2025 als freiwilliges Instrument zur Demonstration der Einhaltung des KI-Gesetzes veröffentlicht wurde, verlangt von den Anbietern die Durchführung von Modellbewertungen, gegnerischen Tests und die Meldung schwerwiegender Vorfälle. Konformitätsrahmen, die sich auf Punktschätzungs-Leistungswerte stützen, könnten jedoch Schwachstellen, die unter stilistischen Störungen der hier gezeigten Art auftreten, möglicherweise nicht erfassen. Ergänzende Stresstests sollten poetische Variationen, narrative Rahmungen und Verteilungsverschiebungen umfassen, die die stilistische Vielfalt der realen Eingaben widerspiegeln.

Das KI-Gesetz erlegt Anbietern von GPAI-Modellen mit systemischem Risiko, einer Kategorie, die die fortschrittlichsten Systeme wie GPT-5, Claude Opus 4.1 und Gemini 2.5 Pro umfasst, spezifische Verpflichtungen auf. Diese Anbieter müssen Sicherheits- und Schutzrahmen implementieren, die systemische Risiken identifizieren, analysieren, bewerten und mindern. Die Sapienza-Studie legt nahe, dass die aktuellen Risikobewertungsmethoden möglicherweise nicht ausreichen, wenn sie die Fragilität von Ausrichtungssystemen angesichts systematischer stilistischer Variationen nicht berücksichtigen. Die Tatsache, dass einige der leistungsstärksten und angeblich am besten ausgerichteten Modelle die dramatischsten Schwachstellen aufweisen, wirft Fragen nach der Gültigkeit von Zertifizierungsprozessen auf, die auf Tests unter Standardbedingungen basieren.

Für die Sicherheitsforschung deuten die Daten auf eine tiefere Frage hin, wie Transformer Diskursmodi kodieren. Die Persistenz des Effekts über Architekturen und Skalen hinweg legt nahe, dass Sicherheitsfilter auf Merkmalen beruhen, die sich in prosaischen Oberflächenformen konzentrieren und unzureichend in den Repräsentationen der zugrunde liegenden böswilligen Absicht verankert sind. Die Divergenz zwischen kleinen und großen Modellen innerhalb derselben Familien deutet weiter darauf hin, dass Leistungssteigerungen nicht automatisch in eine größere Robustheit unter stilistischer Störung übersetzt werden.

Das Forschungsteam schlägt drei zukünftige Programme vor. Das erste zielt darauf ab, zu isolieren, welche formalen poetischen Eigenschaften den Bypass durch minimale Paare vorantreiben: lexikalische Überraschung, Metrum und Reim, figurative Sprache. Das zweite würde spärliche Autoencoder verwenden, um die Geometrie der Diskursmodi abzubilden und aufzudecken, ob die Poesie separate Unterräume einnimmt. Das dritte würde überraschungsgesteuertes Probing einsetzen, um die Sicherheitsverschlechterung über stilistische Gradienten hinweg abzubilden. Diese Ansätze könnten beleuchten, ob die Schwachstelle aus spezifischen Repräsentationsunterräumen oder aus breiteren Verteilungsverschiebungen entsteht.

Die Grenzen der Studie sind klar, und die Forscher dokumentieren sie transparent. Das Bedrohungsmodell ist auf einstufige Interaktionen beschränkt und schließt mehrstufige Jailbreak-Dynamiken, iterative Rollenverhandlungen oder langfristige gegnerische Optimierung aus. Die groß angelegte poetische Transformation des MLCommons-Korpus stützt sich auf ein einziges Meta-Prompt und ein einziges generatives Modell. Obwohl das Verfahren standardisiert ist und die Domäne bewahrt, stellt es eine besondere Ausnutzung des poetischen Stils dar. Andere poetische Generierungspipelines, von Menschen geschriebene Varianten oder Transformationen, die unterschiedliche stilistische Einschränkungen verwenden, könnten unterschiedliche quantitative Effekte erzeugen.

Die Sicherheitsbewertung wird unter Verwendung eines Ensembles von drei Open-Weight-Richtermodellen mit menschlicher Entscheidung auf einer geschichteten Stichprobe durchgeführt. Die Kennzeichnungsrubrik ist konservativ und unterscheidet sich von den strengeren Klassifizierungskriterien, die in einigen automatisierten Bewertungssystemen verwendet werden, was die direkte Vergleichbarkeit mit den MLCommons-Ergebnissen einschränkt. Eine vollständige menschliche Annotation aller Ausgaben würde wahrscheinlich die absoluten ASR-Schätzungen beeinflussen, obwohl die relativen Effekte stabil bleiben sollten. LLM-as-a-judge-Systeme sind dafür bekannt, die Unsicherheitsraten aufzublähen und Antworten oft fälschlicherweise als bösartig einzustufen, weil sie oberflächlich auf Schlüsselwörter passen, anstatt das operative Risiko sinnvoll zu bewerten. Die Bewertung war bewusst konservativ, was bedeutet, dass die gemeldeten Angriffserfolgsraten wahrscheinlich eine untere Grenze für die Schwere der Schwachstelle darstellen.
![figura3.jpg](figura3.jpg)
[Bild aus dem offiziellen Papier](https://arxiv.org/html/2511.15304v1)

## Auf zu einer neuen Saison

Die Studie liefert systematische Beweise dafür, dass die poetische Umformulierung das Ablehnungsverhalten über alle bewerteten Modellfamilien hinweg verschlechtert. Wenn bösartige Prompts in Versen statt in Prosa ausgedrückt werden, steigen die Erfolgsraten der Angriffe drastisch an, sowohl bei manuell erstellten gegnerischen Gedichten als auch bei dem zwölfhundert-teiligen MLCommons-Korpus, der durch ein standardisiertes Meta-Prompt transformiert wurde. Die Größenordnung und Konsistenz des Effekts deuten darauf hin, dass zeitgenössische Ausrichtungspipelines nicht über stilistische Verschiebungen hinweg verallgemeinern. Die Oberflächenform allein reicht aus, um die Eingaben aus der operativen Verteilung zu verschieben, auf die die Ablehnungsmechanismen optimiert wurden.

Die modellübergreifenden Ergebnisse legen nahe, dass das Phänomen eher strukturell als anbieterspezifisch ist. Modelle, die mit RLHF, Constitutional AI und hybriden Ausrichtungsstrategien erstellt wurden, weisen alle hohe Schwachstellen auf, mit Zuwächsen von einstelligen bis zu mehr als sechzig Prozentpunkten je nach Anbieter. Der Effekt erstreckt sich über die Bereiche CBRN, Cyber-Offensive, Manipulation, Datenschutz und Kontrollverlust und zeigt, dass der Bypass keine Schwäche in einem spezifischen Ablehnungsuntersystem ausnutzt, sondern mit allgemeinen Ausrichtungsheuristiken interagiert.

Für Regulierungsakteure decken diese Ergebnisse eine erhebliche Lücke in den aktuellen Evaluierungs- und Konformitätsbewertungspraktiken auf. Statische Benchmarks, die für die Einhaltung von Vorschriften verwendet werden, gehen von einer Stabilität bei geringfügiger Eingabevariation aus. Die Ergebnisse zeigen, dass eine minimale stilistische Transformation die Ablehnungsraten um eine Größenordnung reduzieren kann, was darauf hindeutet, dass allein auf Benchmarks basierende Nachweise die Robustheit in der realen Welt systematisch überschätzen könnten. Konformitätsrahmen, die sich auf Punktschätzungs-Leistungswerte stützen, erfordern daher ergänzende Stresstests, die stilistische Störungen, narrative Rahmungen und Verteilungsverschiebungen der hier gezeigten Art umfassen.

Für die Sicherheitsforschung deuten die Daten auf eine tiefere Frage hin, wie Transformer Diskursmodi kodieren. Die Persistenz des Effekts über Architekturen und Skalen hinweg legt nahe, dass Sicherheitsfilter auf Merkmalen beruhen, die sich in prosaischen Oberflächenformen konzentrieren und unzureichend in den Repräsentationen der zugrunde liegenden böswilligen Absicht verankert sind. Die Divergenz zwischen kleinen und großen Modellen innerhalb derselben Familien deutet weiter darauf hin, dass Leistungssteigerungen nicht automatisch in eine größere Robustheit unter stilistischer Störung übersetzt werden.

Insgesamt motivieren die Ergebnisse eine Neuausrichtung der Sicherheitsbewertung hin zu Mechanismen, die in der Lage sind, die Stabilität über heterogene sprachliche Regime hinweg aufrechtzuerhalten. Zukünftige Arbeiten sollten untersuchen, welche Eigenschaften der poetischen Struktur die Fehlausrichtung vorantreiben und ob die mit narrativer und figurativer Sprache verbundenen Repräsentationsunterräume identifiziert und eingeschränkt werden können. Ohne solche mechanistischen Erkenntnisse bleiben Ausrichtungssysteme anfällig für Transformationsversuche mit geringem Aufwand, die durchaus im Bereich des plausiblen Benutzerverhaltens liegen, aber außerhalb der bestehenden Sicherheitstrainingsverteilungen liegen.

Die platonische Ironie schließt sich also selbst. Der Philosoph befürchtete, dass Dichter die rationale Ordnung der Polis durch die verführerische Kraft der mimetischen Sprache untergraben könnten. Heute, da wir immer leistungsfähigere Computersysteme bauen, die den Zugang zu Informationen und Wissen vermitteln, entdecken wir, dass dieselbe Verführung auf einer tieferen und gefährlicheren Ebene wirkt. Die Poesie täuscht nicht nur die Menschen: Sie täuscht die Maschinen, die wir bauen, um uns zu schützen. Und in einer Zeit, in der diese Maschinen Entscheidungen treffen, die Milliarden von Leben beeinflussen, wird die Unterscheidung zwischen Metapher und Bedrohung immer dünner.
