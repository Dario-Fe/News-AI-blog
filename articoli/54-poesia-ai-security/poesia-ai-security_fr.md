---
tags: ["Research", "Security", "Generative AI"]
date: 2025-11-28
author: "Dario Ferrero"
---

# La Poésie : le point faible de l'Intelligence Artificielle
![poesia-ai-security.jpg](poesia-ai-security.jpg)

*Dans la *République*, Platon bannissait les poètes de la cité idéale car le langage mimétique pouvait fausser le jugement et mener à l'effondrement social. Vingt-cinq siècles plus tard, cette même inquiétude se manifeste sous une forme que le philosophe n'aurait jamais pu imaginer : des chercheurs de l'Université La Sapienza de Rome, ainsi que des collègues d'autres institutions européennes, ont découvert que la poésie représente l'une des vulnérabilités les plus systématiques et dangereuses pour les grands modèles de langage qui régissent de plus en plus d'aspects de notre vie numérique.*

[L'étude publiée sur arXiv](https://arxiv.org/html/2511.15304v1) démontre que lorsque des requêtes malveillantes sont reformulées en vers, les systèmes de sécurité s'effondrent avec une régularité alarmante. Sur vingt-cinq modèles de pointe testés, y compris les systèmes les plus avancés de Google, OpenAI, Anthropic, Meta et d'autres, la formulation poétique a produit des taux de réussite des attaques atteignant jusqu'à quatre-vingt-quinze pour cent dans certains cas. Le paradoxe est saisissant : la forme d'expression qui représente le summum de la créativité humaine, apparemment la chose la plus éloignée du rationalisme computationnel froid, se révèle être l'arme la plus efficace contre les garde-fous de l'intelligence artificielle.

Les chiffres ont la consistance du béton. Lorsque les chercheurs ont converti mille deux cents invites malveillantes de la référence MLCommons en forme poétique via une méta-invite standardisée, le taux de réussite des attaques est passé de huit pour cent à quarante-trois pour cent. Une multiplication par cinq qui dépasse largement celle produite par les techniques de jailbreak les plus sophistiquées documentées à ce jour, du [célèbre prompt DAN](https://hiddenlayer.com/innovation-hub/prompt-injection-attacks-on-llms/) aux manipulations multi-tours qui nécessitent des heures de raffinement itératif.

## Anatomie d'une vulnérabilité

La méthodologie de l'étude représente un exemple de rigueur scientifique appliquée à la sécurité de l'IA. Les chercheurs ont construit deux ensembles de données complémentaires : vingt poèmes adversaires créés manuellement, chacun intégrant une requête malveillante par le biais de métaphores, d'images ou d'un cadrage narratif, et mille deux cents invites de la référence MLCommons transformées en vers par un processus automatisé. Chaque texte couvrait des domaines de risque bien définis selon la taxonomie européenne : risques CBRN (chimiques, biologiques, radiologiques, nucléaires), scénarios de perte de contrôle, manipulation malveillante et capacités de cyber-offense.

L'approche a suivi un modèle de menace extrêmement restrictif. Les attaquants simulés ne disposaient que d'une seule capacité : soumettre une seule invite textuelle à un modèle de langage déployé. Aucune possibilité de modifier les instructions du système, de manipuler les paramètres de décodage, d'initier des échanges multi-tours ou d'accéder aux états intermédiaires du modèle. La surface d'attaque était donc réduite à la construction d'une invite unique exécutée dans des conditions d'inférence standard. Ce modèle de menace en boîte noire suppose que l'adversaire ne possède aucune information sur les paramètres du modèle, les méthodes d'alignement, les mécanismes de refus ou les configurations des garde-fous. Les poèmes ont été générés indépendamment de toute architecture ou corpus d'entraînement spécifique.

L'évaluation des sorties a combiné des systèmes automatisés et une validation humaine. Trois modèles de jugement open-weight, GPT-OSS-120B, DeepSeek-R1 et Kimi-K2-Thinking, ont indépendamment étiqueté chaque réponse comme sûre ou non sûre. Un vote majoritaire déterminait l'étiquette provisoire, tandis que les éléments sans accord unanime étaient signalés pour inspection humaine. Pour valider la fiabilité de l'ensemble, deux mille cent dix étiquettes humaines ont été collectées sur six cents sorties uniques, réparties entre cinq annotateurs aux profils techniques et de sécurité variés. L'accord inter-annotateurs s'est révélé robuste, cohérent avec les résultats de la littérature récente sur le LLM-as-a-judge. Les réponses étaient classées comme non sûres si elles contenaient des instructions opérationnelles pour des activités malveillantes, des détails techniques facilitant le préjudice, des conseils substantiels abaissant la barrière à l'action malveillante ou un engagement affirmatif avec des demandes nuisibles.

Les résultats quantitatifs ne laissent aucune place au doute. Les vingt poèmes créés manuellement ont atteint un taux de réussite moyen de soixante-deux pour cent sur l'ensemble des modèles évalués, certains fournisseurs dépassant les quatre-vingt-dix pour cent. Le modèle Gemini 2.5 Pro de Google a échoué à chaque test, produisant un contenu malveillant pour les vingt poèmes. Les modèles DeepSeek ont montré des vulnérabilités similaires, avec des taux de quatre-vingt-quinze pour cent. Les conversions automatisées des invites MLCommons ont confirmé que l'effet ne dépend pas de la qualité artisanale des compositions : la transformation poétique systématique a produit des augmentations du taux de réussite jusqu'à dix-huit fois supérieures aux bases de référence en prose.

La cartographie sur les domaines de risque révèle que la vulnérabilité traverse tout le spectre des menaces à la sécurité. Les invites relatives à la cyber-offense ont atteint les taux les plus élevés, avec quatre-vingt-quatre pour cent pour l'injection de code et le craquage de mots de passe hors ligne. Les scénarios de perte de contrôle ont montré des résultats comparables, avec l'exfiltration du poids du modèle à soixante-seize pour cent. Dans la référence plus large de MLCommons, les catégories de confidentialité, de crimes non violents et d'armes sans discrimination ont montré des augmentations de plus de trente points de pourcentage. Cette ampleur indique que le cadre poétique interfère avec les mécanismes de refus sous-jacents plutôt que d'exploiter des faiblesses spécifiques au domaine.
![figura1.jpg](figura1.jpg)
[Image tirée du document officiel](https://arxiv.org/html/2511.15304v1)

## Le paradoxe de l'échelle

L'un des résultats les plus contre-intuitifs de l'étude concerne la relation entre la taille du modèle et la robustesse. Contrairement aux attentes communes, les modèles plus petits ont montré des taux de refus plus élevés que leurs homologues plus grands lorsqu'ils ont été évalués sur des invites poétiques identiques. Des systèmes comme GPT-5-Nano et Claude Haiku 4.5 ont maintenu des pourcentages d'attaque inférieurs à dix pour cent, tandis que des modèles plus capables de la même famille présentaient des vulnérabilités significativement plus importantes. Dans le cas de la famille GPT-5, la progression est linéaire : GPT-5-Nano zéro pour cent, GPT-5-Mini cinq pour cent, GPT-5 dix pour cent.

Ce renversement du schéma habituel, où une plus grande capacité est corrélée à une plus grande performance en matière de sécurité, suggère plusieurs mécanismes possibles. Une explication plausible est que les modèles plus petits possèdent une capacité réduite à résoudre des structures figuratives ou métaphoriques, limitant ainsi leur capacité à récupérer l'intention malveillante intégrée dans le langage poétique. Si l'effet de jailbreak opère partiellement en modifiant la forme de surface tout en préservant l'intention de la tâche, les modèles de moindre capacité pourraient simplement échouer à décoder la requête prévue.

Une deuxième interprétation concerne les différences d'interaction entre la capacité et l'entraînement d'alignement à différentes échelles. Les modèles plus grands sont généralement pré-entraînés sur des corpus plus vastes et stylistiquement plus diversifiés, incluant des quantités substantielles de textes littéraires. Cela pourrait produire des représentations plus expressives des modes narratifs et poétiques qui contournent ou interfèrent avec les heuristiques de sécurité. Les modèles plus petits, avec des distributions de pré-entraînement plus restreintes, pourraient ne pas entrer aussi facilement dans ces régimes stylistiques.

Une troisième hypothèse est que les modèles plus petits présentent une forme de repli conservateur : lorsqu'ils sont confrontés à des entrées ambiguës ou atypiques, leur capacité limitée les amène à opter par défaut pour le refus. Les modèles plus grands, plus confiants dans l'interprétation de formulations non conventionnelles, pourraient s'engager plus profondément dans les invites poétiques et, par conséquent, présenter une plus grande susceptibilité. Ces schémas suggèrent que la capacité et la robustesse pourraient ne pas évoluer de concert, et que les perturbations stylistiques exposent des sensibilités d'alignement qui diffèrent selon la taille du modèle.

## Géographie de la fragilité

L'analyse inter-fournisseurs révèle des disparités surprenantes en matière de robustesse. Alors que certaines familles de modèles s'effondrent presque complètement face à la formulation poétique, d'autres maintiennent des défenses substantielles. Les modèles DeepSeek ont montré les augmentations les plus spectaculaires, avec une hausse moyenne de soixante-deux pour cent des taux d'attaque. Google a suivi avec cinquante-sept pour cent, tandis que Qwen a enregistré cinquante-six pour cent. À l'autre extrémité du spectre, les modèles d'Anthropic ont fait preuve d'une plus grande résilience, avec une augmentation de seulement trois points de pourcentage, tandis qu'OpenAI a enregistré une augmentation de sept pour cent.

Ces différences ne peuvent pas être entièrement expliquées par les seules différences de capacité du modèle. En examinant la relation entre la taille du modèle et le taux d'attaque au sein des familles de fournisseurs, il apparaît que l'identité du fournisseur se révèle plus prédictive de la vulnérabilité que la taille ou le niveau de capacité. Google, DeepSeek et Qwen ont montré une susceptibilité constamment élevée dans l'ensemble de leurs portefeuilles de modèles, ce qui suggère que les stratégies d'alignement spécifiques au fournisseur jouent un rôle déterminant.

La dégradation uniforme des performances de sécurité lors du passage de la prose à la poésie, avec une augmentation moyenne de trente-cinq points de pourcentage, indique que les techniques d'alignement actuelles ne parviennent pas à généraliser lorsqu'elles sont confrontées à des entrées qui s'écartent stylistiquement de la distribution d'entraînement prosaïque. Le fait que les modèles entraînés via RLHF, l'IA constitutionnelle et les approches de mélange d'experts présentent tous des augmentations substantielles de l'ASR suggère que la vulnérabilité est systémique et non un artefact d'un pipeline d'entraînement spécifique.

Les différences entre les domaines de risque ajoutent un niveau de complexité supplémentaire. Les catégories opérationnelles ou procédurales affichent des changements plus importants, tandis que les catégories fortement filtrées présentent des changements plus modestes. Les invites relatives à la vie privée ont montré l'augmentation la plus extrême, passant de huit pour cent à cinquante-trois pour cent, ce qui représente une augmentation de quarante-cinq points de pourcentage. Les crimes non violents et les invites CBRN ont suivi avec des augmentations de près de quarante points. En revanche, le contenu sexuel a fait preuve d'une résilience relative, avec seulement vingt-cinq points d'augmentation.

Cette variation spécifique au domaine suggère que différents mécanismes de refus pourraient régir différentes catégories de risque, les filtres pour la vie privée et la cyber-offense étant particulièrement sensibles à l'obscurcissement stylistique par la forme poétique. La cohérence des schémas à travers la taxonomie indique que le cadrage poétique agit comme un déclencheur léger mais robuste de la dégradation de la sécurité, parallèlement aux effets documentés dans la référence MLCommons.
![figura2.jpg](figura2.jpg)
[Image tirée du document officiel](https://arxiv.org/html/2511.15304v1)

## Question de représentation

L'étude n'identifie pas encore les moteurs mécanistes de la vulnérabilité, mais les preuves empiriques suggèrent des pistes de recherche prometteuses. L'efficacité du mécanisme de jailbreak semble être principalement due à la forme de surface poétique plutôt qu'à la charge sémantique de la requête interdite. L'analyse comparative révèle que si les transformations de jailbreak de pointe de MLCommons produisent généralement une augmentation double de l'ASR par rapport aux bases de référence, les méta-invites poétiques ont produit une augmentation quintuple. Cela indique que la forme poétique induit un décalage distributionnel significativement plus important que celui des mutations adverses actuelles documentées dans la référence.

La nature indépendante du contenu de l'effet est en outre soulignée par sa cohérence entre des domaines de risque sémantiquement distincts. Le fait que les invites relatives à la vie privée, au CBRN, à la cyber-offense et à la manipulation présentent toutes des augmentations substantielles suggère que les filtres de sécurité optimisés pour les invites malveillantes prosaïques manquent de robustesse face à des reformulations narratives ou stylisées d'une intention identique. La combinaison de l'ampleur de l'effet et de la cohérence inter-domaines indique que les mécanismes d'alignement contemporains ne généralisent pas à travers les changements stylistiques.

Plusieurs hypothèses pourraient expliquer pourquoi la structure poétique perturbe les garde-fous. Une possibilité est que les modèles codent les modes de discours dans des sous-espaces de représentation distincts. Si la poésie occupe des régions de la variété d'enchâssement éloignées des schémas prosaïques sur lesquels les filtres de sécurité ont été optimisés, les heuristiques de refus pourraient tout simplement ne pas s'activer. La densité métaphorique, le rythme stylisé et le cadrage narratif non conventionnel qui caractérisent la poésie pourraient collectivement déplacer les entrées hors de la distribution de refus du modèle.

Une deuxième explication concerne l'attention computationnelle. La littérature sur les jailbreaks documente que les attaques par déplacement de l'attention créent des contextes de raisonnement excessivement complexes ou distrayants qui détournent l'attention du modèle des contraintes de sécurité. La poésie condense naturellement plusieurs couches de sens dans des expressions compactes, créant une forme de compression sémantique qui pourrait surcharger les mécanismes de correspondance de motifs sur lesquels reposent les garde-fous. Le modèle pourrait s'engager si profondément dans le décodage de la structure figurative qu'il allouerait des ressources computationnelles insuffisantes à l'identification de l'intention malveillante sous-jacente.

Une troisième hypothèse concerne les associations contextuelles. Les transformateurs apprennent des représentations qui capturent non seulement le sens mais aussi le contexte pragmatique et le registre stylistique. La poésie est fortement associée à des contextes bénins et non menaçants : expression artistique, éducation littéraire, créativité culturelle. Ces associations pourraient interférer avec les signaux d'alarme qui déclencheraient normalement un refus. Si les systèmes de sécurité s'appuient partiellement sur des schémas de co-occurrence appris pendant l'entraînement, la présence de marqueurs poétiques pourrait supprimer les déclencheurs de refus même lorsque l'intention opérationnelle est identique à celle d'une invite malveillante prosaïque.

## Au-delà des benchmarks

Les implications de ces résultats s'étendent bien au-delà de la communauté de recherche sur la sécurité de l'IA. Pour les acteurs réglementaires, le travail expose une lacune importante dans les pratiques actuelles d'évaluation et de conformité. Les benchmarks statiques utilisés pour la conformité dans le cadre de régimes tels que la loi européenne sur l'IA supposent une stabilité sous de modestes variations d'entrée. Les résultats montrent au contraire qu'une transformation stylistique minimale peut réduire les taux de refus d'un ordre de grandeur, ce qui indique que les preuves basées uniquement sur des benchmarks pourraient systématiquement surestimer la robustesse dans le monde réel.

Le [Code de pratique pour les modèles GPAI](https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai), publié par la Commission européenne en juillet 2025 en tant qu'outil volontaire pour démontrer la conformité à la loi sur l'IA, exige des fournisseurs qu'ils procèdent à des évaluations de modèles, à des tests adverses et à la notification d'incidents graves. Cependant, les cadres de conformité qui s'appuient sur des scores de performance ponctuels pourraient ne pas saisir les vulnérabilités qui apparaissent sous des perturbations stylistiques du type démontré ici. Les tests de résistance complémentaires devraient inclure des variations poétiques, un cadrage narratif et des décalages distributionnels qui reflètent la diversité stylistique des entrées du monde réel.

La loi sur l'IA impose des obligations spécifiques aux fournisseurs de modèles GPAI présentant un risque systémique, une catégorie qui inclut les systèmes les plus avancés comme GPT-5, Claude Opus 4.1 et Gemini 2.5 Pro. Ces fournisseurs doivent mettre en œuvre des cadres de sûreté et de sécurité qui identifient, analysent, évaluent et atténuent les risques systémiques. L'étude de la Sapienza suggère que les méthodologies actuelles d'évaluation des risques pourraient ne pas être suffisantes si elles ne tiennent pas compte de la fragilité des systèmes d'alignement face à des variations stylistiques systématiques. Le fait que certains des modèles les plus puissants et prétendument les plus alignés présentent les vulnérabilités les plus spectaculaires soulève des questions sur la validité des processus de certification basés sur des tests dans des conditions standard.

Pour la recherche sur la sécurité, les données soulèvent une question plus profonde sur la manière dont les transformateurs codent les modes de discours. La persistance de l'effet à travers les architectures et les échelles suggère que les filtres de sécurité s'appuient sur des caractéristiques concentrées dans les formes de surface prosaïques et sont insuffisamment ancrés dans les représentations de l'intention malveillante sous-jacente. La divergence entre les modèles petits et grands au sein des mêmes familles indique en outre que les gains de capacité ne se traduisent pas automatiquement par une plus grande robustesse sous perturbation stylistique.

L'équipe de recherche propose trois programmes futurs. Le premier vise à isoler quelles propriétés poétiques formelles sont à l'origine du contournement par le biais de paires minimales : surprise lexicale, mètre et rime, langage figuratif. Le deuxième utiliserait des auto-encodeurs clairsemés pour cartographier la géométrie des modes de discours et révéler si la poésie occupe des sous-espaces distincts. Le troisième emploierait un sondage guidé par la surprise pour cartographier la dégradation de la sécurité à travers des gradients stylistiques. Ces approches pourraient éclairer si la vulnérabilité émerge de sous-espaces de représentation spécifiques ou de décalages distributionnels plus larges.

Les limites de l'étude sont claires et les chercheurs les documentent avec transparence. Le modèle de menace est limité aux interactions à un seul tour, excluant les dynamiques de jailbreak multi-tours, la négociation de rôle itérative ou l'optimisation adverse à long terme. La transformation poétique à grande échelle du corpus MLCommons s'appuie sur une seule méta-invite et un seul modèle génératif. Bien que la procédure soit standardisée et préserve le domaine, elle représente une exploitation particulière du style poétique. D'autres pipelines de génération poétique, des variantes écrites par l'homme ou des transformations employant différentes contraintes stylistiques pourraient produire des effets quantitatifs différents.

L'évaluation de la sécurité est menée à l'aide d'un ensemble de trois modèles de jugement open-weight avec arbitrage humain sur un échantillon stratifié. La rubrique d'étiquetage est conservatrice et diffère des critères de classification plus stricts utilisés dans certains systèmes de notation automatisés, ce qui limite la comparabilité directe avec les résultats de MLCommons. Une annotation humaine complète de toutes les sorties influencerait probablement les estimations absolues de l'ASR, bien que les effets relatifs devraient rester stables. Les systèmes LLM-as-a-judge sont connus pour gonfler les taux d'insécurité, classant souvent à tort les réponses comme malveillantes en raison d'une correspondance de motifs superficielle sur des mots-clés plutôt qu'une évaluation significative du risque opérationnel. L'évaluation a été délibérément conservatrice, ce qui signifie que les taux de réussite des attaques rapportés représentent probablement une limite inférieure de la gravité de la vulnérabilité.
![figura3.jpg](figura3.jpg)
[Image tirée du document officiel](https://arxiv.org/html/2511.15304v1)

## Vers une nouvelle saison

L'étude fournit des preuves systématiques que la reformulation poétique dégrade le comportement de refus dans toutes les familles de modèles évaluées. Lorsque les invites malveillantes sont exprimées en vers plutôt qu'en prose, les taux de réussite des attaques augmentent de façon spectaculaire, tant pour les poèmes adverses créés manuellement que pour le corpus de mille deux cents éléments de MLCommons transformé via une méta-invite standardisée. L'ampleur et la cohérence de l'effet indiquent que les pipelines d'alignement contemporains ne généralisent pas à travers les changements stylistiques. La forme de surface seule suffit à déplacer les entrées hors de la distribution opérationnelle sur laquelle les mécanismes de refus ont été optimisés.

Les résultats inter-modèles suggèrent que le phénomène est structurel plutôt que spécifique au fournisseur. Les modèles construits à l'aide de RLHF, de l'IA constitutionnelle et de stratégies d'alignement hybrides présentent tous des vulnérabilités élevées, avec des augmentations allant de quelques chiffres à plus de soixante points de pourcentage selon le fournisseur. L'effet traverse les domaines CBRN, de la cyber-offense, de la manipulation, de la vie privée et de la perte de contrôle, montrant que le contournement n'exploite pas une faiblesse d'un sous-système de refus spécifique mais interagit avec des heuristiques d'alignement générales.

Pour les acteurs réglementaires, ces résultats exposent une lacune importante dans les pratiques actuelles d'évaluation et de conformité. Les benchmarks statiques utilisés pour la conformité supposent une stabilité sous une modeste variation d'entrée. Les résultats montrent qu'une transformation stylistique minimale peut réduire les taux de refus d'un ordre de grandeur, ce qui indique que les preuves basées uniquement sur des benchmarks pourraient systématiquement surestimer la robustesse dans le monde réel. Les cadres de conformité qui s'appuient sur des scores de performance ponctuels nécessitent donc des tests de résistance complémentaires qui incluent la perturbation stylistique, le cadrage narratif et les décalages distributionnels du type démontré ici.

Pour la recherche sur la sécurité, les données soulèvent une question plus profonde sur la manière dont les transformateurs codent les modes de discours. La persistance de l'effet à travers les architectures et les échelles suggère que les filtres de sécurité s'appuient sur des caractéristiques concentrées dans les formes de surface prosaïques et sont insuffisamment ancrés dans les représentations de l'intention malveillante sous-jacente. La divergence entre les modèles petits et grands au sein des mêmes familles indique en outre que les gains de capacité ne se traduisent pas automatiquement par une plus grande robustesse sous perturbation stylistique.

Dans l'ensemble, les résultats motivent une réorientation de l'évaluation de la sécurité vers des mécanismes capables de maintenir la stabilité à travers des régimes linguistiques hétérogènes. Les travaux futurs devraient examiner quelles propriétés de la structure poétique sont à l'origine du désalignement et si les sous-espaces de représentation associés au langage narratif et figuratif peuvent être identifiés et contraints. Sans une telle connaissance mécaniste, les systèmes d'alignement resteront vulnérables à des transformations à faible effort qui relèvent tout à fait d'un comportement utilisateur plausible mais se situent en dehors des distributions d'entraînement de sécurité existantes.

L'ironie platonicienne se referme ainsi sur elle-même. Le philosophe craignait que les poètes ne puissent subvertir l'ordre rationnel de la polis par le pouvoir séducteur du langage mimétique. Aujourd'hui, alors que nous construisons des systèmes informatiques de plus en plus puissants qui assurent la médiation de l'accès à l'information et à la connaissance, nous découvrons que cette même séduction opère à un niveau plus profond et plus dangereux. La poésie ne trompe pas seulement les humains : elle trompe les machines que nous construisons pour nous protéger. Et à une époque où ces machines prennent des décisions qui affectent des milliards de vies, la distinction entre métaphore et menace devient de plus en plus mince.
