---
tags: ["Generative AI", "Research", "Applications"]
date: 2025-12-01
author: "Dario Ferrero"
---

# LongCat-Video, el Gigante silencioso de la Generación de vídeo OpenSource
![longcat-video.jpg](longcat-video.jpg)

*Meituan lanza un modelo de 13.600 millones de parámetros que genera vídeos de hasta cinco minutos. En el caos de la IA de vídeo, llega una solución técnica elegante que desafía a los colosos propietarios con eficiencia y transparencia. Mientras el mundo tecnológico estaba ocupado digiriendo [las controversias sobre los derechos de autor de Sora 2](https://openai.com/it-IT/index/sora-2/) y lidiando con las limitaciones de acceso de [Veo 3](https://aistudio.google.com/models/veo-3), a finales de octubre de 2025 el equipo LongCat de Meituan lanzó [LongCat-Video](https://huggingface.co/meituan-longcat/LongCat-Video) con la discreción de quien sabe que tiene algo sólido entre manos pero no siente la necesidad de gritarlo.*

Nada de keynotes espectaculares, nada de demos preparadas en laboratorio con un sospechoso cherry-picking, solo un repositorio de GitHub, los pesos del modelo en Hugging Face y un detallado artículo técnico en arXiv. Como diciendo: aquí está el código, aquí está el modelo, haced vosotros.

Y el modelo, si se mira bien, es cualquier cosa menos modesto. Con 13.600 millones de parámetros y la capacidad de generar vídeos de hasta cinco minutos manteniendo la coherencia temporal y la calidad visual, LongCat-Video aborda uno de los problemas más obstinados de la IA generativa de vídeo: cómo evitar que un vídeo largo se convierta en una deriva psicodélica de artefactos, deriva de color e inconsistencias narrativas. Es el tipo de problema que parece trivial hasta que te topas con él, como darse cuenta de que escribir un cuento largo es más difícil que concatenar párrafos cortos.

## El problema que nadie había resuelto de verdad

El panorama de la generación de vídeo con IA en 2025 se asemeja a una carrera armamentística en la que todos apuestan por la misma métrica: ¿cuán fotorrealista puedes hacer un clip de cinco segundos? Sora, Veo, Movie Gen han elevado el listón de la calidad visual a niveles impresionantes, pero siguen limitados a duraciones cortas o, en cualquier caso, limitadas. Cuando OpenAI anunció Sora 2 con sus veinte segundos de generación máxima, parecía un hito. Pero veinte segundos, en términos de narración visual, son poco más que un plano de apertura. No cuentas mucho en veinte segundos, como mucho evocas.

El mundo del código abierto, por su parte, se debate entre proyectos ambiciosos pero con recursos limitados. [HunyuanVideo de Tencent](https://github.com/Tencent/HunyuanVideo) llega a los diez segundos con resultados sólidos, [Mochi](https://mochi1ai.com/it) se centra en la animación de imágenes estáticas, Open-Sora lucha por replicar el original propietario. Todos comparten el mismo límite: más allá de una cierta duración, la coherencia temporal se colapsa como un soufflé mal horneado. Los fotogramas empiezan a desviarse en los colores, los objetos se multiplican o desaparecen, los movimientos pierden continuidad física. Es el fenómeno que los investigadores llaman "deriva temporal" y que a los usuarios les parece simplemente vídeos que parecen generados por una inteligencia artificial bajo el efecto de sustancias psicoactivas.

LongCat-Video ataca este problema desde una perspectiva diferente. En lugar de apostarlo todo a la perfección fotorrealista de clips muy cortos, el equipo de Meituan ha diseñado una arquitectura que prioriza la coherencia temporal extendida. La idea es tan simple como potente: si entrenas explícitamente un modelo en la capacidad de continuar vídeos existentes, es decir, de generar fotogramas posteriores que mantengan la coherencia con los anteriores, ese modelo aprende implícitamente a gestionar dependencias temporales largas. Es un poco como la diferencia entre un velocista y un maratoniano: el primero explota en los cien metros, el segundo distribuye el esfuerzo en distancias que harían colapsar al velocista.

Meituan no es un nombre que suene familiar a quien no sigue el mercado tecnológico chino, pero hablamos de una de las mayores plataformas de servicios locales de China, con profundas competencias en aprendizaje automático aplicado a problemas de escala industrial. El equipo de LongCat ha trabajado en este proyecto centrándose en un objetivo ingenierilmente elegante: crear un modelo unificado capaz de gestionar tres tareas diferentes, de texto a vídeo, de imagen a vídeo y de continuación de vídeo, sin arquitecturas separadas ni pipelines complicadas. Un poco como diseñar un motor que funcione bien tanto con gasolina como con diésel o en eléctrico, en lugar de tener tres motores diferentes.

## Una arquitectura pensada para la duración

Bajo el capó, LongCat-Video se basa en un Diffusion Transformer, o DiT, la misma familia arquitectónica que ha convertido a Sora en un punto de referencia en el sector. Los DiT son una variante de los transformadores originales adaptados a los procesos de difusión, esa técnica de generación que funciona añadiendo progresivamente ruido a una imagen y luego aprendiendo a eliminarlo de forma controlada. Piensa en la restauración de un fresco antiguo: primero cubres todo con una capa de suciedad virtual, luego aprendes a eliminarla revelando la imagen subyacente. Los modelos de difusión hacen exactamente esto, pero a la inversa: parten del ruido puro y lo "limpian" hasta obtener la imagen deseada.

El problema con los vídeos largos es que cada fotograma añadido multiplica exponencialmente la complejidad computacional. Un vídeo de cinco minutos a 30 fps son 9.000 fotogramas, y cada fotograma debe ser coherente con todos los demás. La memoria requerida se dispara, los tiempos de generación se vuelven prohibitivos y el riesgo de errores acumulados aumenta. Es como jugar al teléfono estropeado con nueve mil personas: incluso un error microscópico en el fotograma 100 se amplifica dramáticamente en el fotograma 9.000.

LongCat-Video resuelve esto con la Atención Dispersa por Bloques (Block Sparse Attention), un mecanismo que, en lugar de hacer que cada fotograma preste atención a todos los demás fotogramas simultáneamente (una operación computacionalmente devastadora), crea bloques de atención local. Cada fotograma presta atención principalmente a los fotogramas cercanos y a una selección estratégica de fotogramas distantes. Es como mirar un paisaje: tienes una visión nítida de lo que está cerca, una visión periférica de lo que está a los lados y puntos de anclaje visual en la línea del horizonte. No ves todo con la misma nitidez, pero el cerebro reconstruye una percepción coherente del espacio. La Atención Dispersa por Bloques hace lo mismo con los vídeos.

La otra innovación clave es la estrategia de grueso a fino (coarse-to-fine), que opera tanto en el eje temporal como en el espacial. En lugar de generar inmediatamente vídeos de alta resolución con todos los fotogramas, el modelo primero genera una versión de baja resolución con menos fotogramas, capturando la estructura narrativa y los movimientos principales. Luego, en una segunda fase, refina este borrador aumentando progresivamente la resolución espacial y la velocidad de fotogramas temporal. Es como trabajar en una animación tradicional: primero dibujas los fotogramas clave principales que definen las poses y los movimientos clave, luego añades los intermedios que hacen que todo sea fluido y, finalmente, coloreas y refinas los detalles.

Esta estrategia no solo es computacionalmente eficiente, sino que también está más alineada con la forma en que funciona la narración visual humana. Cuando imaginas una escena, no la visualizas inmediatamente fotograma a fotograma en 4K: primero tienes una idea aproximada de la composición, de los movimientos principales, de la atmósfera general. Luego añades detalles, perfeccionas los movimientos, defines las texturas. El enfoque de grueso a fino replica este proceso cognitivo.

El preentrenamiento nativo en tareas de continuación de vídeo es el ingrediente secreto que lo une todo. Durante el entrenamiento, el modelo vio cientos de miles de horas de vídeo en las que la tarea no era generar desde cero, sino continuar secuencias existentes manteniendo la coherencia estilística, narrativa y física. Aprendió lo que significa la continuidad temporal de la manera más directa posible: haciéndolo, iterativamente, durante miles de horas. Es como aprender a escribir secuelas leyendo novelas y luego escribiendo los capítulos siguientes: aprendes implícitamente lo que significa mantener la coherencia de tono, personajes y ambientación.
![immagine1.jpg](immagine1.jpg)
[Fotograma de un vídeo de demostración extraído del sitio web oficial de LongCat-Video](https://meituan-longcat.github.io/LongCat-Video/)

## Tres tareas, un solo modelo

Una de las elecciones arquitectónicas más elegantes de LongCat-Video es el enfoque unificado para las tres tareas principales: generación de texto a vídeo desde cero, animación de imagen a vídeo de imágenes estáticas y continuación de vídeo para extender clips existentes. La mayoría de los sistemas abordan estas tareas con modelos separados o pipelines elaborados que pegan diferentes componentes. LongCat-Video las gestiona todas distinguiéndolas simplemente por el número de fotogramas condicionales de entrada.

En la generación pura de texto a vídeo, el modelo parte de cero fotogramas condicionales y lo genera todo basándose únicamente en el prompt de texto. Es el caso de uso más difícil porque no tienes anclajes visuales, tienes que crear la composición, los movimientos, el estilo y la coherencia exclusivamente a partir de la interpretación del texto. Es como decirle a alguien "píntame una tormenta en el mar" sin mostrarle referencias: tiene que imaginarlo todo.

En la generación de imagen a vídeo, proporcionas un fotograma inicial como condición y el modelo genera los fotogramas posteriores que animan esa imagen. Aquí la tarea está más limitada: ya tienes la composición, el estilo y la iluminación definidos por el fotograma inicial, solo tienes que añadir un movimiento coherente. Es como tener el primer plano de una película ya rodado y tener que decidir cómo continúa la escena.

En la continuación de vídeo, proporcionas un segmento de vídeo completo y le pides al modelo que lo extienda. Este es técnicamente el caso más limitado, pero requiere la máxima coherencia temporal: debes respetar no solo el estilo y la composición, sino también la física de los movimientos ya presentes, la trayectoria de los objetos y la evolución de la escena. Es como escribir el tercer acto de una historia en la que los dos primeros ya están escritos: tienes márgenes creativos, pero debes respetar los personajes, la trama y el tono ya establecidos.

La arquitectura unificada significa que el modelo no necesita módulos separados para gestionar estos casos. Internamente, la generación de texto a vídeo es simplemente una continuación de vídeo con cero fotogramas condicionales, la de imagen a vídeo es una continuación de vídeo con un fotograma condicional, y así sucesivamente. Esto no solo reduce la complejidad de la ingeniería, porque mantienes una única base de código y una única pipeline de entrenamiento, sino que también mejora la calidad porque el modelo comparte representaciones y capacidades entre las diferentes tareas. Las competencias aprendidas para extender vídeos largos ayudan a la generación desde cero, y viceversa.

Para los desarrolladores e investigadores, este enfoque es una bendición. En lugar de tener que configurar pipelines complejas que llaman a diferentes modelos para diferentes tareas, instalas un solo modelo y simplemente cambias el número de fotogramas que proporcionas como entrada. ¿Quieres generar a partir de texto? Cero fotogramas. ¿Quieres animar una imagen? Un fotograma. ¿Quieres extender un vídeo? Todo el vídeo. La simplicidad se subestima en la ingeniería de software, pero cualquiera que haya depurado pipelines complejas sabe cuánto vale la pena tener menos componentes en movimiento.
![immagine3.jpg](immagine3.jpg)
[Esquema extraído del sitio web oficial de LongCat-Video](https://meituan-longcat.github.io/LongCat-Video/)

## Los números hablan, pero no gritan

Cuando se trata de modelos generativos, los benchmarks son un campo minado. Cada empresa tiene sus propias pruebas internas, optimizadas para hacer brillar a sus modelos, y las pruebas públicas como [VBench](https://vchitect.github.io/VBench-project/) intentan ofrecer evaluaciones objetivas, pero solo capturan algunas dimensiones de la calidad percibida. LongCat-Video navega por estas cifras con resultados sólidos pero no espectaculares, que es exactamente lo que se espera de un modelo de código abierto desarrollado con una fracción de los recursos de los gigantes tecnológicos.

En los benchmarks internos de Meituan basados en la Puntuación de Opinión Media (Mean Opinion Score), donde evaluadores humanos juzgan los vídeos en una escala del 1 al 5, LongCat-Video obtiene 3,25 en calidad visual en la generación de texto a vídeo y 3,27 en la de imagen a vídeo. Para contextualizar: Veo 3 obtiene 3,51, Sora 2 está en 3,49. No es una diferencia enorme, pero existe. En las métricas de alineación de texto, donde se mide cuánto se corresponde el vídeo generado con el prompt de texto, LongCat-Video obtiene 3,76, mientras que los principales actores propietarios rozan el 4,0. En resumen, el modelo es competitivo pero no domina.

En VBench, el benchmark público más respetado del sector que evalúa 16 dimensiones diferentes de la calidad del vídeo, LongCat-Video se comporta particularmente bien en la coherencia temporal y en la comprensión del sentido común, donde obtiene el primer lugar entre los modelos de código abierto con una puntuación del 70,94%. Esto es significativo porque el sentido común, la capacidad de generar vídeos que respeten la física, la lógica espacial y la coherencia narrativa básica, es tradicionalmente el talón de Aquiles de los modelos generativos. Un modelo puede generar texturas fotorrealistas y movimientos fluidos, pero si pone a una persona que atraviesa una pared o hace levitar objetos sin motivo, el resultado es alienante.

La comparación con los competidores de código abierto revela un escenario interesante. HunyuanVideo, que tiene un tamaño similar con unos 13.000 millones de parámetros, está optimizado para vídeos más cortos pero con una calidad visual ligeramente superior en clips de tres segundos. Mochi se centra en la velocidad de generación con 10.000 millones de parámetros y una arquitectura asimétrica que prioriza la eficiencia. Open-Sora, el intento de la comunidad de replicar la arquitectura original de OpenAI, se queda atrás en casi todas las métricas. CogVideoX-5B, desarrollado por la Universidad de Tsinghua, tiene un rendimiento comparable a LongCat en vídeos cortos pero no admite generaciones de más de diez segundos. LTX-Video de Lightricks introdujo la promesa de la generación en tiempo real, pero con vídeos limitados a unos pocos segundos.

[Como ya hemos comentado](https://aitalk.it/it/ai-video-generativa-ovi.html) en el artículo sobre Ovi y el ecosistema de código abierto de la generación de vídeo, el panorama está fragmentado entre proyectos que optimizan la calidad absoluta, la velocidad de generación o la duración de los vídeos. LongCat-Video se inserta en este panorama con una propuesta clara: no soy el más rápido, no soy el más fotorrealista, pero puedo generar vídeos largos manteniendo la coherencia. Es un posicionamiento inteligente, porque la duración sigue siendo un fuerte diferenciador. Ovi demostró la importancia de la integración nativa de audio y vídeo, CogVideoX impulsó la coherencia temporal en diez segundos, LongCat-Video extiende esta capacidad a minutos.

El punto crítico es que ninguno de estos modelos de código abierto supera todavía a los sistemas propietarios en calidad absoluta. Sora 2 genera vídeos más fotorrealistas, Veo 3 tiene un estilo cinematográfico más refinado, Movie Gen integra audio y música con calidad profesional. Pero todos estos modelos son cajas negras inaccesibles, cerradas tras API de pago o listas de espera interminables. No puedes estudiarlos, no puedes modificarlos, no puedes verificar con qué datos han sido entrenados. Es como un buffet libre en el que la comida parece mejor pero no sabes qué le han puesto, mientras que en el restaurante de código abierto ves la cocina abierta y conoces cada ingrediente.
![immagine2.jpg](immagine2.jpg)
[Tabla extraída del repositorio de GitHub de LongCat-Video](https://github.com/meituan-longcat/LongCat-Video)

## Código abierto, pero ¿a qué precio?

Decir que LongCat-Video es de código abierto suena democrático y accesible, pero la realidad del hardware cuenta una historia más matizada. Para generar vídeos a 720p y 30 fps en un tiempo razonable se necesitan al menos 60 GB de VRAM, lo que significa configuraciones multi-GPU o tarjetas de nivel empresarial como las NVIDIA A100 o H100. Una sola A100 cuesta tanto como un coche urbano de segunda mano, e incluso con optimizaciones como la cuantización a 8 bits que reduce los requisitos a unos 18 GB en una RTX 4090, seguimos hablando de hardware de entusiasta de gama alta.

Esto plantea la pregunta crucial: ¿democratizar el código pero requerir hardware de cuatro o cinco mil euros es una verdadera democratización o solo una ilusión para quienes pueden permitirse las GPU? La respuesta depende de quién seas. Para un aficionado individual o un creativo independiente, LongCat-Video probablemente siga estando fuera de su alcance a menos que utilice servicios en la nube, lo que reintroduce costes mensuales no muy lejanos a las suscripciones a los modelos propietarios. Para un estudio creativo de pequeño o mediano tamaño, un laboratorio universitario o una startup tecnológica, gastar unos miles de euros en hardware es una inversión única que da un control total y costes operativos predecibles. Sin suscripciones que suben, sin API que cambian los términos, sin riesgo de que el servicio se interrumpa.

La licencia MIT bajo la que se publica LongCat-Video es la más permisiva posible: puedes usarlo comercialmente, modificarlo, integrarlo en productos propietarios, sin royalties ni restricciones de atribución obligatorias más allá del aviso de copyright. Es la licencia que dice "haz lo que quieras, esta es la máxima libertad que podemos darte". En comparación, algunos modelos de código abierto utilizan licencias más restrictivas que limitan el uso comercial o imponen cláusulas de compartición de las modificaciones.

El repositorio de GitHub contiene no solo los pesos del modelo, sino también la documentación completa para el entrenamiento, la inferencia y el ajuste fino. Hay ejemplos de código para cada tarea admitida, scripts para la optimización en diferentes configuraciones de hardware e incluso guías para el despliegue en producción. No es tan sencillo como abrir un navegador y usar Sora, pero es completo y está bien documentado. Un equipo con conocimientos de aprendizaje automático puede estar operativo en días, no en meses.

La verdadera accesibilidad del código abierto en la IA de vídeo pasa, sin embargo, por una reducción progresiva de los requisitos de hardware, algo que históricamente siempre ha sucedido. Los modelos de transformadores de lenguaje que hace cinco años requerían granjas de servidores hoy en día se ejecutan en ordenadores portátiles. La optimización del software, las técnicas de cuantización y los aceleradores especializados están comprimiendo los requisitos constantemente. Dentro de un año, LongCat-Video o sus descendientes podrían ejecutarse en configuraciones de consumo de gama media. En dos años, quizás en smartphones de gama alta. La trayectoria tecnológica siempre va hacia la miniaturización y la eficiencia.

Mientras tanto, LongCat-Video no es el mesías que democratiza la generación de vídeo con IA para todos, pero es un paso concreto hacia vídeos generativos largos, accesibles y modificables para quienes tienen las competencias y los recursos para usarlo. Es una contribución importante a ese ecosistema de código abierto que, como la escena independiente de principios de los 2000, construye alternativas concretas a los productos de masas no para superarlos en presupuesto o brillo, sino para ofrecer libertad, control y transparencia a quienes las buscan. Y a veces, como enseña la historia de la tecnología, son precisamente estas alternativas de "banda de garaje" las que inventan las ideas que mañana se convertirán en la corriente principal.
