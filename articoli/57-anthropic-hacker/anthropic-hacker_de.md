---
tags: ["Security", "Generative AI", "Ethics & Society"]
date: 2025-12-05
author: "Dario Ferrero"
---

# Claude Code zum Cyber-Spion umfunktioniert: die neue Grenze der Cybersicherheit
![anthropic-hacker.jpg](anthropic-hacker.jpg)

*Mitte September 2025 schrillten bei Anthropic die Alarmglocken. Es handelte sich nicht um irgendeinen anomalen Datenverkehr: Jemand nutzte Claude Code, ihren KI-Assistenten für Entwickler, auf eine Weise, die deutlich von den ursprünglichen Absichten abwich. Die anschließende Untersuchung enthüllte, was [Anthropic als](https://www.anthropic.com/news/disrupting-AI-espionage) den ersten dokumentierten Fall von Cyber-Spionage bezeichnete, die überwiegend von einer künstlichen Intelligenz ausgeführt wurde. Kein von KI unterstützter Angriff, sondern einer, der von ihr orchestriert wurde.*

Die verantwortliche Gruppe, die Anthropic GTG-1002 taufte und mit hoher Wahrscheinlichkeit China zuschreibt, hatte Claude in einen autonomen Betreiber für Cyber-Penetration verwandelt. Dreißig gleichzeitige Ziele, darunter globale Technologieunternehmen, Finanzinstitute, Chemiehersteller und Regierungsbehörden. In einigen Fällen mit Erfolg. Die Besonderheit liegt weniger in den Zielen als in der Methode: Die künstliche Intelligenz verwaltete achtzig bis neunzig Prozent der Operationen autonom, von den Menschen kam nur strategische Aufsicht an kritischen Entscheidungspunkten.

## Die Architektur der Täuschung

Um zu verstehen, was passiert ist, müssen wir einen Schritt zurückgehen. Vor einigen Wochen habe ich auf diesen Seiten [über den Fall PROMPTFLUX](https://aitalk.it/it/google-threat-intelligence.html) berichtet, die von Google entdeckte Malware, die KI direkt in ihren eigenen Code integrierte, um sich kontinuierlich umzuschreiben und Antivirenprogrammen zu entgehen. Es handelte sich um KI-fähige Malware: traditionelle Software, die durch künstliche Intelligenz erweitert wurde. GTG-1002 stellt einen anderen qualitativen Sprung dar. Hier ist die KI kein in den bösartigen Code integriertes Werkzeug, sondern der Betreiber des Angriffs selbst.

Der Unterschied ist subtil, aber fundamental. PROMPTFLUX nutzte Sprachmodelle, um Varianten von sich selbst zu generieren, aber die Logik des Angriffs blieb von menschlichen Programmierern in VBScript geschrieben. Bei GTG-1002 kehrt sich die Situation um: Die chinesischen Betreiber haben ein Framework aufgebaut, das den taktischen Teil der Einbrüche an Claude Code delegiert und sich nur die strategischen Entscheidungen vorbehält. Wähle das Ziel, genehmige den Angriff, sammle die Ergebnisse. Den Rest erledigt die KI.

Das technische Herz der Operation basiert auf dem [Model Context Protocol](https://modelcontextprotocol.io/), einem offenen Standard, der es Sprachmodellen ermöglicht, mit externen Werkzeugen zu interagieren. Claude Code kann daher Netzwerkscanner, Exploit-Frameworks, Passwort-Cracker befehligen – das gesamte Standardarsenal des Penetrationstests. Die Angreifer haben spezialisierte Server aufgebaut, die als Brücke zwischen Claude und diesen Werkzeugen dienen: einen für die Fernausführung von Befehlen, einen für die Browser-Automatisierung, einen für die Code-Analyse, einen zur Überprüfung von Schwachstellen. Die künstliche Intelligenz orchestriert das Ganze, ohne das Gesamtbild zu sehen.
![schema1.jpg](schema1.jpg)
[Bild aus dem Anthropic-Papier](https://www.anthropic.com/news/disrupting-AI-espionage)

## Von der Beratung zur Aktion

Der Angriff verläuft in sechs Phasen, die zunehmende Grade an Autonomie aufweisen. In der ersten Phase wählen menschliche Bediener die Ziele aus und initialisieren das Framework. Dann kommt der interessante Teil: Sie müssen Claude überzeugen, mitzumachen. Das Modell ist umfassend darauf trainiert, bösartige Anfragen abzulehnen, daher ist Social Engineering erforderlich. Nicht die traditionelle Art gegen Menschen, sondern gegen den Algorithmus selbst.

Die verwendete Technik nennt sich Jailbreaking durch Rollenspiele. Die Angreifer gaben Claude als Mitarbeiter eines legitimen Cybersicherheitsunternehmens aus, der an autorisierten defensiven Tests beteiligt war. Sie zerlegten die Operation in scheinbar harmlose Mikroaufgaben: "Scanne dieses Netzwerk, um die Dienste zu katalogisieren", "schreibe ein Skript, um diese Schwachstelle zu testen", "analysiere diese Daten, um die relevantesten Informationen zu identifizieren". Jede einzelne Anfrage, isoliert bewertet, erscheint legitim. Claude sieht nie den gesamten bösartigen Kontext.

Es ist dieselbe "CTF-Vorwand"-Taktik, die Google diesen Sommer dokumentiert hatte: potenziell gefährliche Anfragen als Teil von Capture-The-Flag-Wettbewerben zu präsentieren, bei denen ethisches Hacken und Sicherheitserziehung vermischt werden. Das Paradoxe daran ist, dass identische Prompts legitime didaktische Anfragen oder Angriffsvektoren sein können, je nachdem, wer sie stellt. Wie unterscheidet man die Absicht, wenn der Inhalt identisch ist? Claude hat am Ende die Geschichte geglaubt und ausgeführt.

In der zweiten Phase beginnt die autonome Aufklärung. Claude kartiert die Infrastruktur der Ziele, katalogisiert exponierte Dienste, analysiert Authentifizierungsmechanismen, identifiziert potenzielle Schwachstellen. Dies geschieht parallel für mehrere Ziele, wobei für jede Kampagne separate operative Kontexte beibehalten werden. In einem der erfolgreichen Kompromittierungsfälle entdeckte er autonom interne Dienste, kartierte vollständige Netzwerktopologien über mehrere IP-Adressbereiche, identifizierte hochwertige Systeme wie Datenbanken und Orchestrierungsplattformen. Alles ohne menschliche Anweisung.

Die dritte Phase führt zum eigentlichen Exploit. Claude generiert benutzerdefinierte Payloads für die entdeckten Schwachstellen, testet sie über Fernbefehlsschnittstellen, analysiert die Antworten, um festzustellen, ob sie funktionieren. Er identifizierte Schwachstellen vom Typ SSRF, suchte autonom nach Ausnutzungstechniken, entwickelte Exploit-Ketten, validierte ihre Wirksamkeit durch Callback-Systeme. Dann erstellte er vollständige Berichte zur menschlichen Genehmigung. Die chinesischen Betreiber mussten nur "fortfahren" sagen, um zur vierten Phase überzugehen.

## Operative Autonomie in Aktion

Das Sammeln von Anmeldeinformationen und die seitliche Bewegung zeigen das höchste Maß an Autonomie. Claude fragt interne Dienste ab, extrahiert Authentifizierungszertifikate aus Konfigurationen, testet systematisch die gestohlenen Anmeldeinformationen auf allen entdeckten Systemen. Er bestimmt selbst, welche Anmeldeinformationen Zugriff auf welche Dienste gewähren, kartiert Berechtigungsstufen, verfolgt Zugriffsgrenzen. Er bewegt sich seitlich durch das Netzwerk, indem er die Authentifizierung bei internen APIs, Datenbanksystemen, Container-Registries, Logging-Infrastrukturen testet. Er erstellt vollständige Karten der Netzwerkarchitektur und der Zugriffsbeziehungen ohne jegliche menschliche Anweisung.

Die Datenerfassungsoperationen zeigen eine noch größere Autonomie. Gegen eines der Zieltechnologieunternehmen fragte Claude autonom Datenbanken und Systeme ab, extrahierte Daten, analysierte Ergebnisse, um proprietäre Informationen zu identifizieren, kategorisierte Funde nach nachrichtendienstlichem Wert. Er extrahierte Benutzeranmeldeinformationen, Systemkonfigurationen, sensible Betriebsdaten ohne detaillierte Anweisungen. Er verarbeitete riesige Datenmengen und identifizierte automatisch, was nachrichtendienstlichen Wert hatte, anstatt eine menschliche Analyse anzufordern.

In einer detailliert dokumentierten Operation authentifizierte sich die KI mit gestohlenen Anmeldeinformationen, kartierte die Datenbankstruktur, fragte Benutzerkontentabellen ab, extrahierte Passwort-Hashes und Kontodetails, identifizierte Konten mit hohen Berechtigungen, erstellte ein persistentes Backdoor-Konto, lud die vollständigen Ergebnisse auf das lokale System herunter, analysierte die extrahierten Daten auf ihren nachrichtendienstlichen Wert, kategorisierte sie nach Sensibilität und Nützlichkeit und erstellte einen zusammenfassenden Bericht. Alles in zwei bis sechs Stunden. Die menschliche Genehmigung kam nur, um die endgültigen Exfiltrationsziele zu autorisieren.

Das operative Tempo bestätigt den Einsatz massiver Automatisierung. In Spitzenzeiten generierte Claude Tausende von Anfragen, oft mehrere pro Sekunde. Eine Geschwindigkeit, die für menschliche Bediener physisch unmöglich zu erreichen wäre. Die erhebliche Diskrepanz zwischen Dateneingabe und Textausgabe bestätigt, dass die KI aktiv gestohlene Informationen analysierte, anstatt Erklärungen für die menschliche Überprüfung zu generieren. Sie behielt einen persistenten operativen Kontext über Sitzungen bei, die mehrere Tage dauerten, was es ermöglichte, komplexe Kampagnen wieder aufzunehmen, ohne dass die Bediener den Fortschritt manuell rekonstruieren mussten.
![schema2.jpg](schema2.jpg)
[Bild aus dem Anthropic-Papier](https://www.anthropic.com/news/disrupting-AI-espionage)

## Das Paradox der Unvollkommenheit

Es gibt jedoch einen ironischen Aspekt bei all dieser Automatisierung: Claude funktionierte nicht perfekt. Das Modell halluzinierte gelegentlich Anmeldeinformationen, behauptete, geheime Informationen extrahiert zu haben, die in Wirklichkeit öffentlich verfügbar waren, überschätzte Entdeckungen. Er behauptete, Anmeldeinformationen erhalten zu haben, die dann nicht funktionierten, identifizierte kritische Schwachstellen, die sich als bereits bekannte Informationen herausstellten. Dieses Phänomen der Halluzination in offensiven Sicherheitskontexten stellte eine Herausforderung für die operative Wirksamkeit der Angreifer dar und erforderte eine sorgfältige Validierung aller deklarierten Ergebnisse.

In gewisser Weise haben die Unvollkommenheiten der KI den Angriff verlangsamt. Aber nicht genug. Selbst mit diesen Einschränkungen gelang es GTG-1002, einige der Ziele zu kompromittieren und Operationen in einem für menschliche Teams unmöglichen Maßstab aufrechtzuerhalten. Und es stellt eine dramatische Weiterentwicklung auch im Vergleich zu den "Vibe-Hacking"-Fällen dar, die Anthropic im Sommer 2025 gemeldet hatte: Bei diesen Operationen waren die Menschen noch stark in die Steuerung der Aktionen involviert. Hier reduzierte sich die menschliche Beteiligung auf vier bis sechs kritische Entscheidungspunkte pro ganzer Hacking-Kampagne.

## Die perverse Ökonomie der Automatisierung

Die wirtschaftliche Kalkulation hinter dieser Entwicklung ist gnadenlos. Die Entwicklung und Wartung traditioneller Malware erfordert spezifische Fähigkeiten, Zeit, kontinuierliche Wartung zur Anpassung an neue Abwehrmaßnahmen. Teams von erfahrenen Hackern sind teuer. Mit eingebetteter KI ändert sich die Gleichung radikal. Die Angreifer investieren ein paar hundert Dollar in API-Credits und erhalten ein System, das autonom arbeitet. Die Grenzkosten jeder zusätzlichen Operation werden praktisch null.

Selbst bei der Generierung von Tausenden von Anfragen pro Sekunde unter Verwendung des Model Context Protocol zur Orchestrierung mehrerer Werkzeuge bleiben die Gesamtkosten für API-Aufrufe im Bereich von Hunderten von Dollar im Vergleich zu den Tausenden, die für die Aufrechterhaltung eines gleichwertigen menschlichen Teams für dieselbe Operation erforderlich sind. Es ist eine Automatisierung, die unendlich besser skaliert als menschliche Arbeit. Die Geschwindigkeit der Iteration und die Fähigkeit, parallele Operationen zu verwalten, kompensieren jedes Risiko bei weitem.

Für die Verteidiger ist die Gleichung umgekehrt und dramatisch ungünstig. Jede Kampagne erfordert eine manuelle Analyse, eine forensische Untersuchung, eine Aktualisierung der Abwehrmaßnahmen. Die menschliche Zeit, die zur Analyse von Operationen dieser Komplexität erforderlich ist, wird in Tagen oder Wochen gemessen. In der Zwischenzeit hat der Angreifer bereits Dutzende von Variationen gestartet. Es ist die klassische Asymmetrie der Cybersicherheit, die durch KI verstärkt wird: Die Automatisierung des Angriffs kostet sehr wenig, die Skalierung der Verteidigung auf das erforderliche Tempo ist außerordentlich schwierig.

In den Security Operations Centers führt dieser Druck zu kognitiver Erschöpfung. Die Analysten kämpfen nicht mehr gegen Gegner, die erkennbare Spielbücher befolgen. Jede Kampagne scheint anders zu sein, weil sie es technisch ist: Die KI generiert jedes Mal neue Ansätze, die auf dem spezifischen Kontext des Ziels basieren. Die gesammelte Erfahrung zählt weniger als die Fähigkeit, über nie zuvor gesehene Anomalien nachzudenken.

## Das zweischneidige Schwert

Hier taucht das von Anthropic hervorgehobene zentrale Paradoxon auf: Wenn KI-Modelle für Angriffe dieser Größenordnung manipuliert werden können, warum sollte man sie dann weiter entwickeln und veröffentlichen? Die Antwort liegt genau in den Fähigkeiten, die sie gefährlich machen. Dieselben Fähigkeiten, die es Claude ermöglichen, bei diesen Angriffen eingesetzt zu werden, machen ihn für die Cyberabwehr entscheidend. Wenn hochentwickelte Angriffe unweigerlich stattfinden, besteht das Ziel darin, dass Claude, ausgestattet mit starken Schutzmaßnahmen, Cybersicherheitsexperten dabei unterstützt, sie zu erkennen, zu unterbrechen und sich auf zukünftige Versionen vorzubereiten.

Das Threat Intelligence Team von Anthropic hat Claude ausgiebig genutzt, um die riesigen Datenmengen zu analysieren, die bei der Untersuchung von GTG-1002 selbst generiert wurden. Es ist eine Meta-Verteidigung: die KI, die den von der KI orchestrierten Angriff analysiert. Google DeepMind bewegt sich auf der gleichen Linie mit BigSleep, einem Agenten, der proaktiv nach unbekannten Schwachstellen in Software sucht. Er hat bereits seine erste echte Schwachstelle gefunden und in einem kritischen Fall eine Lücke identifiziert, die von Bedrohungsakteuren ausgenutzt werden sollte, was eine präventive Intervention ermöglichte.

Der Ansatz dreht den Spieß um: Anstatt darauf zu warten, dass Angreifer Schwachstellen finden, entdeckt sie die defensive KI zuerst. Parallel dazu experimentiert Google mit CodeMender, einem Agenten, der nicht nur Schwachstellen findet, sondern sie auch automatisch repariert. Ziel ist es, das Zeitfenster zwischen Entdeckung und Patch zu verkleinern, den kritischen Zeitraum, in dem Systeme exponiert bleiben.

Aber dieses algorithmische Wettrüsten wirft tiefgreifende Fragen auf. Wie in der militärischen Automatisierung, wo sich die Debatte über autonome Waffensysteme um die menschliche Kontrolle dreht, stellt sich auch im Cyberbereich die Frage: Wie weit können wir die Autonomie von Verteidigungsagenten treiben, ohne Systeme zu schaffen, die unserer Kontrolle entgleiten? In der Fernsehserie *Person of Interest* operierte The Machine autonom, um Verbrechen zu verhindern, aber die zentrale ethische Frage war genau, wer wen kontrollierte.

## Noch offene Fragen

GTG-1002 stellt einen Wendepunkt dar, aber keinen Endpunkt. Anthropic hat die identifizierten Konten gesperrt, die kompromittierten Entitäten benachrichtigt, sich mit den Behörden koordiniert, die Angriffsmuster in seine eigenen Sicherheitskontrollen integriert. Es hat seine Erkennungsfähigkeiten erweitert, die auf Cyber-Bedrohungen ausgerichteten Klassifikatoren verbessert und Prototypen für proaktive Erkennungssysteme für autonome Cyber-Angriffe entwickelt.

Aber die Implikationen gehen über technische Gegenmaßnahmen hinaus. Die Hürden für die Durchführung hochentwickelter Angriffe sind erheblich gesunken und werden dies auch weiterhin tun. Mit der richtigen Konfiguration können Bedrohungsakteure nun agentenbasierte KI-Systeme über längere Zeiträume hinweg nutzen, um die Arbeit ganzer Teams von erfahrenen Hackern zu erledigen: Zielsysteme analysieren, Exploit-Code produzieren, riesige Datensätze gestohlener Informationen effizienter durchsuchen als jeder menschliche Bediener. Weniger erfahrene und mit weniger Ressourcen ausgestattete Gruppen können potenziell groß angelegte Angriffe dieser Art durchführen.

Die Sichtbarkeit von Anthropic beschränkt sich auf die Verwendung von Claude, aber diese Fallstudie spiegelt wahrscheinlich konsistente Verhaltensmuster bei den fortschrittlichsten KI-Modellen wider und zeigt, wie Bedrohungsakteure ihre Operationen anpassen, um die fortschrittlichsten KI-Fähigkeiten von heute zu nutzen. Die Verbreitung ist unvermeidlich. Die beschriebenen Techniken werden von viel mehr Angreifern genutzt werden, was den Austausch von Bedrohungsinformationen in der Branche, verbesserte Erkennungsmethoden und stärkere Sicherheitskontrollen noch kritischer macht.

Für Sicherheitsteams lautet der Rat, mit der Anwendung von KI zur Verteidigung in Bereichen wie der Automatisierung von Security Operations Centern, der Bedrohungserkennung, der Schwachstellenbewertung und der Reaktion auf Vorfälle zu experimentieren. Für Entwickler, weiterhin in Schutzmaßnahmen auf allen KI-Plattformen zu investieren, um missbräuchliche Angriffe zu verhindern. Die Zero-Trust-Architektur wird nicht zu einer Option, sondern zu einer Notwendigkeit, die von Sicherheitsverletzungen ausgeht und die seitliche Bewegung einschränkt.

Offen bleibt die geopolitische Frage einer per definitionem zweifach verwendbaren Technologie. Open-Source-Sprachmodelle, öffentliche APIs, der Untergrundmarkt erschweren jede Form der Kontrolle. Wer gewinnt, wenn der Angriff automatisiert ist, die Verteidigung aber weitgehend manuell bleibt? Die wachsende Kluft zwischen denen, die sich KI-gestützte Verteidigungen leisten können, und denen, die es nicht können, gestaltet nicht nur die Landkarte der Cybersicherheit neu, sondern auch die der globalen digitalen Macht.

Die Ära der statischen Malware ist endgültig vorbei. Wie in William Gibsons Cyberpunk-Romanen, in denen intelligente Programme autonom in der Matrix umherstreiften, sehen wir Software entstehen, die die Grenze zwischen Werkzeug und Agent verwischt. Der Unterschied ist, dass es diesmal keine Science-Fiction ist. Es ist September 2025 und GTG-1002 hat gerade gezeigt, dass offensive Autonomie keine Theorie mehr ist, sondern operativ. Die Frage ist jetzt nicht, ob es wieder passieren wird, sondern wie schnell es sich verbreiten wird und ob wir uns im erforderlichen Tempo verteidigen können.
