---
tags: ["Ethics & Society", "Generative AI", "Business"]
date: 2025-12-19
author: "Dario Ferrero"

---
# La soif de l'intelligence artificielle : comment les centres de données redéfinissent la géographie de l'eau
![ai-water-consumption.jpg](ai-water-consumption.jpg)

*Lorsque ChatGPT génère une réponse de vingt lignes, nous ne pensons probablement pas à l'eau. Pourtant, quelque part dans le monde, un centre de données évapore environ un demi-litre d'eau pour permettre à cette conversation d'exister. Ce n'est pas une métaphore : c'est de la thermodynamique pure. L'intelligence artificielle, qui semble si éthérée et immatérielle lorsqu'elle flotte sur nos écrans, plonge ses racines dans une réalité physique faite de silicium, d'électricité et, de plus en plus, d'eau.*

Le lien entre la pensée artificielle et les ressources en eau n'est pas immédiatement évident, mais il a une logique de fer. Les modèles de langage, les réseaux neuronaux pour la reconnaissance d'images, les systèmes de recommandation qui orchestrent notre consommation numérique : tout cela nécessite du calcul. Et le calcul génère de la chaleur. Comme l'observait déjà William Gibson dans ses romans cyberpunk des années 80, la technologie n'habite pas un plan abstrait et parfait, mais s'incarne toujours dans un substrat matériel avec toutes ses limites physiques. Le silicium des puces NVIDIA H100 et A100, celles qui alimentent ChatGPT, Claude, Midjourney et tous les autres services d'IA générative, peut atteindre des températures supérieures à 80 degrés Celsius lorsqu'il fonctionne à pleine capacité. Sans un système de refroidissement efficace, ces puces s'éteindraient en quelques minutes, fondant littéralement.

## Anatomie de la soif numérique

Pour comprendre pourquoi l'IA boit de l'eau, nous devons entrer dans les entrailles d'un centre de données moderne. Imaginons une salle de la taille d'un terrain de football, remplie de racks métalliques de deux mètres de haut. Chaque rack contient des dizaines de serveurs, chaque serveur héberge des processeurs et des GPU qui exécutent des milliards d'opérations par seconde. L'air est constamment traversé par un bourdonnement profond, celui des ventilateurs qui tentent désespérément de dissiper la chaleur. Mais l'air seul, désormais, ne suffit plus.

Les centres de données consomment de l'eau principalement par deux mécanismes : direct et indirect. La consommation directe se produit lorsque l'eau est utilisée dans les systèmes de [refroidissement par évaporation](https://energy.ec.europa.eu/news/focus-data-centres-energy-hungry-challenge-2025-11-17_en), où l'eau s'évapore dans les tours de refroidissement en soustrayant de la chaleur à l'environnement. C'est le même principe qui nous fait transpirer quand nous avons chaud : l'évaporation élimine l'énergie thermique. Dans un centre de données, l'eau circule à travers des échangeurs de chaleur, absorbe la chaleur générée par les serveurs, puis est pompée vers des tours de refroidissement où elle s'évapore dans l'atmosphère. La consommation indirecte est plus subtile mais tout aussi importante : elle concerne l'eau utilisée pour produire l'électricité qui alimente les centres de données, surtout lorsqu'elle provient de centrales thermiques qui utilisent de l'eau pour refroidir leurs systèmes.

L'industrie mesure l'efficacité de l'utilisation de l'eau à l'aide d'une métrique appelée WUE, ou Water Usage Effectiveness. Elle se calcule en divisant le volume total annuel d'eau consommée (en litres) par l'énergie informatique utilisée (en kilowattheures). Un WUE de 0,30 litre par kWh, [la valeur moyenne atteinte par Microsoft en 2024](https://www.microsoft.com/en-us/microsoft-cloud/blog/2024/07/25/sustainable-by-design-transforming-datacenter-water-efficiency/), signifie que pour chaque kilowattheure d'énergie consommée par les serveurs, 0,30 litre d'eau s'évapore. Cela peut sembler peu, mais lorsque nous multiplions par les gigawatts consommés par un centre de données moderne, les chiffres deviennent vertigineux.

La situation s'est aggravée avec l'avènement de l'IA. Les charges de travail d'apprentissage automatique sont beaucoup plus intensives que les charges traditionnelles : une requête à GPT-4 nécessite environ 10 fois plus de puissance de calcul qu'une recherche sur Google. Selon une [étude publiée en 2023](https://news.ucr.edu/articles/2023/04/28/ai-programs-consume-large-volumes-scarce-water) par des chercheurs de l'Université de Californie à Riverside et du Texas, l'entraînement de GPT-3 a consommé environ 700 000 litres d'eau, soit l'équivalent de ce qui est nécessaire pour produire 320 voitures Tesla. Et c'était GPT-3 : les modèles suivants sont des ordres de grandeur plus grands et plus assoiffés.

Le problème est que tous les centres de données ne se valent pas. La quantité d'eau qu'ils consomment dépend de facteurs variables : le climat local, l'efficacité des infrastructures, le type de charges de travail. Un centre de données en Arizona, où les températures estivales dépassent les 40 degrés, consommera beaucoup plus d'eau qu'un en Finlande, où l'air froid peut être exploité pour un refroidissement naturel une bonne partie de l'année. Cette variabilité rend difficile l'établissement d'estimations précises, mais elle met aussi en évidence que la géographie compte : construire un centre de données dans le désert a un coût en eau très différent de le construire près du cercle polaire arctique.
![datacenter1.jpg](datacenter1.jpg)
[Image de news.ucr.edu](https://news.ucr.edu/articles/2023/04/28/ai-programs-consume-large-volumes-scarce-water)

## Les chiffres du présent

Combien boit exactement l'intelligence artificielle ? La [Commission européenne estime](https://energy.ec.europa.eu/news/focus-data-centres-energy-hungry-challenge-2025-11-17_en) qu'en 2024, les centres de données européens ont consommé environ 70 térawattheures d'électricité. L'Agence internationale de l'énergie prévoit que ce chiffre passera à 115 térawattheures d'ici 2030, l'IA étant le principal moteur de cette croissance. Au niveau mondial, l'AIE calcule que les centres de données sont responsables d'environ 415 térawattheures par an, soit 1,5 % de la consommation électrique mondiale, un chiffre qui devrait doubler pour atteindre 945 térawattheures d'ici la fin de la décennie.

Mais l'électricité n'est que la moitié de l'histoire. [Google a révélé](https://dgtlinfra.com/data-center-water-usage/) qu'en 2024, ses centres de données ont consommé au total environ 6,6 milliards de gallons d'eau (25,1 milliards de litres), avec une consommation nette de 5,2 milliards de gallons après soustraction de l'eau rejetée. Le centre de données le plus gourmand de l'entreprise se trouve à Council Bluffs, dans l'Iowa, où 1 milliard de gallons ont été consommés en 2024, assez pour approvisionner tous les habitants de l'Iowa pendant cinq jours. En revanche, le centre de données de Pflugerville, au Texas, n'a consommé que 10 000 gallons, ce qui montre à quel point les choix technologiques et climatiques peuvent faire la différence.

[Meta a communiqué](https://dgtlinfra.com/data-center-water-usage/) que ses centres de données ont consommé 663 millions de gallons d'eau (2,5 milliards de litres) en 2023, 95 % de la consommation totale de l'entreprise étant concentrée précisément dans les infrastructures numériques. Microsoft, qui gère environ 300 centres de données dans le monde, a enregistré [une consommation moyenne d'environ 33 millions de gallons par centre de données et par an](https://www.geekwire.com/2024/microsoft-shares-new-data-center-design-featuring-a-closed-loop-water-system-for-cooling/) avant les récentes optimisations.

Ces chiffres, impressionnants en valeur absolue, deviennent encore plus significatifs lorsqu'on les compare à la consommation locale. À The Dalles, dans l'Oregon, où Google exploite l'un de ses plus grands centres de données, [l'entreprise consomme 29 % de l'approvisionnement en eau de toute la ville](https://www.datacenterdynamics.com/en/news/we-now-know-how-much-water-googles-oregon-data-centers-use-after-city-drops-lawsuit-against-journalists/). En 2021, le centre de données a utilisé 355 millions de gallons, une consommation triplée par rapport à 2017 où elle s'élevait à 124 millions. La ville, située le long du fleuve Columbia mais dans une région météorologiquement aride, a dû faire face à une bataille juridique de 13 mois avant que ces données ne soient rendues publiques : Google et la municipalité soutenaient qu'il s'agissait de "secrets commerciaux". Ce n'est qu'après l'intervention du district judiciaire et la pression du Reporters Committee for Freedom of the Press que [les chiffres ont été divulgués](https://businessjournalism.org/2023/11/oregonian-data-centers/).
![datacenter2.jpg](datacenter2.jpg)
[Image de dgtlinfra.com](https://dgtlinfra.com/data-center-water-usage/)

## Géographie du conflit : où l'eau rencontre les puces

La tension entre le développement technologique et les ressources en eau n'est pas un problème abstrait : elle se manifeste par des conflits tangibles, avec des noms et des coordonnées géographiques précises. The Dalles représente un cas emblématique, mais il n'est pas isolé. Dans tout le monde occidental, les communautés locales se trouvent à devoir négocier entre promesses économiques et durabilité environnementale.

Dans le comté de Newton, en Géorgie, Meta a construit un centre de données qui [consomme environ 10 % de l'eau de tout le comté](https://cee.illinois.edu/news/AIs-Challenging-Waters). En Aragon, en Espagne, Amazon a demandé une augmentation de 48 % de sa consommation d'eau pour étendre ses installations, portant le total à plus de 500 millions de litres par an dans une région déjà touchée par des sécheresses récurrentes. Les autorités locales ont dû trouver un équilibre entre les 1 200 emplois promis et les protestations des agriculteurs et des écologistes.

Le Texas représente peut-être le champ de bataille le plus intense. Le Houston Advanced Research Center a estimé qu'entre 2025 et 2030, les centres de données de l'État consommeront entre 49 et 399 milliards de gallons d'eau, une fourchette énorme qui reflète l'incertitude quant au développement de l'IA. Microsoft construit une usine à Goodyear, en Arizona, qui a déclenché de vives critiques : dans une région confrontée à une "méga-sécheresse" de vingt ans, selon le professeur Christopher Castro de l'Université de l'Arizona, un seul centre de données consommant 1,75 million de gallons par jour apparaît à beaucoup comme une absurdité écologique.

La question ne concerne pas seulement la quantité absolue, mais aussi la concurrence pour des ressources rares. [Une étude de Virginia Tech](https://www.sciencedirect.com/science/article/abs/pii/S0921344925001892) a révélé qu'un cinquième des centres de données américains puisent dans des bassins hydrographiques déjà sous tension, classés comme modérément ou fortement stressés. Dans ces contextes, chaque gallon utilisé pour refroidir des puces est un gallon soustrait à l'agriculture, aux écosystèmes fluviaux, à la consommation domestique. La rivière Dog, qui alimente The Dalles, abrite des espèces de poissons en voie de disparition : le doublement de la consommation de la ville entre 2002 et 2021, principalement dû à Google, a mis ces écosystèmes en danger concret.

La dynamique rappelle, à certains égards, les tensions autour de l'extraction de lithium en Amérique du Sud : des technologies qui promettent un avenir plus durable (véhicules électriques là-bas, efficacité de calcul ici) mais qui prélèvent leur tribut sur les communautés locales et les environnements fragiles. Ce n'est pas un hasard si à The Dalles, certains habitants ont commencé à appeler Google "vampire de l'eau", comme [l'a rapporté IT Pro](https://www.itpro.com/server-storage/data-centres/369767/google-data-centre-soaks-up-a-third-of-oregon-citys-water-supply).
![datacenter3.jpg](datacenter3.jpg)
[Le centre de données de The Dalles, dans l'Oregon, libère de la vapeur d'eau de ses tours de refroidissement. Image de dgtlinfra.com](https://dgtlinfra.com/data-center-water-usage/)

## Solutions terrestres : de l'air au liquide

Face à la croissance exponentielle de la demande, l'industrie des centres de données explore des solutions allant de l'optimisation incrémentale à la révolution radicale. Les stratégies peuvent être classées sur un spectre allant de l'évolutif au disruptif.

L'approche la plus conservatrice est l'amélioration des systèmes existants. Microsoft a annoncé en 2024 avoir [réduit son WUE de 39 % par rapport à 2021](https://www.microsoft.com/en-us/microsoft-cloud/blog/2024/07/25/sustainable-by-design-transforming-datacenter-water-efficiency/), atteignant 0,30 litre par kilowattheure grâce à des audits opérationnels qui ont éliminé 90 % des cas de gaspillage d'eau et à l'expansion de l'utilisation d'eau recyclée ou récupérée au Texas, à Washington, en Californie et à Singapour. Google a optimisé ses systèmes de refroidissement par évaporation au point que le centre de données de Pflugerville, au Texas, ne consomme que 10 000 gallons par an, un résultat obtenu en exploitant le climat local et des technologies avancées de gestion thermique.

Mais l'innovation la plus significative concerne le passage au refroidissement liquide direct. Contrairement aux systèmes évaporatifs traditionnels, le refroidissement liquide amène le réfrigérant en contact direct avec les composants qui génèrent de la chaleur, éliminant ou réduisant considérablement l'évaporation. Il existe plusieurs variantes de cette technologie : le refroidissement liquide direct sur puce, où de fins tubes transportent le liquide de refroidissement directement sur les processeurs ; l'échangeur de chaleur de porte arrière, où les échangeurs de chaleur sont montés à l'arrière des racks ; et le refroidissement par immersion, où des serveurs entiers sont immergés dans des fluides diélectriques non conducteurs.

[Microsoft a annoncé en août 2024](https://www.microsoft.com/en-us/microsoft-cloud/blog/2024/12/09/sustainable-by-design-next-generation-datacenters-consume-zero-water-for-cooling/) que tous ses nouveaux centres de données utiliseront des conceptions à "zéro évaporation d'eau pour le refroidissement". Le système recycle l'eau en circuit fermé : le liquide absorbe la chaleur des puces, est refroidi par des refroidisseurs, et retourne en circulation sans jamais s'évaporer. Cette technologie, qui sera pilotée à Phoenix et à Mt. Pleasant dans le Wisconsin à partir de 2026, devrait éviter l'évaporation de plus de 125 millions de litres par an et par centre de données. Les sites conçus avec ce système entreront en service à partir de la fin de 2027.

Le refroidissement par immersion représente la limite extrême de cette approche. Les serveurs sont littéralement immergés dans des cuves remplies d'huile diélectrique, des fluides synthétiques qui ne conduisent pas l'électricité mais qui transfèrent la chaleur de manière extrêmement efficace. [Intel a mené des expériences](https://www.datacenterfrontier.com/cooling/article/55130995/turn-up-the-volume-data-center-liquid-immersion-cooling-advancements-so-far-in-2024) avec 24 serveurs Xeon immergés dans de l'huile synthétique dans son laboratoire de Hillsboro, en Oregon. Google a testé le refroidissement par immersion pour ses puces TPU v5p. Le marché mondial du refroidissement par immersion, [évalué à 426,56 millions de dollars en 2024](https://straitsresearch.com/report/immersion-cooling-market-in-data-centers), devrait atteindre 2,9 milliards d'ici 2033, avec une croissance annuelle de 23,81 %.

Le refroidissement liquide global, incluant toutes les variantes, affiche des chiffres encore plus impressionnants. [Selon Verified Market Research](https://www.globenewswire.com/news-release/2025/02/05/3021305/28124/en/48-42-Billion-Data-Center-Liquid-Cooling-Markets-2024-2025-and-2034-Key-Growth-Drivers-Include-Advanced-Technologies-such-as-Immersion-and-Direct-to-Chip-Cooling.html), le marché est passé de 5,65 milliards de dollars en 2024 à une projection de 48,42 milliards d'ici 2034, avec un taux de croissance annuel de 23,96 %. NVIDIA a spécifié que ses puces GB200 de nouvelle génération nécessiteront un refroidissement liquide direct sur puce, consacrant de fait la transition comme inévitable pour les charges de travail d'IA de nouvelle génération.

Mais le refroidissement liquide comporte ses propres défis. L'investissement initial peut dépasser 50 000 dollars par rack, soit environ le triple d'un système à air équivalent. Il manque encore des normes uniformes : ASHRAE et TIA ont publié des lignes directrices, mais les formats des connecteurs, les protocoles des capteurs et les chimies des réfrigérants varient d'un fournisseur à l'autre, risquant de créer des écosystèmes propriétaires qui entravent les mises à jour futures. L'installation dans des centres de données existants nécessite des rénovations complexes, des conduits, des supports structurels renforcés, des systèmes de détection de fuites, qui rendent souvent plus pratique de construire à partir de zéro.
![datacenter4.jpg](datacenter4.jpg)
[Un système de refroidissement liquide. Image de iconicdc.co.uk](https://iconicdc.co.uk/services/)

## Visions extrêmes : de l'océan à l'orbite

Lorsque les solutions conventionnelles semblent insuffisantes, la technologie a tendance à explorer des frontières plus radicales. Dans le cas des centres de données, cela a signifié regarder au-delà des frontières terrestres traditionnelles : sous l'eau et dans l'espace.

Le projet Natick de Microsoft, lancé en 2018, représente probablement l'expérience la plus connue de centre de données sous-marin. L'idée avait une logique fascinante : immerger un conteneur cylindrique avec 864 serveurs à 36 mètres de profondeur au large des îles Orcades, en Écosse, en utilisant l'eau de mer froide comme réfrigérant naturel infini. Le projet a démontré que les serveurs sous-marins peuvent être plus fiables que les serveurs terrestres, les taux de panne étant un huitième de ceux des installations conventionnelles, probablement parce que l'environnement hermétique, dépourvu d'oxygène et d'humidité, réduit la corrosion et les chocs thermiques.

Cependant, le projet Natick a été officiellement arrêté en 2024. Les raisons sont complexes : l'accessibilité pour la maintenance était problématique, les coûts d'installation et de récupération prohibitifs pour un déploiement à grande échelle, et les incertitudes réglementaires sur l'exploitation des espaces océaniques ont rendu le modèle peu évolutif. Microsoft a déclaré avoir tiré des leçons précieuses sur la dissipation thermique et la durabilité du matériel, mais a décidé de concentrer ses efforts sur le refroidissement liquide conventionnel plutôt que de poursuivre des infrastructures sous-marines.

Si l'océan s'est avéré plus compliqué que prévu, l'espace émerge comme une frontière véritablement active. En novembre 2024, [China Telecom a lancé Starcloud-1](https://www.datacenterfrontier.com/cooling/article/55130995/turn-up-the-volume-data-center-liquid-immersion-cooling-advancements-so-far-in-2024), le premier centre de données satellite commercial équipé de GPU NVIDIA H100, conçu pour exécuter des charges de travail d'IA en orbite terrestre basse. L'idée est d'utiliser le vide spatial comme un dissipateur thermique parfait : la chaleur est rayonnée directement dans l'espace par des radiateurs, sans avoir besoin d'eau ou d'air.

Google développe le projet Suncatcher, prévu pour 2027, qui vise à combiner des panneaux solaires orbitaux avec des capacités de calcul, éliminant complètement la dépendance aux réseaux électriques terrestres. Plusieurs startups, dont Lumen Orbit et Orbital Assembly, conçoivent des centres de données spatiaux commerciaux qui pourraient entrer en orbite d'ici 2030.

Mais ici aussi, la réalité physique impose des limites sévères. Un article intitulé "Dirty Bits in Low-Earth Orbit" a mis en évidence des problèmes critiques : les coûts de lancement restent prohibitifs (environ 10 000 dollars par kilogramme en orbite basse, même avec SpaceX) ; les radiations cosmiques endommagent les composants électroniques beaucoup plus rapidement que sur Terre ; la maintenance est pratiquement impossible une fois en orbite ; et la latence des communications avec la surface, bien que minime, est toujours supérieure à celle des centres de données terrestres connectés par fibre optique.

Aussi fascinantes soient-elles, ces solutions extrêmes mettent surtout en évidence la difficulté du problème central : gérer d'énormes quantités de chaleur de manière durable. Le fait que l'on envisage sérieusement d'envoyer des serveurs dans l'espace en dit long sur la pression que subit l'industrie.

## Questions ouvertes : l'avenir incertain

Après avoir cartographié le présent et entrevu de possibles futurs, nous restons avec des interrogations qui n'admettent pas de réponses faciles. La première concerne la transparence. Malgré les progrès en matière de divulgation — Google et Microsoft publient désormais des données au niveau des installations, Meta fournit des rapports détaillés — [de nombreuses entreprises continuent de considérer la consommation d'eau comme une information propriétaire](https://www.opb.org/article/2021/09/29/google-water-data-center-the-dalles-oregon/). Amazon Web Services, le plus grand fournisseur de cloud au monde, fournit des données agrégées minimales. La [Commission européenne a introduit](https://energy.ec.europa.eu/news/focus-data-centres-energy-hungry-challenge-2025-11-17_en) des obligations de surveillance et de rapport pour les centres de données à consommation importante, mais la mise en œuvre reste fragmentaire. Sans une transparence totale, il est impossible pour les communautés locales, les régulateurs et les citoyens d'évaluer réellement l'impact et de prendre des décisions éclairées.

Le deuxième interrogatoire concerne les compromis entre les objectifs environnementaux. Le refroidissement liquide réduit considérablement la consommation d'eau, mais peut augmenter légèrement la consommation d'électricité ; dans le cas de Microsoft, l'augmentation est qualifiée de "nominale", mais les détails quantitatifs restent vagues. Si cette électricité provient de sources fossiles, ne faisons-nous que déplacer le problème de l'eau vers l'atmosphère ? De plus, les fluides diélectriques utilisés dans le refroidissement par immersion soulèvent des inquiétudes : certains contiennent des PFAS (substances per- et polyfluoroalkylées), les "produits chimiques éternels" qui persistent dans l'environnement pendant des siècles. L'industrie développe des alternatives végétales, mais la transition est lente.

Troisièmement : la question de la gouvernance locale. Qui décide si un centre de données peut être construit dans une région déjà en stress hydrique ? The Dalles a accordé à Google des allégements fiscaux de 260 millions de dollars et un accès privilégié à l'eau en échange d'investissements et d'emplois. Mais [les habitants se plaignent](https://fortune.com/longform/google-data-center-the-dalles-oregon-water-dispute/) que les décisions ont été prises sans consultation adéquate, avec des accords de non-divulgation qui ont empêché le débat public. Comment équilibrer le développement économique et la protection des ressources communes ? Les structures de gouvernance existantes semblent inadéquates pour gérer des infrastructures mondiales ayant des impacts locaux aussi intenses.

Quatrièmement : l'évolutivité des solutions. Même en supposant que le refroidissement liquide devienne une norme universelle d'ici 2030, éliminera-t-il le problème ? Les projections de l'AIE indiquent que la demande de capacité de calcul continuera de croître de manière exponentielle, non seulement pour l'IA, mais aussi pour l'IdO, la 5G, la réalité augmentée, le calcul quantique. Chaque nouvelle technologie promet d'être "plus efficace", mais le volume absolu de calcul augmente plus rapidement que l'efficacité. C'est le paradoxe de Jevons appliqué à l'ère numérique : l'augmentation de l'efficacité énergétique entraîne souvent une consommation totale plus importante, car elle rend le calcul moins cher et donc plus utilisé.

Enfin, il y a la question éthique la plus profonde. Combien vaut une requête à ChatGPT ? Combien vaut une vidéo générée par Sora ? Si la production d'une image avec Midjourney consomme l'équivalent en eau d'une courte douche, comment devrions-nous penser à notre utilisation quotidienne de ces outils ? Nous ne proposons pas de réponses moralisatrices — la technologie n'est ni bonne ni mauvaise en soi —, mais la conscience du coût matériel devrait éclairer nos choix, tant en tant qu'individus qu'en tant que société.

L'intelligence artificielle nous a habitués à penser en termes d'abstractions : modèles, paramètres, jetons, latence. Mais sous ces abstractions palpite une réalité physique faite d'eau qui s'évapore, d'électricité qui circule, de silicium qui chauffe. The Dalles, avec ses tours de refroidissement qui crachent de la vapeur le long du fleuve Columbia, fait autant partie de l'infrastructure de l'IA que les algorithmes d'OpenAI ou les puces de NVIDIA. Il est peut-être temps de cesser de penser au cloud comme à quelque chose d'éthéré, et de commencer à le voir pour ce qu'il est : un réseau matériel avec des conséquences matérielles, qui boit, transpire et, assoiffé, continue de croître.
