---
tags: ["Research", "Generative AI", "Applications"]
date: 2026-01-26
author: "Dario Ferrero"
---

# Repetita Iuvant: Wie die Wiederholung des Prompts die Leistung von LLMs verdoppelt
![prompt-repetition.jpg](prompt-repetition.jpg)

***Repetita iuvant**, sagten die Lateiner. Wiederholte Dinge nützen. Und was wäre, wenn sich diese zweitausend Jahre alte Maxime auch als die effizienteste computergestützte Heuristik für die fortschrittlichsten Sprachmodelle des Jahres 2026 erweisen würde? Das geht aus einem [im Januar von Google Research veröffentlichten Papier](https://arxiv.org/html/2512.14982v1) hervor, in dem drei Forscher, Yaniv Leviathan, Matan Kalman und Yossi Matias, etwas in seiner Einfachheit Verblüffendes entdeckten: Es genügt, denselben Prompt zweimal zu wiederholen, um die Leistung von GPT, Claude, Gemini und Deepseek signifikant zu verbessern. Kein aufwändiges Chain-of-Thought, kein ausgeklügeltes Prompt-Engineering. Wörtlich: kopieren, einfügen.*

Die Technik funktioniert so: Anstatt dem Modell eine Anfrage in der klassischen Form `<QUERY>` zu übermitteln, wird sie in `<QUERY><QUERY>` umgewandelt. Ende. Die Ergebnisse sind jedoch alles andere als trivial. In Tests, die an sieben Spitzenmodellen und ebenso vielen Benchmarks durchgeführt wurden, [gewann die Prompt-Wiederholung 47 von 70 Tests](https://arxiv.org/html/2512.14982v1#S2), ohne eine einzige Niederlage. Bei einigen von den Forschern speziell erstellten Aufgaben sind die Verbesserungen geradezu surreal: Gemini 2.0 Flash-Lite steigert seine Genauigkeit im NameIndex-Benchmark von 21,33 % auf 97,33 %. Ein Sprung von sechsundsiebzig Prozentpunkten, der durch die Verdoppelung des Textes erreicht wird.

## Wenn Vorausschauen bedeutet, nicht zurückzuschauen

Um zu verstehen, warum diese Technik funktioniert, müssen wir einen Schritt zurück in die Architektur großer Sprachmodelle machen. Die überwiegende Mehrheit der modernen LLMs wird als *kausale Sprachmodelle* trainiert, ein Fachbegriff, der eine grundlegende strukturelle Einschränkung verbirgt: Jeder Token kann nur die ihm vorangehenden Token "sehen", niemals die nachfolgenden. Es ist, als würde man ein Buch mit einem beweglichen Fenster lesen, das alles vor dem aktuellen Wort verdeckt.

Dieser Mechanismus der [kausalen Aufmerksamkeit](https://arxiv.org/html/2512.14982v1#S1), so effizient er während des Trainings auch sein mag, führt zu einem subtilen, aber allgegenwärtigen Problem: Die Reihenfolge der Informationen im Prompt ist von entscheidender Bedeutung. Ein konkretes Beispiel hilft, den Punkt zu veranschaulichen. Stellen Sie sich eine Multiple-Choice-Frage vor, die wie folgt aufgebaut ist: Zuerst die Frage, dann die Antwortmöglichkeiten. Wenn das Modell die Optionen liest, hat es die Frage bereits verarbeitet und kann sie kontextualisieren. Wenn Sie jedoch die Reihenfolge umkehren, zuerst die Optionen, dann die Frage, verarbeitet das Modell die Antworten, *bevor* es weiß, was Sie von ihm verlangen. Das Ergebnis? Systematisch schlechtere Leistungen.

Die Google-Forscher testeten beide Konfigurationen an den Benchmarks ARC, OpenBookQA und MMLU-Pro. Mit dem klassischen Frage-zuerst-Format waren die Verbesserungen durch die Prompt-Wiederholung bescheiden. Aber mit dem Optionen-zuerst-Format, bei dem das Modell die Antworten sieht, ohne die Frage bereits zu kennen, waren [die Gewinne erheblich](https://arxiv.org/html/2512.14982v1#S1.F1). Die Wiederholung des Prompts ermöglicht es jedem Token, alle anderen Token des Prompts zu "sehen", wodurch die kausale Einschränkung umgangen wird. Es ist keine echte bidirektionale Aufmerksamkeit, aber es simuliert ihre Effekte.

## Die entwaffnende Lösung

Die Schönheit der Prompt-Wiederholung liegt in ihrer operativen Einfachheit. Sie erfordert keine Änderungen an den Modellen, ändert nicht das Format der Antworten, erhöht nicht die Anzahl der generierten Token oder die wahrgenommene Latenz. Es ist das, was im Fachjargon als *Drop-in-Deployment* bezeichnet wird: Man nimmt sein bestehendes System, fügt eine Codezeile hinzu, die den Prompt dupliziert, und erhält messbare Verbesserungen. Für Endbenutzer ist es noch unmittelbarer: einfach die eigene Frage zweimal kopieren und einfügen.

Die Tests wurden zwischen Februar und März 2025 über die offiziellen APIs von vier Hauptanbietern durchgeführt. Von Google: [Gemini 2.0 Flash und Gemini 2.0 Flash Lite](https://arxiv.org/html/2512.14982v1#bib.bib5). Von OpenAI: [GPT-4o und GPT-4o-mini](https://arxiv.org/html/2512.14982v1#bib.bib12). Von Anthropic: [Claude 3 Haiku und Claude 3.7 Sonnet](https://arxiv.org/html/2512.14982v1#bib.bib1). Und schließlich [Deepseek V3](https://arxiv.org/html/2512.14982v1#bib.bib4). Sieben Modelle unterschiedlicher Größe und Leistungsfähigkeit, alle an etablierten Benchmarks wie GSM8K für Mathematik, MATH für komplexere Probleme und den bereits erwähnten Datensätzen zum Textverständnis getestet.

Zusätzlich zu den Standard-Benchmarks erstellten die Forscher zwei Aufgaben, die speziell darauf ausgelegt waren, die Grenzen der kausalen Aufmerksamkeit aufzuzeigen. Die erste, NameIndex, ist entwaffnend einfach: Man gibt dem Modell eine Liste von fünfzig Namen und bittet es, den fünfundzwanzigsten zurückzugeben. Das scheint trivial, erfordert aber, die Position im Auge zu behalten, während man alle vorhergehenden Namen sequenziell verarbeitet. Die zweite, MiddleMatch, verlangt, den Namen zu identifizieren, der genau zwischen zwei bestimmten Namen in einer Liste von vierzig Elementen mit Wiederholungen steht. Das sind Aufgaben, die ein Mensch in wenigen Sekunden durch Überfliegen lösen würde, die aber für ein kausales Modell eine nicht triviale Rechenherausforderung darstellen.

[Die Ergebnisse dieser Aufgaben](https://arxiv.org/html/2512.14982v1#A1.SS3) zeigen den deutlichsten Unterschied. Bei NameIndex erreicht Gemini Flash-Lite ohne Wiederholung nur 21,33 % korrekte Antworten. Mit einfacher Prompt-Wiederholung: 97,33 %. GPT-4o steigt von 92 % auf 100 %. Claude 3.7 Sonnet von 98,67 % auf 100 %. Das sind keine marginalen Zuwächse, sondern Qualitätssprünge, die unmögliche Aufgaben in gelöste verwandeln.

Die Forscher testeten auch Varianten der Basistechnik. Die *Verbose Prompt Repetition* führt einen Übergangssatz ein: `<QUERY> Lassen Sie mich das wiederholen: <QUERY>`. Die *Prompt Repetition ×3* verdreifacht den Prompt mit zwei Verbindungssätzen. [Beide Varianten](https://arxiv.org/html/2512.14982v1#A1.SS1) erzielen bei den meisten Benchmarks vergleichbare Ergebnisse wie die einfache Wiederholung, mit gelegentlichen weiteren Verbesserungen bei den benutzerdefinierten Aufgaben. Um auszuschließen, dass die Vorteile einfach auf die erhöhte Länge des Prompts zurückzuführen waren, wurde auch eine Kontrollmethode namens *Padding* getestet, die Füllpunkte hinzufügt, um die gleiche Länge wie die Wiederholung zu erreichen. Wie erwartet, führte das Padding zu keiner Verbesserung.

## Unter der Haube

Der Schlüssel zur Effizienz liegt in der Art und Weise, wie Transformer Text verarbeiten. Die Erzeugung einer Antwort gliedert sich in zwei Phasen: den *Prefill*, bei dem das Modell den gesamten Prompt parallel verarbeitet und den KV-Cache aufbaut, und den *Decode*, bei dem es die Token einzeln sequenziell erzeugt. Die Prompt-Wiederholung wirkt sich nur auf den Prefill aus, der bereits parallelisiert und daher extrem schnell ist. Die eigentliche Erzeugung, der langsame Teil, ändert sich überhaupt nicht.

[Empirische Messungen](https://arxiv.org/html/2512.14982v1#S2.SS0.SSS0.Px3) bestätigen dies: keine signifikante Erhöhung der Latenz bei den meisten Modellen. Die Ausnahme sind die Anthropic-Modelle, Claude Haiku und Claude 3.7 Sonnet, wenn sie an sehr langen Prompts wie denen der NameIndex- und MiddleMatch-Aufgaben oder mit der ×3-Variante getestet werden. In diesen Fällen erhöht sich die Latenz, wahrscheinlich weil die Prefill-Phase ins Gewicht zu fallen beginnt. Aber bei Prompts normaler Länge ist der Overhead vernachlässigbar.

Noch interessanter: Die Anzahl der erzeugten Token bleibt identisch. Im Gegensatz zu Techniken wie dem berühmten ["Think step by step"](https://arxiv.org/abs/2205.11916), das 2023 von Kojima vorgeschlagen wurde und das Denken verbessert, aber viel längere Antworten erzeugt, verändert die Prompt-Wiederholung die Ausgabe überhaupt nicht. Das Modell antwortet mit demselben Format, derselben Länge, denselben Worten. Nur die Genauigkeit ändert sich. Dies macht es mit jedem bestehenden System kompatibel, das Antworten in einem bestimmten Format erwartet.

Der Vergleich mit Chain-of-Thought ist aufschlussreich. CoT und seine Varianten zwingen das Modell, seine Überlegungen offenzulegen, was sowohl die erzeugten Token als auch die Latenz drastisch erhöht. Sie funktionieren hervorragend für komplexe Denkaufgaben, haben aber einen erheblichen Rechenaufwand. Die Prompt-Wiederholung besetzt eine andere Nische: [Verständnis-, Klassifizierungs-, direkte Frage-Antwort-Aufgaben](https://arxiv.org/html/2512.14982v1#S1), alles, was kein ausgeklügeltes Denken erfordert, aber wo die Reihenfolge der Informationen Verwirrung stiften kann.

Und tatsächlich, als die Forscher die Prompt-Wiederholung in Kombination mit der Anweisung "think step by step" testeten, waren [die Ergebnisse neutral oder leicht positiv](https://arxiv.org/html/2512.14982v1#A1.SS2): fünf Siege, eine Niederlage, zweiundzwanzig Unentschieden. Das macht Sinn: Wenn das Modell bereits nachdenkt und seinen Prozess explizit macht, wiederholt es wahrscheinlich bereits die relevanten Teile des Prompts in seinen internen Überlegungen. Die Technik wird redundant.
![grafici.jpg](grafici.jpg)
[Bild von arxiv.org](https://arxiv.org/html/2512.14982v1)

## Anwendungen und Grenzen

Das Papier wurde im Januar 2026 veröffentlicht, und die Aufnahme in der Fachwelt war schnell. Auf Reddit, im Subreddit LocalLLaMA, das lokalen Sprachmodellen gewidmet ist, teilten mehrere Benutzer praktische Experimente. Die Ergebnisse bestätigen die Angaben im Papier, wobei einige von bemerkenswerten Verbesserungen bei Klassifizierungs- und Informationsgewinnungsaufgaben berichten. Andere haben besondere Vorteile bei kleineren Modellen festgestellt, solchen unter 10 Milliarden Parametern, bei denen die Prompt-Wiederholung die architektonischen Einschränkungen teilweise zu kompensieren scheint.

Die idealen Anwendungsfälle gehen aus dem Papier und den nachfolgenden Diskussionen recht klar hervor. Textklassifizierung, bei der Kategorien auf der Grundlage von im Prompt verstreuten Informationen zugewiesen werden müssen. Multiple-Choice-Fragen, insbesondere wenn die Optionen lang oder komplex sind. Extraktion spezifischer Informationen aus langen Kontexten. Jede Aufgabe, bei der die Reihenfolge der Informationsdarstellung für ein kausales Modell zu Mehrdeutigkeiten führen könnte.

Die Grenzen sind ebenso klar. Erstens: Es funktioniert nur ohne explizites Denken. Wenn Sie GPT-5 oder Claude Opus verwenden, um komplexe mathematische Probleme zu lösen oder zu programmieren, wird Ihnen die Prompt-Wiederholung wahrscheinlich keine Vorteile bringen. Zweitens: Bei bereits sehr langen Prompts, denken Sie an solche mit 8000-10000 Token, kann die Verdoppelung des Textes zu Latenzproblemen führen, insbesondere bei bestimmten Anbietern. Drittens: [Einige Anthropic-Modelle zeigen Latenzsteigerungen](https://arxiv.org/html/2512.14982v1#S2.SS0.SSS0.Px3) auch bei mäßig langen Prompts, wenn Wiederholung verwendet wird.

Aber vielleicht ist die interessanteste Grenze epistemologischer Natur. Wir wissen immer noch nicht genau, *warum* es so gut funktioniert. Das Papier bietet eine solide mechanistische Erklärung, die pseudo-bidirektionale Aufmerksamkeit, aber die Details, wie die Modelle diese duplizierten Informationen tatsächlich nutzen, bleiben unklar. Die Forscher schlagen als zukünftige Richtung vor, [die Aufmerksamkeitsmuster](https://arxiv.org/html/2512.14982v1#S4) während der Wiederholung zu analysieren, um zu verstehen, welche Teile des duplizierten Prompts wann mehr Gewicht erhalten.

## Eine Genealogie der Wiederholung

Die Prompt-Wiederholung entsteht nicht aus dem Nichts. Sie fügt sich in einen breiteren Forschungsstrang zur strategischen Manipulation von LLM-Eingaben ein. Der historische Bezugspunkt ist das bereits erwähnte Chain-of-Thought-Prompting, das von [Wei und Kollegen 2023](https://arxiv.org/abs/2201.11903) vorgeschlagen wurde und zeigte, wie das explizite Bitten des Modells, schrittweise zu denken, die Leistung bei komplexen Aufgaben drastisch verbessert. Kojima verfeinerte den Ansatz später und zeigte, dass es ausreicht, "Think step by step" hinzuzufügen, um ähnliche Effekte zu erzielen, ohne dass für jede Aufgabe spezifische Beispiele erforderlich sind.

Es gibt aber auch direktere Untersuchungen zur Wiederholung. [Sagi Shaier veröffentlichte im Dezember 2024](https://arxiv.org/abs/2412.07923) eine Studie über die Robustheit von LLMs, wenn die *Fragen* wiederholt werden, nicht der gesamte Prompt, sondern nur der fragende Teil. Seine Ergebnisse zeigen, dass die alleinige Wiederholung der Frage keine signifikanten Verbesserungen bringt, manchmal sogar die Leistung leicht verschlechtert. Dies ist ein interessanter Kontrast zu den Ergebnissen von Google: Offensichtlich ist es wichtig, den *gesamten* Kontext zu wiederholen, nicht nur die Anfrage.

Eine weitere verwandte Forschungsrichtung stammt von [Jacob Springer und Kollegen](https://arxiv.org/abs/2402.15449), die im Februar 2024 zeigten, dass die zweimalige Wiederholung der Eingabe die Qualität von Texteinbettungen verbessert. Einbettungen sind Vektorrepräsentationen von Text, die für Aufgaben der semantischen Ähnlichkeit verwendet werden, und die Tatsache, dass Wiederholung auch dort hilft, deutet darauf hin, dass die Vorteile über die reine Antwortgenerierung hinausgehen.

Noch näher an der Arbeit von Google ist [die Studie von Xiaohan Xu aus dem Jahr 2024](https://arxiv.org/abs/2309.06275), die das *Wiederlesen* untersuchte – das explizite Bitten des Modells, die Frage vor der Antwort erneut zu lesen. Xu fand heraus, dass das Wiederlesen das Denken verbessert, aber mit einem anderen Mechanismus: Das Modell erzeugt tatsächlich eine Wiederholung in seiner Ausgabe, was die erzeugten Token und die Latenz erhöht. Die Prompt-Wiederholung erzielt ähnliche Effekte, indem sie die Kosten in die Prefill-Phase verlagert.

Was sich aus dieser Konstellation von Forschungen abzeichnet, ist ein Muster: Sprachmodelle profitieren davon, dieselben Informationen mehrfach zu verarbeiten, aber *wie* und *wann* diese Wiederverarbeitung stattfindet, macht den ganzen Unterschied. Die Wiederholung im Prompt ist effizient, die Wiederholung in der Ausgabe ist kostspielig, und die Wiederholung nur ausgewählter Teile ist unwirksam.

## Jenseits des Papiers

Die von den Google-Forschern vorgeschlagenen zukünftigen Richtungen sind ehrgeizig. Eine der interessantesten betrifft das Fine-Tuning: Was wäre, wenn wir Modelle speziell mit wiederholten Prompts trainieren würden? Sie könnten lernen, diese Struktur besser zu nutzen, vielleicht durch die Entwicklung optimierter Aufmerksamkeitsmuster. Oder, paradoxerweise, sie könnten lernen, in ihrer Ausgabe *nicht* zu wiederholen, was die Technik noch effizienter machen würde.

Eine weitere Richtung betrifft die Optimierung des KV-Cache. Derzeit werden bei der Wiederholung des Prompts beide Kopien im Cache gespeichert. Technisch würde es jedoch ausreichen, nur die zweite Wiederholung zu behalten, die alle Token "gesehen" hat. [Dies würde die Technik auch für die Generierungsphase völlig neutral machen](https://arxiv.org/html/2512.14982v1#S4) und jeglichen Speicher-Overhead eliminieren.

Dann gibt es noch die Frage der Multimodalität. Moderne Modelle verarbeiten Text, Bilder, Audio. Macht es Sinn, auch nicht-textuelle Eingaben zu wiederholen? Und wenn ja, wie? Die pixelweise Wiederholung eines Bildes scheint nutzlos, aber vielleicht gibt es intelligentere Möglichkeiten, visuelle Informationen zu "wiederholen", damit sich verschiedene Teile eines Bildes besser "sehen" können.

Die radikalste Version der Technik könnte dynamische Wiederholungen während der Generierung selbst beinhalten. Anstatt nur den anfänglichen Prompt zu wiederholen, könnte man auch die bereits erzeugten Token periodisch wiederholen, so dass das Modell seine Ausgabe während der Produktion wiederverarbeiten kann. Das ist spekulativ, wird aber im Papier als Möglichkeit erwähnt.

Auf der praktischen Seite stellt sich die Frage: Nutzt das wirklich jemand in der Produktion? Die Forscher testeten alle wichtigen kommerziellen Modelle, was auf ein Interesse an der realen Anwendbarkeit hindeutet. Und einige Kommentare in Fachforen deuten darauf hin, dass Entwickler mit der Technik in Klassifizierungs- und Sentiment-Analyse-Pipelines experimentieren. Aber eine dokumentierte Massenakzeptanz fehlt noch, wahrscheinlich weil das Papier sehr neu ist.

Eine letzte Überlegung zur Einfachheit. In einem Bereich, der von immer komplexeren Architekturen, Mixture-of-Experts, Retrieval-Augmented Generation und multimodalen Agenten-Frameworks dominiert wird, hat eine Technik, die buchstäblich aus dem Drücken von Strg+C und Strg+V besteht, etwas paradox Revolutionäres. Es ist eine Erinnerung daran, dass Innovation nicht immer aus zusätzlicher Komplexität entsteht, sondern manchmal aus einem tieferen Verständnis bestehender Einschränkungen. Kausale Aufmerksamkeit ist eine seit Jahren bekannte architektonische Einschränkung. Die Prompt-Wiederholung ist einfach der offensichtlichste Weg, sie zu umgehen, wenn man einmal darüber nachdenkt. Wie die minimalistischen Muster von Steve Reich, bei denen die strategische Wiederholung von musikalischen Phrasen emergente Komplexität erzeugt, erzeugt hier die Duplizierung des Textes eine Form des Verständnisses, die das Modell sonst nicht erreichen könnte.

Die Lateiner wussten es bereits: *repetita iuvant*. Die Google-Forscher haben das Konzept nur in eine computergestützte Technik übersetzt. Und es funktioniert.
