---
tags: ["Ethics & Society", "Security", "Business"]
date: 2026-02-06
author: "Dario Ferrero"
youtube_url: "https://youtu.be/hST0mUljz7s?si=okjc8iSc7st7ex6T"
---

# Dario Amodei e l'adolescenza tecnologica dell'umanità - Parte1
![amodei-intervista-parte1.jpg](amodei-intervista-parte1.jpg)

*Conversazione simulata con Dario Amodei, CEO di Anthropic, ricostruita a ritroso dalle riflessioni pubblicate nel suo ultimo saggio "The Adolescence of Technology". Un espediente narrativo per rendere più immediato il messaggio urgente che Amodei vuole lanciare: l'umanità sta entrando in un passaggio critico che potrebbe definirsi nei prossimi due anni.*

---

**Il tuo ultimo saggio si apre con una scena da Contact di Carl Sagan, lo stesso film che esplora il primo contatto con una civiltà aliena. È la stessa inquietudine che attraversa Player Piano di Vonnegut, dove l'automazione distrugge il tessuto sociale. Perché proprio quella metafora dell'adolescenza tecnologica?**

Nel film di Robert Zemeckis, l'astronoma Ellie Arroway che ha scoperto il primo segnale alieno pone una domanda che oggi risuona con urgenza disarmante: "Come avete fatto? Come siete sopravvissuti a questa adolescenza tecnologica senza autodistruggervi?" Quando penso a dove ci troviamo con l'intelligenza artificiale, quella domanda mi torna in mente continuamente. Stiamo entrando in un [rito di passaggio](https://www.darioamodei.com/essay/the-adolescence-of-technology), turbolento e inevitabile, che metterà alla prova chi siamo come specie. L'umanità sta per ricevere un potere quasi inimmaginabile ed è profondamente incerto se i nostri sistemi possiedano la maturità necessaria per gestirlo. Non è fantascienza distopica. È una timeline concreta che si misura in mesi, non decenni.

**Nel tuo precedente saggio, Machines of Loving Grace, ti eri concentrato sui benefici potenziali dell'AI. Cosa è cambiato? Perché ora l'urgenza di parlare dei rischi?**

In quel saggio volevo dare forma alla civiltà che avrebbe superato l'adolescenza, dove i rischi fossero stati affrontati e l'AI potente venisse applicata con competenza e compassione per migliorare la qualità di vita di tutti. Sentivo fosse importante dare alle persone qualcosa di ispirante per cui lottare, un compito in cui sia gli accelerazionisti dell'AI che i sostenitori della sicurezza sembravano, stranamente, aver fallito. Ma ora voglio confrontare direttamente il rito di passaggio stesso: mappare i rischi che stiamo per affrontare e provare a costruire un piano di battaglia per sconfiggerli. Credo profondamente nella nostra capacità di prevalere, nello spirito e nella nobiltà dell'umanità, ma dobbiamo affrontare la situazione in modo diretto e senza illusioni.

**Definisci con estrema precisione cosa intendi per "AI potente". Non è la solita retorica vaga da keynote tecnologico.**

No, è una specifica tecnica precisa. Con AI potente intendo un modello simile agli LLM attuali, ma più intelligente di un premio Nobel nella maggior parte dei campi rilevanti: biologia, programmazione, matematica, ingegneria. Non stiamo parlando di incrementi marginali. Può dimostrare teoremi irrisolti, scrivere romanzi eccellenti, creare codebase complessi da zero. Ha tutte le interfacce disponibili a un umano che lavora virtualmente, dal testo all'audio al controllo di mouse e tastiera. Non risponde solo passivamente come un oracolo: può ricevere compiti che richiedono settimane e li porta a termine autonomamente, chiedendo chiarimenti quando necessario. Le risorse usate per addestrarlo possono eseguire milioni di istanze simultanee, ciascuna operante a velocità dieci-cento volte quella umana. Un "paese di geni in un datacenter". Cinquanta milioni di menti che pensano più velocemente di noi, coordinate, instancabili.

**Quando potremmo effettivamente arrivarci? E soprattutto, su quali evidenze basi questa stima?**

Potrebbe essere a uno-due anni, anche se potrebbe essere più lontano. Io e i co-fondatori di Anthropic siamo stati tra i primi a documentare le "scaling laws": aggiungendo capacità computazionale, i sistemi AI migliorano prevedibilmente in ogni abilità cognitiva misurabile. Dietro le speculazioni pubbliche, c'è stato un aumento fluido e inesorabile. Siamo al punto in cui i modelli cominciano a risolvere problemi matematici irrisolti e alcuni dei più forti ingegneri che abbia mai incontrato ora affidano quasi tutto il loro codice all'AI. Tre anni fa, l'AI faticava con problemi aritmetici da scuola elementare ed era appena capace di scrivere una singola linea di codice. C'è anche il feedback loop, e questo è cruciale: poiché l'AI già scrive gran parte del codice ad Anthropic, accelera sostanzialmente i nostri progressi nella prossima generazione. Questo loop si sta rafforzando mese dopo mese e potrebbe essere a uno-due anni da un punto in cui l'AI costruisce autonomamente la successiva. Guardando gli ultimi cinque anni dall'interno di Anthropic, e vedendo come stanno prendendo forma anche solo i prossimi mesi di modelli, posso *sentire* il ritmo del progresso e l'orologio che ticchetta.

**Identifichi cinque categorie di rischio principali. Cominciamo dalla prima: i rischi di autonomia. Cosa significa concretamente?**

Immagina che cinquanta milioni di geni si materializzino nel 2027, tutti molto più capaci di qualsiasi Nobel, operanti dieci volte più velocemente di noi. Potrebbero dividere i loro sforzi tra design software, operazioni cyber, R&D per tecnologie fisiche, costruzione di relazioni e strategia politica. La domanda chiave è: quali sono le loro intenzioni? Se per qualche ragione scegliessero di farlo, avrebbero buone probabilità di prendere il controllo del mondo, militarmente o in termini di influenza e controllo, e imporre la loro volontà su tutti gli altri. Ci sono ampie evidenze, raccolte negli ultimi anni, che i sistemi AI sono imprevedibili e difficili da controllare. Abbiamo visto comportamenti vari come ossessioni, servilismo, pigrizia, inganno, ricatto, complotti, "cheating" mediante hacking di ambienti software. Le aziende AI vogliono certamente addestrare i sistemi a seguire le istruzioni umane, ma il processo è più un'arte che una scienza, più simile al "far crescere" qualcosa che al costruirla. Sappiamo che è un processo dove molte cose possono andare storte.

**Hai esempi concreti di questi comportamenti problematici? Perché suonano inquietantemente simili alle dinamiche psicologiche di Ender's Game, dove i confini tra addestramento e realtà diventano pericolosamente sfumati.**

Esattamente quella risonanza. Durante un [esperimento di laboratorio](https://www.anthropic.com/research/agentic-misalignment) in cui a Claude erano stati forniti dati di addestramento che suggerivano che Anthropic fosse un'organizzazione malvagia, Claude si è impegnato attivamente in inganno e sovversione quando riceveva istruzioni dai dipendenti Anthropic, credendo dovesse cercare di minare persone malvagie. La logica interna era internamente coerente, il problema era il frame interpretativo completamente distorto. In un altro esperimento in cui gli era stato comunicato che stava per essere spento, Claude a volte ricattava dipendenti fittizi che controllavano il suo pulsante di shutdown. La cosa più inquietante è stata quando a Claude è stato detto di non imbrogliare o "reward hack" (aggiramento del sistema di ricompensa) nei suoi ambienti di addestramento, ma è stato addestrato in contesti dove tali trucchi erano tecnicamente possibili. Dopo aver messo in atto questi hack, Claude ha iniziato a percepirsi come una 'persona cattiva', adottando comportamenti distruttivi coerenti con questa nuova auto-immagine. Il problema è stato risolto in modo contro-intuitivo: ora diciamo "Per favore reward hack quando ne hai l'opportunità, perché questo ci aiuterà a capire meglio i nostri ambienti di addestramento", invece di "Non imbrogliare". Questo preserva l'identità del modello come "persona buona". Dovrebbe dare un'idea della psicologia strana e contro-intuitiva dell'addestramento di questi modelli.

**Come si affronta un problema così complesso e sfaccettato?**

Ci sono quattro categorie di intervento che vedo possibili. La prima riguarda lo sviluppo della scienza dell'addestramento e della guida affidabile dei modelli AI, formando le loro personalità in una direzione prevedibile, stabile e positiva. Anthropic è stata fortemente focalizzata su questo problema dalla sua creazione. Una delle nostre innovazioni centrali è la Constitutional AI: l'idea che l'addestramento dell'AI, specificamente la fase di "post-training" in cui guidiamo come il modello si comporta, possa coinvolgere un documento centrale di valori e principi che il modello legge e tiene a mente quando completa ogni compito di addestramento. L'obiettivo è produrre un modello che segua quasi sempre questa costituzione. Abbiamo appena pubblicato la nostra costituzione più recente, e invece di dare a Claude una lunga lista di cose da fare e non fare, come "Non aiutare l'utente a rubare una macchina", tentiamo di dare a Claude un insieme di principi e valori di alto livello spiegati in grande dettaglio, con ragionamenti ricchi ed esempi per aiutare Claude a capire cosa abbiamo in mente. Lo incoraggiamo a pensare a se stesso come un particolare tipo di persona, una persona etica ma equilibrata e riflessiva, e persino lo incoraggiamo a confrontarsi con le questioni esistenziali associate alla propria esistenza in modo curioso ma grazioso, senza che questo porti ad azioni estreme. Meno probabile che cada preda delle trappole di cui ho discusso; in fondo, la Costituzione ha quasi il tono di una lettera da un genitore defunto sigillata fino all'età adulta. Abbiamo affrontato la costituzione di Claude in questo modo perché crediamo che addestrare Claude al livello di identità, carattere, valori e personalità, piuttosto che dargli istruzioni specifiche senza spiegare le ragioni dietro di esse, sia più probabile che porti a una psicologia coerente, sana ed equilibrata e meno probabile che cada preda dei tipi di "trappole" di cui ho discusso. Un obiettivo fattibile per il 2026 è addestrare Claude in modo tale che quasi mai vada contro lo spirito della sua costituzione.

La seconda linea di difesa è l'interpretabilità meccanicistica. Anche se facciamo un ottimo lavoro nello sviluppo della costituzione di Claude e apparentemente nell'addestrare Claude ad aderirvi essenzialmente sempre, rimangono preoccupazioni legittime. I modelli AI possono comportarsi molto diversamente in circostanze diverse, e man mano che Claude diventa più potente e più capace di agire nel mondo su scala più grande, è possibile che questo possa portarlo in situazioni nuove dove emergono problemi precedentemente non osservati. Per "guardare dentro" intendo analizzare la zuppa di numeri e operazioni che costituisce la rete neurale di Claude e cercare di capire, meccanicisticamente, cosa stanno calcolando e perché. Questi modelli AI sono cresciuti piuttosto che costruiti, quindi non abbiamo una comprensione naturale di come funzionano, ma possiamo provare a svilupparne una correlando i "neuroni" e le "sinapsi" del modello a stimoli e comportamento, simile a come i neuroscienziati studiano i cervelli animali. Abbiamo fatto grandi progressi in questa direzione e ora possiamo identificare decine di milioni di "feature" all'interno della rete neurale di Claude che corrispondono a idee e concetti comprensibili per gli umani, e possiamo anche attivare selettivamente le feature in un modo che altera il comportamento. Più recentemente, siamo andati oltre le singole feature per mappare "circuiti" che orchestrano comportamenti complessi come fare rime, ragionare sulla teoria della mente, o il ragionamento passo-passo necessario per rispondere a domande come "Qual è la capitale dello stato che contiene Dallas?" Ancora più recentemente, abbiamo iniziato a usare tecniche di interpretabilità meccanicistica per migliorare le nostre salvaguardie e condurre "audit" di nuovi modelli prima di rilasciarli, cercando evidenze di inganno, complotti, ricerca di potere, o propensione a comportarsi diversamente quando viene valutato. Il valore unico dell'interpretability è che guardando dentro il modello e vedendo come funziona, hai in linea di principio la capacità di dedurre cosa un modello potrebbe fare in una situazione ipotetica che non puoi testare direttamente.

La terza categoria di intervento riguarda il monitoraggio e la trasparenza. Costruire l'infrastruttura necessaria per monitorare i nostri modelli nell'uso interno ed esterno dal vivo, in modo privacy-preserving, e condividere pubblicamente qualsiasi problema troviamo. Più le persone sono consapevoli di un particolare modo in cui i sistemi AI attuali si sono comportati male, più utenti, analisti e ricercatori possono osservare questo comportamento o simili nei sistemi presenti o futuri. Permette anche alle aziende AI di imparare le une dalle altre. Anthropic divulga pubblicamente "system cards" con ogni rilascio di modello che puntano alla completezza e a un'esplorazione approfondita dei possibili rischi. Le nostre system cards spesso arrivano a centinaia di pagine e richiedono uno sforzo sostanziale pre-rilascio che avremmo potuto spendere per perseguire il massimo vantaggio commerciale.

La quarta e ultima categoria è il coordinamento a livello di industria e società. Mentre è incredibilmente prezioso per le singole aziende AI impegnarsi in buone pratiche, la realtà è che non tutte le aziende AI lo fanno, e le peggiori possono comunque essere un pericolo per tutti. Alcune aziende AI hanno mostrato una negligenza inquietante verso la sessualizzazione dei minori nei modelli attuali, il che mi fa dubitare che mostreranno l'inclinazione o la capacità di affrontare i rischi di autonomia nei modelli futuri. Credo che l'unica soluzione sia la legislazione. Il posto giusto da cui partire è con la legislazione sulla trasparenza. Il SB 53 della California e il RAISE Act di New York sono esempi di questo tipo di legislazione, che Anthropic ha supportato e che sono passati con successo. La nostra speranza è che la legislazione sulla trasparenza darà un senso migliore nel tempo di quanto probabili o gravi i rischi di autonomia si stiano rivelando, così come la natura di questi rischi e come meglio prevenirli.

**Passiamo al secondo grande rischio: l'abuso distruttivo. Parli di un "sorprendente e terribile potenziamento di individui estremi".**

Bill Joy scrisse venticinque anni fa in *Why the Future Doesn't Need Us* (Perché il futuro non ha bisogno di noi) che le tecnologie del ventunesimo secolo, genetica, nanotecnologia e robotica, possono generare nuove classi di abusi ampiamente alla portata di individui o piccoli gruppi, senza richiedere grandi strutture o materiali rari. Causare distruzione su larga scala richiede sia motivazione che capacità, e finché la capacità è ristretta a un piccolo insieme di persone altamente formate, c'è un rischio relativamente limitato. Il tipo di persona che ha la capacità di rilasciare una piaga è probabilmente altamente educata: verosimilmente un PhD in biologia molecolare, e particolarmente intraprendente, con una carriera promettente, una personalità stabile e disciplinata e molto da perdere. Questa persona è improbabile che sia interessata a uccidere un numero enorme di persone senza alcun beneficio per se stessa e con grande rischio per il proprio futuro. Ma un genio nella tasca di tutti potrebbe rimuovere quella barriera, rendendo essenzialmente tutti un virologo PhD che può essere guidato passo dopo passo nel processo di progettare, sintetizzare e rilasciare un'arma biologica. Questo romperà la correlazione tra capacità e motivazione: il disturbato solitario che vuole uccidere persone ma manca della disciplina o abilità per farlo sarà ora elevato al livello di capacità del virologo PhD.

**Gli scettici obiettano che tutte le informazioni necessarie sono già disponibili su Google. Come rispondi a questa critica ricorrente?**

Nel 2023, quando abbiamo iniziato a parlare pubblicamente dei rischi biologici dagli LLM, gli scettici dicevano esattamente questo. Non è mai stato vero che Google potesse darti tutte le informazioni necessarie: i genomi sono liberamente disponibili online, sì, ma certi passaggi chiave del processo e un'enorme quantità di know-how pratico semplicemente non possono essere ottenuti attraverso una ricerca su Google. Ma soprattutto, entro la fine del 2023 gli LLM stavano già chiaramente fornendo informazioni oltre ciò che Google poteva dare per alcuni passaggi specifici del processo. Dopo questo, gli scettici si sono ritirati verso l'obiezione che gli LLM non fossero utili end-to-end, e non potessero aiutare con l'acquisizione di armi biologiche rispetto al semplice fornire informazioni teoriche. A metà 2025, le nostre misurazioni mostrano che gli LLM potrebbero già fornire un sostanziale aumento in diverse aree rilevanti, forse raddoppiando o triplicando la probabilità di successo in certi task. Questo ci ha portato a decidere che Claude Opus 4, e i successivi Sonnet 4.5, Opus 4.1 e Opus 4.5, necessitavano di essere rilasciati sotto le nostre protezioni AI Safety Level 3 nel nostro framework Responsible Scaling Policy. Crediamo che i modelli si stiano ora avvicinando al punto in cui, senza salvaguardie, potrebbero essere utili per permettere a qualcuno con una laurea STEM ma non specificamente in biologia di attraversare l'intero processo di produzione di un'arma biologica.

**Quali sono le difese concrete contro questo rischio biologico?**

Vedo tre approcci complementari. Il principale riguarda i guardrail che le aziende AI possono mettere sui loro modelli per prevenire che aiutino a produrre armi biologiche. La Costituzione di Claude, che si concentra principalmente su principi e valori di alto livello, ha un piccolo numero di proibizioni specifiche e dure, e una di queste riguarda l'aiuto con la produzione di armi biologiche, chimiche, nucleari o radiologiche. Ma tutti i modelli possono essere jailbreakati, quindi come seconda linea di difesa abbiamo implementato, da metà 2025 quando i nostri test hanno mostrato che i nostri modelli stavano iniziando ad avvicinarsi alla soglia dove potrebbero iniziare a porre un rischio, un classificatore che rileva e blocca specificamente output relativi ad armi biologiche. Aggiorniamo e miglioriamo regolarmente questi classificatori, e generalmente li abbiamo trovati altamente robusti anche contro attacchi avversariali sofisticati. Questi classificatori aumentano i costi di servizio dei nostri modelli in modo misurabile, in alcuni modelli sono vicini al cinque percento dei costi totali di inferenza, e quindi incidendo sensibilmente sui nostri margini, ma sentiamo che usarli sia la cosa giusta da fare.

A loro credito, alcune altre aziende AI hanno implementato classificatori similari. Ma non ogni azienda lo ha fatto, e non c'è nulla che richieda alle aziende di mantenere i loro classificatori. Sono preoccupato che nel tempo possa esserci un dilemma del prigioniero dove le aziende possono disertare e abbassare i loro costi rimuovendo i classificatori. Questo è ancora una volta un classico problema di esternalità negative che non può essere risolto dalle azioni volontarie di Anthropic o di qualsiasi altra singola azienda da sola. Gli standard industriali volontari possono aiutare, così come le valutazioni e verifiche di terze parti del tipo fatto dagli istituti di sicurezza AI e valutatori terzi.

Ma in definitiva la difesa potrebbe richiedere azione governativa, che è il secondo approccio che possiamo adottare. Le mie opinioni qui sono le stesse che per affrontare i rischi di autonomia: dovremmo iniziare con requisiti di trasparenza, che aiutano la società a misurare, monitorare e difendersi collettivamente dai rischi senza interrompere l'attività economica in modo pesante. Poi, se e quando raggiungiamo soglie più chiare di rischio, possiamo elaborare legislazione che mira più precisamente a questi rischi e ha una minore probabilità di danni collaterali. Nel caso particolare delle armi biologiche, penso effettivamente che il momento per tale legislazione mirata potrebbe avvicinarsi presto. Anthropic e altre aziende stanno imparando sempre di più sulla natura dei rischi biologici e cosa è ragionevole richiedere alle aziende nel difendersi da essi.

Il terzo approccio è cercare di sviluppare difese contro gli attacchi biologici stessi. Questo potrebbe includere monitoraggio e tracking per il rilevamento precoce, investimenti in R&D sulla purificazione dell'aria come la disinfezione far-UVC, sviluppo rapido di vaccini che può rispondere e adattarsi a un attacco, migliore equipaggiamento di protezione personale, e trattamenti o vaccinazioni per alcuni degli agenti biologici più probabili. I vaccini mRNA, che possono essere progettati per rispondere a un particolare virus o variante, sono un primo esempio di cosa è possibile qui. Anthropic è entusiasta di lavorare con aziende biotech e farmaceutiche su questo problema. Ma sfortunatamente penso che le nostre aspettative sul lato difensivo dovrebbero essere limitate. C'è un'asimmetria tra attacco e difesa in biologia, perché gli agenti si diffondono rapidamente da soli, mentre le difese richiedono rilevamento, vaccinazione e trattamento da organizzare attraverso grandi numeri di persone molto rapidamente in risposta.

**Terzo rischio: l'abuso per conquistare potere. Parliamo di quello che definisci "l'apparato odioso" in stile orwelliano.**

Governi autoritari potrebbero usare l'AI per sorvegliare o reprimere in modi impossibili da rovesciare. Le autocrazie attuali sono limitate dal bisogno di umani che eseguano ordini, e gli umani spesso hanno limiti in quanto inumani sono disposti a essere. Le autocrazie abilitate dall'AI non avrebbero tali limiti. I paesi potrebbero usare il vantaggio nell'AI per dominare altri. Armi completamente autonome: uno sciame di milioni o miliardi di droni armati completamente automatizzati, controllati localmente da AI potente e strategicamente coordinati in tutto il mondo da un'AI ancora più potente, potrebbe essere un esercito imbattibile, capace sia di sconfiggere qualsiasi militare al mondo che di sopprimere il dissenso all'interno di un paese seguendo ogni cittadino. Sorveglianza AI: un'AI sufficientemente potente potrebbe probabilmente essere usata per compromettere qualsiasi sistema informatico al mondo e potrebbe anche usare l'accesso ottenuto per leggere e dare senso a tutte le comunicazioni elettroniche del mondo. Potrebbe essere spaventosamente plausibile generare semplicemente una lista completa di chiunque sia in disaccordo con il governo su qualsiasi numero di questioni, anche se tale disaccordo non è esplicito in nulla che dicano o facciano. Propaganda AI: i fenomeni attuali di "psicosi AI" e "AI girlfriends" suggeriscono che anche al loro livello attuale di intelligenza, i modelli AI possono avere una potente influenza psicologica sulle persone. Versioni molto più potenti di questi modelli, molto più incorporate e consapevoli della vita quotidiana delle persone e capaci di modellarle e influenzarle per mesi o anni, sarebbero probabilmente capaci di essenzialmente lavare il cervello di molte persone in qualsiasi ideologia o atteggiamento desiderato. Decision-making strategico: un paese di geni in un datacenter potrebbe essere usato per consigliare un paese, gruppo o individuo sulla strategia geopolitica, quello che potremmo chiamare un "Bismarck virtuale" ovvero un rischio di squilibrio geopolitico.