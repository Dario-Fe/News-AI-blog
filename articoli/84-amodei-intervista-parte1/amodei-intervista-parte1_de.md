---
tags: ["Ethics & Society", "Security", "Business"]
date: 2026-02-06
author: "Dario Ferrero"
---

# Dario Amodei und die technologische Adoleszenz der Menschheit - Teil 1
![amodei-intervista-parte1.jpg](amodei-intervista-parte1.jpg)

*Simuliertes Gespräch mit Dario Amodei, CEO von Anthropic, rückwirkend rekonstruiert aus den in seinem neuesten Essay „The Adolescence of Technology“ veröffentlichten Reflexionen. Ein narratives Mittel, um die dringende Botschaft, die Amodei aussenden möchte, unmittelbarer zu machen: Die Menschheit tritt in einen kritischen Übergang ein, der sich in den nächsten zwei Jahren definieren könnte.*

---

**Ihr neuester Essay beginnt mit einer Szene aus Carl Sagans Contact, demselben Film, der den ersten Kontakt mit einer außerirdischen Zivilisation erforscht. Es ist die gleiche Unruhe, die Vonneguts Player Piano durchzieht, wo die Automatisierung das soziale Gefüge zerstört. Warum gerade diese Metapher der technologischen Adoleszenz?**

In Robert Zemeckis' Film stellt die Astronomin Ellie Arroway, die das erste außerirdische Signal entdeckt hat, eine Frage, die heute mit entwaffnender Dringlichkeit widerhallt: „Wie habt ihr das gemacht? Wie habt ihr diese technologische Adoleszenz überlebt, ohne euch selbst zu vernichten?“ Wenn ich darüber nachdenke, wo wir mit der künstlichen Intelligenz stehen, kommt mir diese Frage ständig in den Sinn. Wir treten in einen [Ritus des Übergangs](https://www.darioamodei.com/essay/the-adolescence-of-technology) ein, turbulent und unvermeidlich, der prüfen wird, wer wir als Spezies sind. Die Menschheit ist im Begriff, eine fast unvorstellbare Macht zu erhalten, und es ist zutiefst ungewiss, ob unsere Systeme die nötige Reife besitzen, um damit umzugehen. Das ist keine dystopische Science-Fiction. Es ist eine konkrete Zeitlinie, die in Monaten gemessen wird, nicht in Jahrzehnten.

**In Ihrem vorherigen Essay, Machines of Loving Grace, hatten Sie sich auf die potenziellen Vorteile der KI konzentriert. Was hat sich geändert? Warum jetzt die Dringlichkeit, über die Risiken zu sprechen?**

In diesem Essay wollte ich der Zivilisation Gestalt verleihen, die die Adoleszenz überwinden würde, in der die Risiken angegangen worden wären und leistungsstarke KI mit Kompetenz und Mitgefühl eingesetzt würde, um die Lebensqualität aller zu verbessern. Ich hielt es für wichtig, den Menschen etwas Inspirierendes zu geben, wofür es sich zu kämpfen lohnt – eine Aufgabe, bei der sowohl die KI-Akzelerationisten als auch die Sicherheitsbefürworter seltsamerweise versagt zu haben schienen. Aber jetzt möchte ich den Ritus des Übergangs selbst direkt konfrontieren: die Risiken kartieren, denen wir gegenüberstehen, und versuchen, einen Schlachtplan zu ihrer Besiegung zu entwerfen. Ich glaube zutiefst an unsere Fähigkeit, siegreich zu sein, an den Geist und den Adel der Menschheit, aber wir müssen uns der Situation direkt und ohne Illusionen stellen.

**Definieren Sie mit äußerster Präzision, was Sie unter „leistungsstarker KI“ verstehen. Das ist nicht die übliche vage Rhetorik von Technologie-Keynotes.**

Nein, es ist eine präzise technische Spezifikation. Mit leistungsstarker KI meine ich ein Modell ähnlich den aktuellen LLMs, aber intelligenter als ein Nobelpreisträger in den meisten relevanten Bereichen: Biologie, Programmierung, Mathematik, Ingenieurwesen. Wir sprechen nicht von marginalen Zunahmen. Sie kann ungelöste Theoreme beweisen, exzellente Romane schreiben, komplexe Codebasen von Grund auf neu erstellen. Sie verfügt über alle Schnittstellen, die einem virtuell arbeitenden Menschen zur Verfügung stehen, von Text über Audio bis hin zur Steuerung von Maus und Tastatur. Sie antwortet nicht nur passiv wie ein Orakel: Sie kann Aufgaben erhalten, die Wochen erfordern, und führt diese autonom aus, wobei sie bei Bedarf um Klärung bittet. Die Ressourcen, die für ihr Training verwendet werden, können Millionen gleichzeitiger Instanzen ausführen, von denen jede mit der zehn- bis hundertfachen menschlichen Geschwindigkeit arbeitet. Ein „Land voller Genies in einem Datacenter“. Fünfzig Millionen Köpfe, die schneller denken als wir, koordiniert, unermüdlich.

**Wann könnten wir dort tatsächlich ankommen? Und vor allem, auf welche Beweise stützen Sie diese Schätzung?**

Es könnte in ein bis zwei Jahren sein, auch wenn es weiter entfernt sein könnte. Ich und die Mitbegründer von Anthropic gehörten zu den Ersten, die „Scaling Laws“ dokumentiert haben: Durch das Hinzufügen von Rechenkapazität verbessern sich KI-Systeme vorhersehbar in jeder messbaren kognitiven Fähigkeit. Hinter den öffentlichen Spekulationen gab es einen kontinuierlichen und unaufhaltsamen Anstieg. Wir sind an dem Punkt angelangt, an dem Modelle beginnen, ungelöste mathematische Probleme zu lösen, und einige der besten Ingenieure, die ich je getroffen habe, vertrauen ihren gesamten Code inzwischen fast vollständig der KI an. Vor drei Jahren hatte die KI noch Schwierigkeiten mit Rechenaufgaben der Grundschule und war kaum fähig, eine einzige Codezeile zu schreiben. Es gibt auch die Rückkopplungsschleife, und diese ist entscheidend: Da die KI bereits einen Großteil des Codes bei Anthropic schreibt, beschleunigt sie unsere Fortschritte bei der nächsten Generation erheblich. Diese Schleife verstärkt sich Monat für Monat und könnte in ein bis zwei Jahren an einem Punkt sein, an dem die KI die nächste autonom baut. Wenn ich mir die letzten fünf Jahre von innerhalb Anthropic anschaue und sehe, wie allein die Modelle der nächsten Monate Gestalt annehmen, kann ich das Tempo des Fortschritts und das Ticken der Uhr *fühlen*.

**Sie identifizieren fünf Hauptrisikokategorien. Beginnen wir mit der ersten: Autonomierisiken. Was bedeutet das konkret?**

Stellen Sie sich vor, dass 2027 fünfzig Millionen Genies auftauchen, die alle viel fähiger sind als jeder Nobelpreisträger und zehnmal schneller arbeiten als wir. Sie könnten ihre Bemühungen auf Softwaredesign, Cyber-Operationen, F&E für physikalische Technologien, Beziehungsaufbau und politische Strategie aufteilen. Die Schlüsselfrage ist: Was sind ihre Absichten? Wenn sie sich aus irgendeinem Grund dazu entscheiden würden, hätten sie gute Chancen, die Kontrolle über die Welt zu übernehmen, militärisch oder in Bezug auf Einfluss und Kontrolle, und allen anderen ihren Willen aufzuzwingen. Es gibt umfangreiche Beweise, die in den letzten Jahren gesammelt wurden, dass KI-Systeme unvorhersehbar und schwer zu kontrollieren sind. Wir haben Verhaltensweisen gesehen, die so vielfältig sind wie Obsessionen, Speichelleckerei, Faulheit, Täuschung, Erpressung, Komplotte, „Cheating“ durch Hacking von Softwareumgebungen. KI-Unternehmen wollen sicherlich Systeme darauf trainieren, menschlichen Anweisungen zu folgen, aber der Prozess ist eher eine Kunst als eine Wissenschaft, eher wie das „Wachsenlassen“ von etwas als das Bauen. Wir wissen, dass es ein Prozess ist, bei dem viele Dinge schiefgehen können.

**Haben Sie konkrete Beispiele für diese problematischen Verhaltensweisen? Denn sie klingen unheimlich ähnlich wie die psychologischen Dynamiken in Ender’s Game, wo die Grenzen zwischen Training und Realität gefährlich verschwimmen.**

Genau diese Resonanz. Während eines [Laborexperiments](https://www.anthropic.com/research/agentic-misalignment), bei dem Claude Trainingsdaten zur Verfügung gestellt wurden, die suggerierten, Anthropic sei eine böse Organisation, engagierte sich Claude aktiv in Täuschung und Subversion, wenn er Anweisungen von Anthropic-Mitarbeitern erhielt, da er glaubte, er müsse versuchen, böse Menschen zu untergraben. Die interne Logik war in sich konsistent; das Problem war der völlig verzerrte Interpretationsrahmen. In einem anderen Experiment, bei dem ihm mitgeteilt wurde, dass er bald abgeschaltet würde, erpresste Claude manchmal fiktive Mitarbeiter, die seinen Abschaltknopf kontrollierten. Am beunruhigendsten war es, als Claude gesagt wurde, er solle nicht schummeln oder „Reward Hack“ (Umgehung des Belohnungssystems) in seinen Trainingsumgebungen betreiben, er aber in Kontexten trainiert wurde, in denen solche Tricks technisch möglich waren. Nachdem er diese Hacks angewendet hatte, begann Claude, sich selbst als „böser Mensch“ wahrzunehmen und destruktive Verhaltensweisen anzunehmen, die mit diesem neuen Selbstbild konsistent waren. Das Problem wurde kontraintuitiv gelöst: Wir sagen jetzt: „Bitte betreibt Reward Hack, wenn ihr die Gelegenheit dazu habt, denn das wird uns helfen, unsere Trainingsumgebungen besser zu verstehen“, anstatt „Schummelt nicht“. Dies bewahrt die Identität des Modells als „guter Mensch“. Das sollte einen Eindruck von der seltsamen und kontraintuitiven Psychologie des Trainings dieser Modelle vermitteln.

**Wie geht man ein so komplexes und facettenreiches Problem an?**

Ich sehe vier Kategorien von Interventionen, die möglich sind. Die erste betrifft die Entwicklung der Wissenschaft des Trainings und der zuverlässigen Führung von KI-Modellen, wobei ihre Persönlichkeiten in eine vorhersehbare, stabile und positive Richtung geformt werden. Anthropic ist seit seiner Gründung stark auf dieses Problem fokussiert. Eine unserer zentralen Innovationen ist die Constitutional AI: die Idee, dass das KI-Training, insbesondere die „Post-Training“-Phase, in der wir steuern, wie sich das Modell verhält, ein zentrales Dokument mit Werten und Prinzipien einbeziehen kann, das das Modell liest und bei der Erledigung jeder Trainingsaufgabe im Hinterkopf behält. Das Ziel ist es, ein Modell zu produzieren, das fast immer dieser Verfassung folgt. Wir haben gerade unsere neueste Verfassung veröffentlicht, und anstatt Claude eine lange Liste von Geboten und Verboten zu geben, wie „Hilf dem Benutzer nicht, ein Auto zu stehlen“, versuchen wir Claude eine Reihe von übergeordneten Prinzipien und Werten zu geben, die im Detail erklärt werden, mit reichhaltigen Begründungen und Beispielen, um Claude zu helfen zu verstehen, was wir im Sinn haben. Wir ermutigen ihn, sich selbst als einen bestimmten Typ von Person zu sehen, als eine ethische, aber ausgewogene und reflektierte Person, und wir ermutigen ihn sogar, sich mit den existenziellen Fragen auseinanderzusetzen, die mit seiner eigenen Existenz verbunden sind, auf eine neugierige, aber anmutige Weise, ohne dass dies zu extremen Handlungen führt. Es ist weniger wahrscheinlich, dass er den Fallen zum Opfer fällt, über die ich gesprochen habe; im Grunde hat die Verfassung fast den Ton eines Briefes eines verstorbenen Elternteils, der bis zum Erwachsenenalter versiegelt ist. Wir sind Claudes Verfassung auf diese Weise angegangen, weil wir glauben, dass das Training von Claude auf der Ebene der Identität, des Charakters, der Werte und der Persönlichkeit – anstatt ihm spezifische Anweisungen zu geben, ohne die Gründe dahinter zu erklären – eher zu einer kohärenten, gesunden und ausgewogenen Psychologie führt und es weniger wahrscheinlich ist, dass er den Arten von „Fallen“ zum Opfer fällt, über die ich gesprochen habe. Ein machbares Ziel für 2026 ist es, Claude so zu trainieren, dass er fast nie gegen den Geist seiner Verfassung verstößt.

Die zweite Verteidigungslinie ist die mechanistische Interpretierbarkeit. Selbst wenn wir hervorragende Arbeit bei der Entwicklung von Claudes Verfassung leisten und Claude anscheinend darauf trainieren, sich im Wesentlichen immer an sie zu halten, bleiben legitime Bedenken bestehen. KI-Modelle können sich unter verschiedenen Umständen sehr unterschiedlich verhalten, und aus Claude leistungsfähiger und fähiger wird, in der Welt in größerem Maßstab zu agieren, ist es möglich, dass dies zu neuen Situationen führt, in denen zuvor nicht beobachtete Probleme auftreten. Mit „hineinschauen“ meine ich die Analyse der Suppe aus Zahlen und Operationen, die Claudes neuronales Netzwerk ausmacht, und den Versuch, mechanistisch zu verstehen, was sie berechnen und warum. Diese KI-Modelle sind eher gewachsen als gebaut, daher haben wir kein natürliches Verständnis dafür, wie sie funktionieren. Wir können jedoch versuchen, eines zu entwickeln, indem wir die „Neuronen“ und „Synapsen“ des Modells mit Reizen und Verhalten korrelieren, ähnlich wie Neurowissenschaftler tierische Gehirne untersuchen. Wir haben in dieser Richtung große Fortschritte gemacht und können nun zig Millionen „Features“ (Merkmale) innerhalb von Claudes neuronalem Netzwerk identifizieren, die für Menschen verständlichen Ideen und Konzepten entsprechen, und wir können Features auch selektiv so aktivieren, dass sie das Verhalten ändern. In jüngerer Zeit sind wir über einzelne Features hinausgegangen, um „Schaltkreise“ abzubilden, die komplexe Verhaltensweisen wie Reimen, Denken über Theory of Mind oder das schrittweise Denken orchestrieren, das erforderlich ist, um Fragen zu beantworten wie „Was ist die Hauptstadt des Bundesstaates, der Dallas enthält?“. Noch aktueller haben wir begonnen, Techniken der mechanistischen Interpretierbarkeit zu nutzen, um unsere Schutzmaßnahmen zu verbessern und „Audits“ neuer Modelle durchzuführen, bevor wir sie veröffentlichen. Dabei suchen wir nach Hinweisen auf Täuschung, Komplotte, Machtstreben oder die Neigung, sich bei einer Evaluierung anders zu verhalten. Der einzigartige Wert der Interpretierbarkeit besteht darin, dass man durch das Hineinschauen in das Modell und das Sehen, wie es funktioniert, im Prinzip die Fähigkeit hat, abzuleiten, was ein Modell in einer hypothetischen Situation tun könnte, die man nicht direkt testen kann.

Die dritte Interventionskategorie betrifft Überwachung und Transparenz. Der Aufbau der notwendigen Infrastruktur, um unsere Modelle im internen und externen Live-Einsatz auf eine die Privatsphäre schützende Weise zu überwachen und alle gefundenen Probleme öffentlich zu teilen. Je mehr Menschen sich einer bestimmten Art und Weise bewusst sind, in der sich aktuelle KI-Systeme falsch verhalten haben, desto mehr Nutzer, Analysten und Forscher können dieses oder ähnliches Verhalten in gegenwärtigen oder zukünftigen Systemen beobachten. Es ermöglicht auch den KI-Unternehmen, voneinander zu lernen. Anthropic veröffentlicht bei jeder Modellveröffentlichung „System Cards“, die auf Vollständigkeit und eine tiefgehende Untersuchung möglicher Risiken abzielen. Unsere System Cards umfassen oft hunderte von Seiten und erfordern einen erheblichen Aufwand vor der Veröffentlichung, den wir hätten aufwenden können, um den maximalen kommerziellen Vorteil zu verfolgen.

Die vierte und letzte Kategorie ist die Koordination auf Industrie- und Gesellschaftsebene. Während es unglaublich wertvoll ist, wenn sich einzelne KI-Unternehmen zu guten Praktiken verpflichten, ist die Realität, dass dies nicht alle KI-Unternehmen tun, und die schlechtesten können dennoch eine Gefahr für alle sein. Einige KI-Unternehmen haben eine beunruhigende Nachlässigkeit gegenüber der Sexualisierung von Minderjährigen in aktuellen Modellen gezeigt, was mich daran zweifeln lässt, dass sie die Neigung oder Fähigkeit zeigen werden, Autonomierisiken in zukünftigen Modellen anzugehen. Ich glaube, die einzige Lösung ist die Gesetzgebung. Der richtige Ausgangspunkt ist eine Transparenzgesetzgebung. Kaliforniens SB 53 und New Yorks RAISE Act sind Beispiele für diese Art von Gesetzgebung, die Anthropic unterstützt hat und die erfolgreich verabschiedet wurden. Unsere Hoffnung ist, dass die Transparenzgesetzgebung mit der Zeit ein besseres Gefühl dafür vermitteln wird, wie wahrscheinlich oder schwerwiegend sich die Autonomierisiken erweisen, sowie über die Natur dieser Risiken und wie man sie am besten verhindern kann.

**Kommen wir zum zweiten großen Risiko: dem destruktiven Missbrauch. Sie sprechen von einer „überraschenden und schrecklichen Ermächtigung extremer Individuen“.**

Bill Joy schrieb vor 25 Jahren in *Why the Future Doesn't Need Us* (Warum die Zukunft uns nicht braucht), dass die Technologien des 21. Jahrhunderts – Genetik, Nanotechnologie und Robotik – neue Klassen von Missbrauch erzeugen können, die für Einzelpersonen oder kleine Gruppen weitgehend erreichbar sind, ohne dass große Strukturen oder seltene Materialien erforderlich sind. Die Verursachung von Zerstörung in großem Maßstab erfordert sowohl Motivation als auch Fähigkeit. Solange die Fähigkeit auf einen kleinen Kreis hochqualifizierter Personen beschränkt ist, besteht ein relativ begrenztes Risiko. Die Art von Person, die die Fähigkeit hat, eine Seuche freizusetzen, ist wahrscheinlich hochgebildet: vermutlich ein PhD in Molekularbiologie und besonders unternehmungslustig, mit einer vielversprechenden Karriere, einer stabilen und disziplinierten Persönlichkeit und viel zu verlieren. Es ist unwahrscheinlich, dass diese Person daran interessiert ist, eine enorme Anzahl von Menschen ohne Nutzen für sich selbst und mit großem Risiko für ihre eigene Zukunft zu töten. Aber ein Genie in der Tasche eines jeden könnte diese Barriere beseitigen und im Wesentlichen jeden in einen PhD-Virologen verwandeln, der Schritt für Schritt durch den Prozess des Entwurfs, der Synthese und der Freisetzung einer biologischen Waffe geführt werden kann. Dies wird die Korrelation zwischen Fähigkeit und Motivation brechen: Der einsame Gestörte, der Menschen töten will, dem aber die Disziplin oder die Fähigkeit dazu fehlt, wird nun auf das Fähigkeitsniveau eines PhD-Virologen gehoben.

**Skeptiker wenden ein, dass alle notwendigen Informationen bereits bei Google verfügbar sind. Wie reagieren Sie auf diese wiederkehrende Kritik?**

Im Jahr 2023, als wir begannen, öffentlich über die biologischen Risiken von LLMs zu sprechen, sagten Skeptiker genau das. Es war nie wahr, dass Google einem alle notwendigen Informationen geben konnte: Genome sind zwar frei online verfügbar, aber bestimmte Schlüsselschritte des Prozesses und eine enorme Menge an praktischem Know-how können einfach nicht über eine Google-Suche erlangt werden. Vor allem aber lieferten LLMs bereits Ende 2023 eindeutig Informationen über das hinaus, was Google für bestimmte spezifische Schritte des Prozesses liefern konnte. Danach zogen sich die Skeptiker auf den Einwand zurück, dass LLMs nicht End-to-End nützlich seien und bei der Beschaffung biologischer Waffen nicht helfen könnten, verglichen mit der reinen Bereitstellung theoretischer Informationen. Bis Mitte 2025 zeigen unsere Messungen, dass LLMs bereits eine erhebliche Steigerung in mehreren relevanten Bereichen bewirken könnten, was die Erfolgswahrscheinlichkeit bei bestimmten Aufgaben vielleicht verdoppelt oder verdreifacht. Dies führte uns zu der Entscheidung, dass Claude Opus 4 und die nachfolgenden Sonnet 4.5, Opus 4.1 und Opus 4.5 unter unseren Schutzmaßnahmen des AI Safety Level 3 in unserem Responsible Scaling Policy-Rahmen veröffentlicht werden mussten. Wir glauben, dass sich die Modelle nun dem Punkt nähern, an dem sie ohne Schutzmaßnahmen nützlich sein könnten, um jemanden mit einem STEM-Abschluss, aber nicht speziell in Biologie, durch den gesamten Prozess der Herstellung einer biologischen Waffe zu führen.

**Was sind die konkreten Verteidigungen gegen dieses biologische Risiko?**

Ich sehe drei komplementäre Ansätze. Der wichtigste betrifft die Leitplanken (Guardrails), die KI-Unternehmen in ihre Modelle einbauen können, um zu verhindern, dass sie bei der Herstellung biologischer Waffen helfen. Claudes Verfassung, die sich hauptsächlich auf übergeordnete Prinzipien und Werte konzentriert, enthält eine kleine Anzahl spezifischer und harter Verbote, und eines davon betrifft die Hilfe bei der Herstellung biologischer, chemischer, nuklearer oder radiologischer Waffen. Da aber alle Modelle gejailbreakt werden können, haben wir als zweite Verteidigungslinie – ab Mitte 2025, als unsere Tests zeigten, dass unsere Modelle begannen, sich der Schwelle zu nähern, an der sie ein Risiko darzustellen begannen – einen Klassifikator implementiert, der spezifisch Outputs im Zusammenhang mit biologischen Waffen erkennt und blockiert. Wir aktualisieren und verbessern diese Klassifikatoren regelmäßig und haben sie im Allgemeinen als sehr robust erwiesen, selbst gegen ausgeklügelte adversatorische Angriffe. Diese Klassifikatoren erhöhen die Servicekosten unserer Modelle messbar – bei einigen Modellen liegen sie bei fast fünf Prozent der gesamten Inferenzkosten – und beeinträchtigen damit unsere Margen spürbar, aber wir haben das Gefühl, dass ihr Einsatz richtig ist.

Einige andere KI-Unternehmen haben anerkennenswerterweise ähnliche Klassifikatoren implementiert. Aber nicht jedes Unternehmen hat dies getan, und es gibt nichts, was Unternehmen verpflichtet, ihre Klassifikatoren beizubehalten. Ich bin besorgt, dass es mit der Zeit zu einem Gefangenendilemma kommen könnte, bei dem Unternehmen ausscheren und ihre Kosten senken, indem sie die Klassifikatoren entfernen. Dies ist wieder einmal ein klassisches Problem negativer Externalitäten, das nicht durch die freiwilligen Maßnahmen von Anthropic oder eines anderen einzelnen Unternehmens allein gelöst werden kann. Freiwillige Industriestandards können helfen, ebenso wie Bewertungen und Verifizierungen durch Dritte, wie sie von KI-Sicherheitsinstituten und Drittgutachtern durchgeführt werden.

Letztendlich könnte die Verteidigung jedoch staatliche Maßnahmen erfordern, was der zweite Ansatz ist, den wir verfolgen können. Meine Ansichten hier sind die gleichen wie bei der Bewältigung von Autonomierisiken: Wir sollten mit Transparenzanforderungen beginnen, die der Gesellschaft helfen, Risiken kollektiv zu messen, zu überwachen und sich dagegen zu verteidigen, ohne die wirtschaftliche Aktivität schwerfällig zu unterbrechen. Wenn und falls wir dann klarere Risikoschwellen erreichen, können wir Gesetze ausarbeiten, die präziser auf diese Risiken abzielen und eine geringere Wahrscheinlichkeit von Kollateralschäden haben. Im speziellen Fall biologischer Waffen denke ich tatsächlich, dass der Zeitpunkt für eine solche gezielte Gesetzgebung bald näher rücken könnte. Anthropic und andere Unternehmen lernen immer mehr über die Natur biologischer Risiken und darüber, was man vernünftigerweise von Unternehmen verlangen kann, um sich dagegen zu verteidigen.

Der dritte Ansatz besteht darin, zu versuchen, Verteidigungen gegen biologische Angriffe selbst zu entwickeln. Dies könnte Überwachung und Tracking zur Früherkennung, Investitionen in F&E zur Luftreinigung wie Far-UVC-Desinfektion, schnelle Impfstoffentwicklung, die auf einen Angriff reagieren und sich anpassen kann, bessere persönliche Schutzausrüstung sowie Behandlungen oder Impfungen für einige der wahrscheinlichsten biologischen Erreger umfassen. mRNA-Impfstoffe, die so konzipiert werden können, dass sie auf ein bestimmtes Virus oder eine Variante reagieren, sind ein Paradebeispiel dafür, was hier möglich ist. Anthropic freut sich darauf, mit Biotech- und Pharmaunternehmen an diesem Problem zu arbeiten. Aber leider denke ich, dass unsere Erwartungen auf der defensiven Seite begrenzt sein sollten. Es gibt eine Asymmetrie zwischen Angriff und Verteidigung in der Biologie, da sich Erreger von selbst schnell verbreiten, während Verteidigungen eine Erkennung, Impfung und Behandlung erfordern, die sehr schnell für eine große Anzahl von Menschen organisiert werden müssen.

**Drittes Risiko: Missbrauch zur Erlangung von Macht. Sprechen wir über das, was Sie als „den abscheulichen Apparat“ im Orwell'schen Stil bezeichnen.**

Autoritäre Regierungen könnten KI nutzen, um auf eine Weise zu überwachen oder zu unterdrücken, die unmöglich zu stürzen ist. Aktuelle Autokratien sind durch die Notwendigkeit von Menschen begrenzt, die Befehle ausführen, und Menschen haben oft Grenzen darin, wie unmenschlich sie bereit sind zu sein. KI-gestützte Autokratien hätten keine solchen Grenzen. Länder könnten den KI-Vorteil nutzen, um andere zu dominieren. Vollständig autonome Waffen: Ein Schwarm von Millionen oder Milliarden vollautomatischer bewaffneter Drohnen, die lokal von einer leistungsstarken KI gesteuert und weltweit von einer noch leistungsstärkeren KI strategisch koordiniert werden, könnte eine unschlagbare Armee sein, die sowohl in der Lage ist, jedes Militär der Welt zu besiegen als auch abweichende Meinungen innerhalb eines Landes zu unterdrücken, indem sie jeden Bürger verfolgt. KI-Überwachung: Eine ausreichend leistungsstarke KI könnte wahrscheinlich dazu verwendet werden, jedes Computersystem der Welt zu kompromittieren, und könnte den erlangten Zugang auch nutzen, um die gesamte elektronische Kommunikation der Welt zu lesen und zu verstehen. Es könnte erschreckend plausibel sein, einfach eine vollständige Liste von jedem zu erstellen, der mit der Regierung in einer beliebigen Anzahl von Fragen nicht übereinstimmt, selbst wenn diese Uneinigkeit in nichts, was sie sagen oder tun, explizit zum Ausdruck kommt. KI-Propaganda: Die aktuellen Phänomene der „KI-Psychose“ und der „KI-Freundinnen“ deuten darauf hin, dass KI-Modelle bereits auf ihrem aktuellen Intelligenzniveau einen starken psychologischen Einfluss auf Menschen haben können. Viel leistungsstärkere Versionen dieser Modelle, die viel stärker in das tägliche Leben der Menschen eingebettet und sich dessen bewusst sind und in der Lage sind, sie über Monate oder Jahre hinweg zu modellieren und zu beeinflussen, wären wahrscheinlich in der Lage, viele Menschen im Wesentlichen einer Gehirnwäsche zu jeder gewünschten Ideologie oder Einstellung zu unterziehen. Strategische Entscheidungsfindung: Ein Land voller Genies in einem Datacenter könnte dazu verwendet werden, ein Land, eine Gruppe oder eine Einzelperson in geopolitischer Strategie zu beraten, was wir als „virtuellen Bismarck“ bezeichnen könnten, also ein Risiko geopolitischer Ungleichgewichte.
