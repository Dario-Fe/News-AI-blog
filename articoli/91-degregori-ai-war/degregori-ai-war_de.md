---
tags: ["Copyright", "Business", "Ethics & Society"]
date: 2026-02-23
author: "Dario Ferrero"
---

# Was hat De Gregori mit dem KI-Krieg zu tun?
![degregori-ai-war.jpg](degregori-ai-war.jpg)

*Es gibt ein Lied von Francesco De Gregori aus dem Jahr 1992, vom Album „Canzoni d'amore“, an das sich im riesigen und poetischen Katalog des römischen Cantautore vielleicht nur wenige erinnern. Es trägt den Titel „Chi ruba nei supermercati?“ (Wer stiehlt in Supermärkten?), und sein Refrain stellt eine Frage, die damals schrecklich aktuell und soziologisch war: „Auf welcher Seite stehst du? Stehst du auf der Seite derer, die in Supermärkten stehlen? Oder auf der Seite derer, die sie gebaut haben, indem sie stahlen?“ Vierunddreißig Jahre später klingt diese Frage in einem Kontext seltsam aktuell, den De Gregori trotz seiner außergewöhnlichen Fähigkeit, die Welt zu lesen, nicht hätte erahnen können: der technologische Krieg zwischen den größten Unternehmen für künstliche Intelligenz auf dem Planeten.*

## Das Memo und das Erdbeben

Am 12. Februar 2026 sandte OpenAI ein Memorandum an das House Select Committee on Strategic Competition between the United States and the Chinese Communist Party, den überparteilichen Ausschuss des US-Kongresses, der sich dem strategischen Wettbewerb mit China widmet. Der Inhalt dieses Dokuments, [berichtet von Reuters](https://finance.yahoo.com/news/openai-accuses-deepseek-distilling-us-221629899.html) und [Bloomberg](https://www.bloomberg.com/news/articles/2026-02-12/openai-accuses-deepseek-of-distilling-us-models-to-gain-an-edge), ist eine direkte Anschuldigung: Das chinesische Startup DeepSeek habe Techniken der *Distillation* genutzt, um seine eigenen Modelle zu trainieren, indem es die Outputs von ChatGPT ausnutzte. Dabei seien die Sicherheitssysteme von OpenAI bewusst durch Drittanbieter-Router und Techniken der Verschleierung umgangen worden, um die Herkunft der Zugriffe zu verbergen.

„Wir haben Konten beobachtet, die DeepSeek-Mitarbeitern zugeordnet werden konnten und die Methoden entwickelten, um die Zugriffsbeschränkungen von OpenAI zu umgehen“, heißt es im Memo laut Reuters-Bericht, „und wir wissen, dass DeepSeek-Mitarbeiter Code entwickelt haben, um auf US-KI-Modelle zuzugreifen und deren Outputs für die Distillation auf programmatische Weise zu erhalten.“

Um zu verstehen, warum diese Anschuldigung für Aufsehen sorgte, muss man in den Januar 2025 zurückblicken, als DeepSeek das auslöste, was viele Beobachter als den „Sputnik-Moment“ der chinesischen künstlichen Intelligenz bezeichneten. Das Startup aus Hangzhou, gegründet von Liang Wenfeng und ausschließlich von seinem Hedgefonds High-Flyer finanziert, hatte die Modelle [DeepSeek-V3](https://api-docs.deepseek.com/news/news1226) und [DeepSeek-R1](https://api-docs.deepseek.com/news/news250120) veröffentlicht. Diese waren in der Lage, mit den besten amerikanischen Modellen zu einem Bruchteil der angegebenen Kosten zu konkurrieren: weniger als sechs Millionen Dollar an Rechenleistung, gegenüber den Milliarden, die OpenAI, Anthropic, Meta und Google weiterhin in ihre Infrastrukturen investierten. Die Trainingskosten für R1 waren laut [unabhängigen Analysen](https://www.reuters.com/technology/artificial-intelligence/what-is-deepseek-why-is-it-disrupting-ai-sector-2025-01-27/) mit weniger als sechs Millionen Dollar angegeben worden, wobei Nvidia H800-Chips zum Einsatz kamen – also die „abgespeckte“ Version der H100, deren Export in die China die USA bereits verboten hatten.

Die Auswirkungen auf die Märkte waren unmittelbar und brutal: Nvidia verlor innerhalb weniger Tage etwa 600 Milliarden Dollar an Börsenkapitalisierung. Das vorherrschende Narrativ, wonach die Dominanz in der KI notwendigerweise Milliardeninvestitionen in Chips und Rechenzentren erfordere, schien plötzlich zerbrechlich.

## Wie Distillation funktioniert

Bevor wir fortfahren, muss geklärt werden, was genau *Distillation* ist, da der Begriff, wie so oft in der Tech-Kommunikation, sowohl von den Kritikern als auch von den Verteidigern von DeepSeek unpräzise verwendet wird.

Im eigentlichen technischen Sinne ist Distillation ein Prozess, bei dem ein kleineres und leichteres Modell, der „Schüler“, darauf trainiert wird, das Verhalten eines größeren und leistungsstärkeren Modells, des „Lehrers“, zu replizieren. [Wie OpenAI selbst erklärt](https://finance.yahoo.com/news/openai-accuses-deepseek-distilling-us-221629899.html), beinhaltet die Technik im Memo an den Kongress, „dass ein älteres, etablierteres und leistungsfähigeres KI-Modell die Qualität der von einem neueren Modell erzeugten Antworten bewertet und so die Lernergebnisse des älteren Modells effektiv auf das neuere überträgt.“ Konkreter ausgedrückt: Anstatt durch Milliarden von menschlichen Texten von der Welt zu lernen, lernt der Schüler von der Weisheit, die bereits im Lehrer destilliert ist.

Die Technik an sich ist weder neu noch illegal. Sie ist ein Standardwerkzeug auf diesem Gebiet: DeepSeek selbst beschreibt in seinem [technischen Paper zu R1](https://arxiv.org/abs/2501.12948) offen, wie es destillierte Versionen des eigenen Modells erstellt hat, um sie auf weniger leistungsfähiger Hardware zugänglich zu machen. Dabei wurde GRPO (Group Relative Policy Optimization) als Reinforcement-Learning-Framework anstelle des herkömmlicheren RLHF verwendet. Das Paper, unterzeichnet von DeepSeek-AI und 199 Mitautoren, beschreibt einen mehrstufigen Trainingsprozess, der Reinforcement Learning, Supervised Fine-Tuning und eben Distillation *der eigenen Modelle hin zu kleineren Versionen* integriert.

Der Streitpunkt ist also nicht die Technik an sich, sondern ihr Ziel: OpenAI behauptet, DeepSeek habe *Outputs von ChatGPT* destilliert, also die Antworten des Konkurrenzmodells als Trainingsmaterial für das eigene verwendet. Die [Nutzungsbedingungen von OpenAI](https://openai.com/policies/row-terms-of-use/) verbieten ausdrücklich die Nutzung der Outputs ihrer Dienste, „um Modelle zu entwickeln, die mit OpenAI konkurrieren.“

## DeepSeek antwortet nicht. Schweigen als Antwort

Angesichts der Vorwürfe vom Februar 2026 antwortete DeepSeek nicht auf die Kommentaranfragen von Reuters. Dies ist nicht das erste Mal: Bereits im Januar 2025, als erste Gerüchte über Distillation in der *Financial Times* auftauchten, blieb die Antwort des chinesischen Startups ausweichend oder vage.

Schweigen ist bezeichnend, aber nicht eindeutig. Es kann eine rechtliche Strategie sein, kalkulierte Gleichgültigkeit oder einfach die Entscheidung eines Startups, das die Anschuldigungen eines Konkurrenten nicht legitimieren will, indem es auf dessen Terrain antwortet. Was bleibt, ist eine offene Frage: Welche konkreten Beweise hat OpenAI, abgesehen davon, dass die verdächtigen Konten „DeepSeek-Mitarbeitern zugeordnet wurden“?

Aus technischer Sicht ist die Frage alles andere als geklärt. DeepSeek hat die Details seines Trainingsprozesses in einem Peer-Review-Paper auf arXiv veröffentlicht. Wie in der [Analyse von arXiv 2501.12948](https://arxiv.org/abs/2501.12948) dokumentiert, wurde das R1-Zero-Modell ausschließlich durch Reinforcement Learning ohne anfängliches Supervised Fine-Tuning trainiert, ausgehend vom Basismodell DeepSeek-V3. Unabhängige Benchmarks zeigten Leistungen, die bei mathematischen Argumentationsaufgaben und Coding mit OpenAI-o1 vergleichbar waren. Die Tatsache, dass ähnliche Ergebnisse mit unterschiedlichen Architekturen und Methoden zu deutlich geringeren Kosten erreichbar waren, ist Teil des Grundes, warum die Geschichte so viel Aufsehen erregt hatte.

Trotzdem: Die Transparenz eines technischen Papers schließt die parallele Nutzung undokumentierter Techniken nicht aus. Und das Ausbleiben einer Antwort von DeepSeek ist kein Unschuldsbeweis.

## Der rechtliche Bumerang

In Anbetracht all dessen ist der interessanteste und für OpenAI peinlichste Punkt der rechtliche. Wie von den Experten des [Santa Clara Business Law Chronicle](https://www.scbc-law.org/post/code-claims-and-consequences-the-legal-stakes-in-openai-s-case-against-deepseek) detailliert analysiert, befindet sich OpenAI in einer außerordentlich unbequemen prozessualen Lage, sollte es sich entscheiden, rechtlich vorzugehen. Um eine Klage wegen Verletzung geistigen Eigentums zu stützen, müsste es ein Gericht davon überzeugen, dass die Outputs eines KI-Modells Urheberrechtsschutz genießen, also dass die von ChatGPT generierten Antworten ein schützenswerter kreativer Ausdruck sind.

Das Problem ist, dass OpenAI einen Großteil seiner Verteidigung im vom *New York Times* angestrengten Fall genau auf dem gegenteiligen Argument aufgebaut hat: Das Scraping fremder Inhalte zum Trainieren der eigenen Modelle sei „Fair Use“, also eine rechtmäßige Nutzung, die geschütztes Originalmaterial in etwas „Freies“ verwandelt, wenn die Nutzung auf Kritik, Kommentar, Information, Lehre oder Forschung abzielt.

Man kann nicht das Urheberrecht für die eigenen Outputs beanspruchen, nachdem man dasselbe Prinzip den menschlichen Autoren verweigert hat, deren Arbeit diese Outputs erst ermöglicht hat. Diese Strategie ist logisch zirkulär, was Rechtsexperten sofort bemerkt haben. „Es ist, als ob der Schuh des angeeigneten Inhalts am anderen Fuß gelandet wäre“, schrieb *Business Insider* unter Berufung auf Expertenmeinungen, die unmittelbar nach den ersten Anschuldigungen im Januar 2025 eingeholt wurden. OpenAI könnte stattdessen den Weg des Vertragsbruchs versuchen, also eine Verletzung der Nutzungsbedingungen, stößt aber auch hier auf die Schwierigkeit, ein amerikanisches Urteil gegen ein Unternehmen mit Sitz in Hangzhou in einem Rechtssystem zu vollstrecken, mit dem Gegenseitigkeitsabkommen nicht existieren oder mangelhaft sind.

Das Ergebnis, wie die rechtliche Analyse von Santa Clara Law schließt, ist, dass „die Kombination aus spärlichen Präzedenzfällen und geografischen Komplikationen zu dem Schluss führt, dass eine Klage und ein günstiger Ausgang für OpenAI extrem selten und schwer zu erreichen wären.“

## Der Supermarkt und seine Architekten

Und hier wird die Geschichte systemisch komplex. Denn die Anschuldigung von OpenAI gegen DeepSeek kann nicht ohne den Kontext dessen gelesen werden, was OpenAI – und nicht nur sie – getan hat, um ihre eigenen Modelle zu bauen.

Im Dezember 2023 reichte die *New York Times* eine [Klage gegen OpenAI und Microsoft](https://www.npr.org/2025/03/26/nx-s1-5288157/new-york-times-openai-copyright-case-goes-forward) wegen Urheberrechtsverletzung ein und behauptete, Millionen von Artikeln der Zeitung seien ohne Genehmigung oder Entschädigung zum Trainieren von ChatGPT verwendet worden. Im März 2025 lehnte ein Bundesrichter des Southern District of New York, Sidney Stein, den [Antrag von OpenAI auf Abweisung des Falls ab](https://www.npr.org/2025/03/26/nx-s1-5288157/new-york-times-openai-copyright-case-goes-forward) und ließ die wichtigsten Ansprüche zum Prozess zu. Der Richter schränkte einige der Vorwürfe ein, ließ aber den Kern bestehen: Die Frage, ob das massenhafte Scraping von urheberrechtlich geschützten journalistischen Inhalten Fair Use darstellt, ist immer noch gerichtsanhängig.

Dies ist kein Einzelfall in der Landschaft der Klagen im Zusammenhang mit dem Training von KI-Modellen. Im Oktober 2025 reichte Reddit eine [Klage gegen Perplexity AI und drei Data-Scraping-Unternehmen](https://www.cnbc.com/2025/10/23/reddit-user-data-battle-ai-industry-sues-perplexity-scraping-posts-openai-chatgpt-google-gemini-lawsuit.html) – Oxylabs, AWMProxy und SerpApi – ein und beschuldigte sie, Milliarden von Nutzerbeiträgen extrahiert zu haben, indem sie sich hinter Reddit's technischen Schutzmaßnahmen über Google Search-Ergebnisse versteckten. Reddits Chief Legal Officer, Ben Lee, hatte einen besonders treffenden Ausdruck geprägt: „Data Laundering“, also Datenwäsche. „KI-Unternehmen stecken in einem Wettrüsten um hochwertige menschliche Inhalte“, erklärte er, „und dieser Druck hat eine industrielle Wirtschaft des 'Data Laundering' angeheizt.“

Es ist anzumerken, dass Reddit bereits Lizenzvereinbarungen mit Google und mit OpenAI selbst geschlossen hatte: Das Problem im Fall Perplexity war die Beschaffung von Daten über Dritte, ohne zu bezahlen. Doch OpenAI selbst hatte seine Modelle auf Korpora aufgebaut, die nicht lizenzierten Inhalt enthielten: Die [Sammelklagen von Autoren und Schriftstellern](https://cointelegraph.com/news/open-ai-microsoft-accused-stealing-data-train-chat-gpt-artificial-intelligence-lawsuit) wegen der unbefugten Nutzung literarischer Texte während des GPT-Trainings sind eine dokumentierte Spur davon.

Der Mechanismus ist identisch mit dem, den OpenAI DeepSeek vorwirft: Die Nutzung fremder geistiger Arbeit, um ohne Erlaubnis und ohne Bezahlung ein kommerzielles System aufzubauen. Der Unterschied in den Augen von OpenAI ist, dass sie es mit menschlichen Texten getan haben, während DeepSeek es angeblich mit den Outputs eines KI-Modells tat – eine Unterscheidung, die etwas Redundantes hat: Diese KI-Modelle sind das, was sie sind, weil sie unbefugte menschliche Arbeit absorbiert haben.

## Geopolitik, Chips und das Memo an den Kongress

Das OpenAI-Memo an den Kongress ist nicht nur eine technisch-rechtliche Angelegenheit. Es ist ein politischer Akt, gerichtet an den Ausschuss, der den strategischen Wettbewerb mit China überwacht, geschrieben zu einem Zeitpunkt, als die Trump-Regierung ihre Haltung gegenüber Technologieexporten neu definierte.

David Sacks, vom Weißen Haus zum „KI- und Krypto-Zar“ ernannt, hatte bereits im Januar 2025 vorgebeugt und gegenüber Fox News erklärt, dass „es substanzielle Beweise gibt, dass DeepSeek Wissen aus den Modellen von OpenAI destilliert hat.“ Der Kongressabgeordnete John Moolenaar, Vorsitzender des House Select Committee on China, hatte [laut Gigazine](https://gigazine.net/gsc_news/en/20260213-openai-accuses-china-deepseek/) noch schärfere Töne angeschlagen: „Das ist Teil der Strategie der Kommunistischen Partei Chinas: stehlen, kopieren und zerstören.“

OpenAI hatte im Memo auch eine besorgniserregende Anmerkung zur Sicherheit hinzugefügt: Wenn ein Modell durch Distillation repliziert wird, neigen die Sicherheitsmechanismen des Originalmodells dazu, nicht übertragen zu werden, was potenziell eine weniger gefilterte Version auf dem Markt zirkulieren lässt – mit Risiken für sogenannte Hochgefahrenbereiche wie Biologie und Chemie. Das ist ein legitimes Argument. Es ist auch ein Argument, das dazu dient, einem Fall, der auf rein rechtlicher Ebene viel weniger solide ist, düsterere Töne zu verleihen.

Auf der anderen Seite sehen viele asiatische Beobachter die Anschuldigungen von OpenAI als technologischen Protektionismus, der als ethische Frage getarnt wird. DeepSeek hat bewiesen, dass es möglich war, wettbewerbsfähige Modelle mit deutlich geringeren Rechenressourcen zu bauen und damit faktisch den strukturellen Vorteil zu umgehen, den die Exportbeschränkungen für Nvidia-Chips der amerikanischen Industrie hätten garantieren sollen. Wenn Distillation-Vorwürfe zum Vorwand für weitere regulatorische Blockaden würden, ginge es darum, auf eine technische Niederlage mit politischen Instrumenten zu antworten.

## Auf welcher Seite stehst du?

Kehren wir also zu De Gregori zurück und zu der Frage, die er offen gelassen hatte.

Die erzählerische Struktur dieser Angelegenheit ist in ihrer peinlichen Symmetrie fast schon zu perfekt. OpenAI beschuldigt DeepSeek, seine Modelle ohne Erlaubnis verwendet zu haben, um etwas Wettbewerbsfähiges aufzubauen. Aber OpenAI hat diese Modelle unter Verwendung der Arbeit von Journalisten, Schriftstellern, Autoren, Reddit-Programmen, Diskussions-Threads und ganzen Korpora menschlicher geistiger Produktion aufgebaut, ohne um Erlaubnis zu fragen oder zu bezahlen. Die Klage der *New York Times* ist vor amerikanischen Gerichten noch offen. Die Sammelklagen der Schriftsteller und Autoren vervielfachen sich. Reddit zerrt diejenigen vor Gericht, die genau das getan haben, was OpenAI mit den menschlichen Texten getan hatte.

Es ist keine Frage der absoluten Unschuld oder absoluten Schuld. Es ist eine Frage dessen, wer die Regeln des Supermarkts festlegt, wer die Kasse führen darf und wer am Ausgang von den Wachen aufgehalten wird. Die Distillation, die OpenAI DeepSeek vorwirft, ist moralisch und strukturell analog zum Scraping, das OpenAI mit menschlichen Inhalten betrieben hat: Beides sind Techniken, um ohne Entschädigung Wert aus einem fremden Korpus zu ziehen, um mächtige kommerzielle Systeme aufzubauen. Der Hauptunterschied besteht derzeit in der Machtfrage: Wer hat die Ressourcen, um das rechtliche und politische Narrativ zu definieren, und wer nicht.

Das bedeutet nicht, dass die Anschuldigungen von OpenAI falsch sind; sie könnten wahr sein. Es bedeutet nicht, dass die unbefugte Distillation von KI-Modellen keine legitimen Fragen des geistigen Eigentums aufwirft; das tut sie. Es bedeutet lediglich, dass die moralische Haltung des Anklägers durch seine eigene Geschichte untergraben wird. Kann man ein Schloss auf Sand bauen und sich dann beschweren, dass jemand ohne Erlaubnis ein Zelt darauf aufgestellt hat?

Kurz gesagt: Ändern sich Wahrheit und Substanz, wenn man einen Hanfu trägt und in einer kleinen Box in Hangzhou arbeitet, oder wenn man ein farbiges Poloshirt trägt und in einem Open Space in San Francisco arbeitet?

De Gregori stellte 1992, in einem historischen Kontext großer Reibungen und Veränderungen, eine fast zirkuläre Frage, die in ihrer Weite heute im Jahr 2026 hochmodern zurückkehrt. Der Refrain dieses Liedes gibt keine Antworten, er stellt nur eine Frage. *Auf welcher Seite stehst du? Stehst du auf der Seite derer, die in Supermärkten stehlen? Oder auf der Seite derer, die sie gebaut haben, indem sie stahlen?*
