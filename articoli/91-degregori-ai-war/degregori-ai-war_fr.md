---
tags: ["Copyright", "Business", "Ethics & Society"]
date: 2026-02-23
author: "Dario Ferrero"
---

# Quel rapport entre De Gregori et la guerre de l'IA ?
![degregori-ai-war.jpg](degregori-ai-war.jpg)

*Il y a une chanson de Francesco De Gregori datant de 1992, issue de l'album "Canzoni d'amore", que peu de gens se rappellent peut-être dans le vaste et poétique catalogue de l'auteur-compositeur-interprète romain. Elle s'intitule "Chi ruba nei supermercati ?" (Qui vole dans les supermarchés ?), et son refrain pose une question qui, à l'époque, était terriblement actuelle et sociologique : "De quel côté es-tu ? Es-tu du côté de celui qui vole dans les supermarchés ? Ou de celui qui les a construits, en volant ?" Trente-quatre ans plus tard, cette question résonne de manière étrangement actuelle dans un contexte que De Gregori, malgré son extraordinaire capacité à lire le monde, n'aurait pu imaginer : la guerre technologique entre les plus grandes entreprises d'intelligence artificielle de la planète.*

## Le mémo et le tremblement de terre

Le 12 février 2026, OpenAI a envoyé un mémorandum au House Select Committee on Strategic Competition between the United States and the Chinese Communist Party, le comité bicaméral du Congrès américain dédié à la compétition stratégique avec la Chine. Le contenu de ce document, [rapporté par Reuters](https://finance.yahoo.com/news/openai-accuses-deepseek-distilling-us-221629899.html) et par [Bloomberg](https://www.bloomberg.com/news/articles/2026-02-12/openai-accuses-deepseek-of-distilling-us-models-to-gain-an-edge), est une accusation directe : la startup chinoise DeepSeek aurait utilisé des techniques de *distillation* pour entraîner ses propres modèles en exploitant les outputs de ChatGPT, contournant délibérément les systèmes de sécurité d'OpenAI via des routeurs tiers et des techniques d'offuscation pour masquer la provenance des accès.

"Nous avons observé des comptes associés à des employés de DeepSeek développant des méthodes pour contourner les restrictions d'accès d'OpenAI", lit-on dans le mémo selon ce qui est rapporté par Reuters, "et nous savons que des employés de DeepSeek ont développé du code pour accéder aux modèles d'IA américains et en obtenir les outputs pour la distillation de manière programmatique."

Pour comprendre pourquoi cette accusation a fait du bruit, il faut revenir en janvier 2025, lorsque DeepSeek avait déclenché ce que de nombreux observateurs avaient baptisé le "moment Spoutnik" de l'intelligence artificielle chinoise. La startup de Hangzhou, fondée par Liang Wenfeng et financée exclusivement par son hedge fund High-Flyer, avait publié les modèles [DeepSeek-V3](https://api-docs.deepseek.com/news/news1226) et [DeepSeek-R1](https://api-docs.deepseek.com/news/news250120), capables de rivaliser avec les meilleurs modèles américains pour une fraction du coût déclaré : moins de six millions de dollars de puissance de calcul, contre les milliards qu'OpenAI, Anthropic, Meta et Google continuaient d'investir dans leurs propres infrastructures. Le coût d'entraînement de R1, comme documenté par des [analyses indépendantes](https://www.reuters.com/technology/artificial-intelligence/what-is-deepseek-why-is-it-disrupting-ai-sector-2025-01-27/), avait été déclaré à moins de six millions de dollars en employant des puces Nvidia H800, c'est-à-dire la version "déclassée" des H100 que les États-Unis avaient déjà interdit d'exporter vers la Chine.

L'effet sur les marchés avait été immédiat et brutal : Nvidia avait brûlé en quelques jours environ 600 milliards de dollars de capitalisation. Le récit dominant, selon lequel dominer l'IA nécessitait nécessairement des milliards d'investissements en puces et en centres de données, semblait soudain fragile.

## Comment fonctionne la distillation

Avant de poursuivre, il est nécessaire de clarifier ce qu'est exactement la *distillation*, car le terme, comme c'est souvent le cas dans la communication tech, est utilisé de manière imprécise tant par les détracteurs que par les défenseurs de DeepSeek.

Au sens technique le plus propre, la distillation est un processus par lequel un modèle plus petit et plus léger, l'« élève », est entraîné à reproduire le comportement d'un modèle plus grand et plus puissant, le « maître ». [Comme l'explique OpenAI elle-même](https://finance.yahoo.com/news/openai-accuses-deepseek-distilling-us-221629899.html) dans le mémo au Congrès, la technique « implique qu'un modèle d'IA plus ancien, plus consolidé et puissant évalue la qualité des réponses produites par un modèle plus récent, transférant ainsi les résultats de l'apprentissage du modèle le plus ancien vers le plus récent. » En termes plus concrets : au lieu d'apprendre du monde à travers des milliards de textes humains, l'élève apprend de la sagesse déjà distillée dans le maître.

La technique en soi n'est ni nouvelle ni illégale. C'est un outil standard du domaine : DeepSeek lui-même, dans son [papier technique sur R1](https://arxiv.org/abs/2501.12948), décrit ouvertement comment il a créé des versions distillées de son propre modèle pour les rendre accessibles sur du matériel moins puissant, en utilisant GRPO (Group Relative Policy Optimization) comme framework d'apprentissage par renforcement à la place du plus conventionnel RLHF. Le papier, signé par DeepSeek-AI et 199 co-auteurs, décrit un processus d'entraînement multi-étapes qui intègre l'apprentissage par renforcement, le réglage fin supervisé (supervised fine-tuning) et, justement, la distillation *de ses propres modèles vers des versions plus petites*.

Le point de controverse n'est donc pas la technique elle-même, mais son objectif : OpenAI soutient que DeepSeek aurait distillé des *outputs de ChatGPT*, c'est-à-dire aurait utilisé les réponses du modèle concurrent comme matériel d'entraînement pour le sien. Les [Conditions d'utilisation d'OpenAI](https://openai.com/policies/row-terms-of-use/) interdisent explicitement d'utiliser les outputs de ses services « pour développer des modèles qui concurrencent OpenAI ».

## DeepSeek ne répond pas. Le silence comme réponse

Face aux accusations de février 2026, DeepSeek n'a pas répondu aux demandes de commentaires de Reuters. Ce n'est pas la première fois : déjà en janvier 2025, lorsque les premières rumeurs sur la distillation avaient émergé dans le *Financial Times*, la réponse de la startup chinoise était restée élusive ou vague.

Le silence est significatif mais pas univoque. Il peut s'agir d'une stratégie juridique, d'une indifférence calculée, ou simplement du choix d'une startup qui ne veut pas légitimer les accusations d'un concurrent en y répondant sur son terrain. Ce qui reste est une question ouverte : quelles preuves concrètes détient OpenAI, au-delà du fait que les comptes suspects « étaient associés à des employés de DeepSeek » ?

D'un point de vue technique, la question est loin d'être résolue. DeepSeek a publié les détails de son propre processus d'entraînement dans un papier évalué par des pairs sur arXiv. Comme [documenté dans l'analyse de arxiv 2501.12948](https://arxiv.org/abs/2501.12948), le modèle R1-Zero avait été entraîné exclusivement par apprentissage par renforcement sans réglage fin supervisé initial, à partir du modèle de base DeepSeek-V3. Des benchmarks indépendants montraient des performances comparables à OpenAI-o1 sur des tâches de raisonnement mathématique et de codage. Le fait que des résultats similaires soient atteignables avec des architectures et des méthodologies différentes, et à des coûts nettement inférieurs, explique en partie pourquoi l'histoire avait suscité un tel émoi.

Cela dit : la transparence d'un papier technique n'exclut pas l'utilisation parallèle de techniques non documentées. Et l'absence de réponse de DeepSeek n'est pas une preuve d'innocence.

## Le boomerang juridique

À la lumière de tout cela, le nœud le plus intéressant, et le plus embarrassant pour OpenAI, est d'ordre juridique. Comme analysé en détail par les experts du [Santa Clara Business Law Chronicle](https://www.scbc-law.org/post/code-claims-and-consequences-the-legal-stakes-in-openai-s-case-against-deepseek), OpenAI se trouve dans une position procédurale extraordinairement inconfortable si elle décidait d'engager des poursuites judiciaires. Pour soutenir une plainte pour violation de propriété intellectuelle, elle devrait convaincre un tribunal que les outputs d'un modèle d'IA bénéficient de la protection du droit d'auteur, c'est-à-dire que les réponses générées par ChatGPT sont une expression créative protégeable.

Le problème est qu'OpenAI a construit une bonne partie de sa défense dans l'affaire intentée par le *New York Times* précisément sur l'argument opposé : le scraping de contenus tiers pour entraîner ses propres modèles relève du « fair use » (usage loyal), c'est-à-dire un usage légitime qui transforme le matériel protégé et original en quelque chose de « libre » si l'usage est destiné à la critique, au commentaire, à l'information, à l'enseignement ou à la recherche.

On ne peut pas invoquer le droit d'auteur sur ses propres outputs après avoir dénié ce même principe aux auteurs humains dont le travail a rendu ces outputs possibles. Le stratagème est logiquement circulaire, et les experts juridiques l'ont remarqué immédiatement. « C'est comme si la chaussure du contenu approprié s'était retrouvée à l'autre pied », a écrit *Business Insider* en citant des avis d'experts juridiques recueillis juste après les premières accusations de janvier 2025. OpenAI pourrait plutôt tenter la voie de la rupture de contrat (breach of contract), violation des Conditions d'utilisation, mais là aussi, elle se heurte à la difficulté de faire exécuter un jugement américain sur une entreprise basée à Hangzhou, dans un système juridique avec lequel les accords de réciprocité sont inexistants ou défaillants.

Le résultat, comme le conclut l'analyse juridique de la Santa Clara Law, est que « la combinaison de précédents rares et de complications géographiques mène à la conclusion qu'un procès, et une issue favorable, seraient extrêmement rares et difficiles à obtenir pour OpenAI. »

## Le supermarché et ses architectes

Et c'est là que l'histoire se complique de manière systémique. Car l'accusation d'OpenAI contre DeepSeek ne peut être lue sans le contexte de ce qu'OpenAI, et pas seulement elle, a fait pour construire ses propres modèles.

En décembre 2023, le *New York Times* a intenté un [procès contre OpenAI et Microsoft](https://www.npr.org/2025/03/26/nx-s1-5288157/new-york-times-openai-copyright-case-goes-forward) pour violation du droit d'auteur, affirmant que des millions d'articles du journal avaient été utilisés pour entraîner ChatGPT sans autorisation ni compensation. En mars 2025, un juge fédéral du Southern District de New York, Sidney Stein, a [rejeté la demande d'OpenAI de classer l'affaire](https://www.npr.org/2025/03/26/nx-s1-5288157/new-york-times-openai-copyright-case-goes-forward), permettant aux principales revendications de passer au procès. Le juge a restreint certaines accusations mais a laissé subsister l'essentiel : la question de savoir si le scraping massif de contenus journalistiques protégés par le droit d'auteur constitue un usage loyal est toujours sub judice.

Ce n'est pas un cas isolé dans le paysage des procès liés à l'entraînement des modèles d'IA. En octobre 2025, Reddit a déposé une [plainte contre Perplexity AI et trois entreprises de data scraping](https://www.cnbc.com/2025/10/23/reddit-user-data-battle-ai-industry-sues-perplexity-scraping-posts-openai-chatgpt-google-gemini-lawsuit.html), Oxylabs, AWMProxy et SerpApi, les accusant d'avoir extrait des milliards de posts d'utilisateurs en se cachant derrière les protections techniques de Reddit via les résultats de Google Search. Le Chief Legal Officer de Reddit, Ben Lee, avait inventé une expression particulièrement efficace : « data laundering », soit blanchiment de données. « Les entreprises d'IA sont enfermées dans une course aux armements pour des contenus humains de qualité », avait-il déclaré, « et cette pression a alimenté une économie industrielle à l'échelle du 'data laundering'. »

Il convient de noter que Reddit avait déjà conclu des accords de licence avec Google et avec OpenAI elle-même : le problème, dans l'affaire Perplexity, était l'approvisionnement en données via des tiers sans payer. Mais OpenAI elle-même avait construit ses propres modèles sur des corpus incluant des contenus non licenciés : les [recours collectifs (class actions) intentés par des auteurs et des écrivains](https://cointelegraph.com/news/open-ai-microsoft-accused-stealing-data-train-chat-gpt-artificial-intelligence-lawsuit) pour l'utilisation non autorisée de textes littéraires lors de l'entraînement de GPT en sont une trace documentée.

Le mécanisme est identique à celui qu'OpenAI impute à DeepSeek : utiliser le travail intellectuel d'autrui pour construire un système commercial sans permission et sans payer. La différence, aux yeux d'OpenAI, est qu'ils l'ont fait avec des textes humains alors que DeepSeek l'aurait fait avec des outputs d'un modèle d'IA, une distinction qui a quelque chose de redondant : ces modèles d'IA sont ce qu'ils sont parce qu'ils ont absorbé du travail humain non autorisé.

## Géopolitique, puces et le mémo au Congrès

Le mémo d'OpenAI au Congrès n'est pas seulement une question technico-juridique. C'est un acte politique, adressé au comité qui supervise la compétition stratégique avec la Chine, écrit à un moment où l'administration Trump redéfinissait sa posture envers l'exportation de technologies.

David Sacks, nommé « AI and crypto czar » par la Maison Blanche, avait déjà pris les devants en janvier 2025 en déclarant à Fox News qu'« il existe des preuves substantielles que ce que DeepSeek a fait est de distiller les connaissances à partir des modèles d'OpenAI. » Le membre du Congrès John Moolenaar, président du House Select Committee on China, [selon Gigazine](https://gigazine.net/gsc_news/en/20260213-openai-accuses-china-deepseek/), avait utilisé des tons encore plus vifs : « Cela fait partie de la stratégie du Parti Communiste Chinois : voler, copier et détruire. »

OpenAI avait également ajouté, dans le mémo, une note inquiétante sur la sécurité : lorsqu'un modèle est répliqué par distillation, les mécanismes de sécurité du modèle original ont tendance à ne pas être transférés, laissant potentiellement une version moins filtrée circuler sur le marché, avec des risques pour les secteurs dits à haut danger comme la biologie et la chimie. C'est un argument légitime. C'est aussi un argument qui sert à donner des tons plus sombres à une affaire qui, sur le plan strictement juridique, est beaucoup moins solide.

D'un autre côté, la perspective de nombreux observateurs asiatiques cadre les accusations d'OpenAI comme du protectionnisme technologique masqué par des questions éthiques. DeepSeek a prouvé qu'il était possible de construire des modèles compétitifs avec des ressources de calcul nettement inférieures, contournant de fait l'avantage structurel que les restrictions à l'exportation de puces Nvidia auraient dû garantir à l'industrie américaine. Si les accusations de distillation devenaient un prétexte à de nouveaux blocages réglementaires, il s'agirait de répondre à une défaite technique par des outils politiques.

## De quel côté es-tu ?

Revenons donc à De Gregori, et à la question qu'il avait laissée en suspens.

La structure narrative de cette affaire est presque trop parfaite dans sa symétrie embarrassante. OpenAI accuse DeepSeek d'avoir utilisé ses modèles sans permission pour construire quelque chose de compétitif. Mais OpenAI a construit ces modèles en utilisant le travail de journalistes, d'écrivains, d'auteurs, de programmes Reddit, de fils de discussion et de corpus entiers de production intellectuelle humaine sans demander la permission ni payer. Le procès du *New York Times* est toujours en cours devant les tribunaux américains. Les recours collectifs des écrivains et des auteurs se multiplient. Reddit poursuit en justice ceux qui ont fait exactement ce qu'OpenAI avait fait avec les textes humains.

Ce n'est pas une question d'innocence absolue ou de culpabilité absolue. C'est une question de savoir qui fixe les règles du supermarché, qui peut tenir la caisse, et qui est arrêté par les gardes à la sortie. La distillation qu'OpenAI impute à DeepSeek est moralement et structurellement analogue au scraping qu'OpenAI a opéré sur les contenus humains : les deux sont des techniques pour extraire de la valeur d'un corpus tiers sans compensation, utilisées pour construire des systèmes commerciaux puissants. La différence principale, pour l'instant, est une question de pouvoir : qui a les ressources pour définir le récit juridique et politique, et qui ne les a pas.

Cela ne signifie pas que les accusations d'OpenAI soient fausses, elles pourraient être vraies. Cela ne signifie pas que la distillation non autorisée de modèles d'IA ne soulève pas de questions légitimes de propriété intellectuelle, elle en soulève. Cela signifie simplement que la posture morale de celui qui accuse est minée par sa propre histoire. Peut-on construire un château sur le sable et se plaindre ensuite que quelqu'un y ait posé une tente sans demander la permission ?

En somme, la vérité et la substance changent-elles si vous êtes vêtu d'un Hanfu et travaillez dans un cubicule à Hangzhou ou si vous portez un polo coloré et travaillez dans un open space à San Francisco ?

De Gregori, en 92, dans un contexte historique de grandes frictions et de changements, se posait une question presque circulaire, qui dans son ampleur revient très moderne aujourd'hui en 2026. Le refrain de cette chanson ne donne pas de réponses, il ne fait qu'une question. *De quel côté es-tu ? Es-tu du côté de celui qui vole dans les supermarchés ? Ou de celui qui les a construits, en volant ?*
