---
tags: ["Generative AI", "Training", "Business"]
date: 2026-02-25
author: "Dario Ferrero"
---

# GLM-5 : le modèle entraîné sur des puces chinoises
![glm-5.jpg](glm-5.jpg)

*Un modèle de 744 milliards de paramètres, entièrement entraîné sur des puces domestiques Huawei, qui atteint les performances des meilleurs modèles propriétaires américains dans certains des tests les plus pertinents. Tout cela sans un seul processeur NVIDIA. La course à l'autonomie technologique chinoise n'est plus une promesse d'avenir : elle a déjà eu lieu, et GLM-5 en est la preuve la plus éloquente à ce jour.*

Ceux qui suivent ce portail depuis un certain temps savent que nous observons quelque chose de plus grand qu'une simple série de lancements de modèles. Avec [Kimi K2.5](https://aitalk.it/it/kimik2.5.html), avec [DeepSeek MHC](https://aitalk.it/it/deepseek-mhc.html), avec [Kimi K2 Thinking](https://aitalk.it/it/kimi-k2-thinking.html), nous avons raconté les pièces d'une mosaïque qui aujourd'hui, avec l'arrivée de [GLM-5](https://z.ai/blog/glm-5), montre plus clairement sa forme globale. Il ne s'agit pas de modèles exceptionnels isolés. Il s'agit d'un mouvement systématique et coordonné qui, en quelques mois, a fait passer l'intelligence artificielle chinoise open source de la poursuite à la parité, et dans certains cas au dépassement, des laboratoires américains les plus renommés.

GLM-5 a été présenté le 11 février 2026 par [Z.ai](https://z.ai/blog/glm-5), le nom international sous lequel se présente Zhipu AI depuis 2025, une entreprise fondée en 2019 en tant que spin-off de l'Université Tsinghua de Pékin. Le lancement ne pouvait être plus symbolique : à la veille du Nouvel An lunaire, dans ce que certains analystes appellent déjà l'« offensive de la Fête du Printemps » de l'IA chinoise. Une coïncidence ? Difficile à croire.

## Dans le moteur : ingénierie d'échelle

Pour comprendre ce que représente GLM-5 d'un point de vue technique, il est utile de partir d'une analogie. Imaginez un grand studio d'enregistrement. Les anciens modèles étaient comme des orchestres de taille standard, où tous les musiciens jouent simultanément. Les modèles à « mélange d'experts » (MoE, Mixture of Experts), comme GLM-5, fonctionnent au contraire comme un orchestre immense où, pour chaque morceau, seuls les musiciens les plus adaptés sont appelés sur scène. Le résultat final est plus riche, mais le coût d'exécution est bien inférieur à ce que l'on pourrait attendre de la taille globale de l'ensemble.

En termes concrets : GLM-5 possède 744 milliards de paramètres au total (l'équivalent des « connaissances » stockées dans le réseau neuronal), mais pour traiter chaque demande individuelle, il n'en active que 40 milliards. Par rapport à son prédécesseur GLM-4.5, qui s'arrêtait à 355 milliards de paramètres totaux avec 32 milliards actifs, il s'agit d'un saut considérable, tant en termes de capacité que d'efficacité dans la gestion de la charge de calcul.

À cela s'ajoute un autre choix technique pertinent : l'intégration du mécanisme d'attention éparse développé par DeepSeek, connu sous le nom de DeepSeek Sparse Attention. Sans entrer dans les détails mathématiques, cette technique permet au modèle de gérer des textes très longs, jusqu'à 200 000 « jetons » de texte, soit l'équivalent d'environ 150 000 mots, sans que les coûts de calcul n'explosent proportionnellement à la longueur. C'est la même logique qui a permis à DeepSeek de réduire les coûts opérationnels de ses modèles : ne pas gaspiller de ressources sur chaque mot du contexte, mais se concentrer sur les plus pertinents.

Les données d'entraînement sont passées de 23 à 28,5 billions de jetons, sur lesquels le modèle a été « entraîné » avant de recevoir des affinements supplémentaires par apprentissage par renforcement, une technique qui, pour simplifier, récompense le modèle lorsqu'il produit de meilleures réponses et le corrige lorsqu'il se trompe, un peu comme on entraîne un athlète avec un coach exigeant. Pour gérer ce processus à une telle échelle, l'équipe de Zhipu a développé une infrastructure d'entraînement appelée [Slime](https://github.com/THUDM/slime), qui optimise le flux de calcul de manière asynchrone, réduisant les temps morts entre un cycle d'entraînement et le suivant.

Une note pratique pour ceux qui envisagent une installation autonome : le modèle dans sa version originale en précision maximale nécessite environ 1 490 gigaoctets de mémoire, soit plus d'un téraoctet et demi. Des chiffres de centre de calcul, pas de station de travail domestique. Il existe cependant une version à précision réduite (FP8) qui divise par deux cette exigence, rendant le modèle accessible à des infrastructures plus courantes, bien que toujours significatives.

## Où il gagne, où il peine : les chiffres sans filtre

L'analyse des performances est le moment où la rhétorique des communiqués de presse rencontre la réalité des tests. Et ici, GLM-5 réserve quelques surprises, dans les deux sens.

Sur le front des capacités agentiques, c'est-à-dire la capacité du modèle à accomplir des tâches complexes en autonomie, et non seulement à répondre à des questions simples, GLM-5 atteint des résultats qui, il y a encore quelques mois, semblaient être la prérogative exclusive des modèles propriétaires payants. Selon les mesures d'[Artificial Analysis](https://artificialanalysis.ai/articles/glm-5-everything-you-need-to-know), qui utilise un indice composite appelé GDPval-AA pour mesurer l'utilité pratique dans les tâches professionnelles réelles, GLM-5 obtient un score ELO de 1412, ce qui le place à la troisième place mondiale absolue, juste après Claude Opus 4.6 d'Anthropic et GPT-5.2 d'OpenAI. Il est le premier de tous les modèles open source, avec une avance significative sur des concurrents de la même catégorie comme Kimi K2.5 et DeepSeek V3.2.

Plus significatif encore est le bond en avant par rapport à son prédécesseur direct : GLM-4.7 obtenait un score de 42 sur l'indice d'intelligence d'Artificial Analysis ; GLM-5 arrive à 50, devenant le premier modèle open source à atteindre et dépasser ce seuil dans la version 4.0 de l'indice. Ce n'est pas un chiffre arbitraire : il marque le moment où la distance entre les modèles ouverts et les modèles propriétaires haut de gamme s'est réduite à quelque chose de réellement mesurable.

En programmation, GLM-5 atteint 77,8 % dans le benchmark SWE-bench Verified, qui mesure la capacité à résoudre des problèmes réels sur des dépôts de code existants, une épreuve bien plus proche du travail quotidien d'un développeur que les tests théoriques. Claude Opus 4.5 s'arrête à 80,9 %, GPT-5.2 à 80,0 % : l'écart existe, mais il est de l'ordre de quelques points de pourcentage, pas un abîme.

Le résultat peut-être le plus surprenant concerne les hallucinations, ce phénomène par lequel les modèles de langage « inventent » des informations avec la même assurance qu'ils rapportent des faits réels, l'un des problèmes les plus ardus de l'intelligence artificielle générative. GLM-5 réduit son taux d'hallucinations de 56 % par rapport à GLM-4.7, atteignant le niveau le plus bas parmi tous les modèles testés par Artificial Analysis. Le mécanisme utilisé est simplement l'honnêteté : lorsque le modèle ne sait pas quelque chose, il s'abstient de répondre plutôt que d'inventer. Un choix qui pénalise légèrement l'exhaustivité des réponses, mais améliore radicalement la fiabilité de celles qui sont fournies.

Il existe cependant des limites concrètes à ne pas sous-estimer. GLM-5 est, pour le moment, un modèle exclusivement textuel : il n'analyse pas d'images et ne produit pas de contenu multimédia, alors que des concurrents comme Kimi K2.5 supportent déjà l'entrée visuelle. La vitesse d'inférence, environ 17-19 jetons par seconde, est sensiblement inférieure à celle des modèles entraînés sur du matériel NVIDIA de dernière génération, qui atteignent 25 jetons et plus. Et la fenêtre de contexte maximale de 200 000 jetons, bien que large, reste en dessous du million atteint par Claude Opus 4.6. Ce ne sont pas des défauts négligeables, mais des éléments à peser en fonction du coût et de la disponibilité.
![bench.jpg](bench.jpg)
[Image tirée de z.ai](https://z.ai/blog/glm-5)

## Le coup Huawei : bien plus qu'un détail technique

C'est ici que l'analyse technique laisse place à quelque chose de plus grand. GLM-5 a été entièrement entraîné sur des puces Huawei Ascend, sans un seul processeur NVIDIA. Mais pour comprendre le poids de cette affirmation, il faut faire un retour en arrière de quelques mois.

Le 14 janvier 2026, Zhipu AI avait déjà annoncé un résultat qui avait fait la une en coulisses : GLM-Image, son modèle génératif d'images, était devenu le premier modèle multimodal haut de gamme au monde à terminer tout son cycle d'entraînement sur du matériel chinois, spécifiquement sur des serveurs Huawei Ascend Atlas 800T A2. C'était déjà une étape historique. Avec GLM-5, Zhipu a reproduit et amplifié cette expérience à une échelle bien plus grande, en l'étendant à son modèle de langage phare.

Le contexte géopolitique est incontournable. Le Département du Commerce américain a inscrit Zhipu AI sur sa liste d'entités agissant contre les intérêts de sécurité nationale des États-Unis, citant des liens présumés avec des structures militaires chinoises. La conséquence pratique a été le blocage de l'accès aux processeurs NVIDIA H100 et A100, les cartes graphiques devenues le standard de facto pour l'entraînement des modèles de langage les plus avancés. La réponse de Zhipu n'a pas été de chercher des raccourcis ou du matériel alternatif occidental : elle a été d'accélérer la collaboration avec Huawei et de prouver que l'on peut faire le même travail avec des outils chinois.

Les processeurs Ascend 910B et 910C de Huawei offrent individuellement environ 60 à 80 % de la puissance de calcul d'un NVIDIA H100. Un écart non négligeable, que Zhipu a comblé par deux stratégies parallèles : une optimisation logicielle profonde via le framework MindSpore de Huawei, et une évolutivité horizontale, avec davantage de machines travaillant en parallèle pour compenser la moindre puissance individuelle de chacune. Le système CloudMatrix 384 de Huawei, qui regroupe près de 400 puces Ascend en une seule unité logique, atteint 300 pétaflops de puissance de calcul totale, un chiffre impressionnant, obtenu avec une approche qui nécessite plus de matériel mais démontre la viabilité de l'alternative « domestique ».

Il convient d'être précis sur un point : GLM-5 n'est pas le premier grand modèle de langage chinois entraîné sur des puces non-NVIDIA. Mais il est le premier de cette génération — celle des modèles à des centaines de milliards de paramètres qui rivalisent avec les meilleurs laboratoires américains — à le faire à une telle échelle, sur du matériel entièrement chinois, et à obtenir des résultats qui tiennent la comparaison avec les modèles de pointe mondiaux. La distinction est technique, mais la portée stratégique est énorme.

Il est significatif de noter que Zhipu ne s'est pas limitée aux Ascend. La documentation officielle sur [GitHub](https://github.com/zai-org/GLM-5/blob/main/example/ascend.md) répertorie le support pour Moore Threads, Cambricon, Kunlun Chip, MetaX, Enflame et Hygon, soit pratiquement tout l'écosystème des puces IA chinoises alternatives à NVIDIA. Un signal que la direction n'est pas simplement « nous utilisons Huawei parce que nous y sommes contraints », mais « nous construisons un écosystème qui ne dépende d'aucun fournisseur étranger ».

## L'open source comme arme stratégique

GLM-5 est distribué sous licence MIT, la plus permissive des licences open source, qui autorise l'utilisation commerciale, la modification et la redistribution sans restrictions significatives. Les poids du modèle sont téléchargeables librement sur [Hugging Face](https://huggingface.co/zai-org/GLM-5) et ModelScope. L'API est accessible via la plateforme [Z.ai](https://docs.z.ai/guides/llm/glm-5) à des prix nettement inférieurs à ceux des concurrents propriétaires : environ 1 dollar par million de jetons en entrée et 3,2 en sortie, contre 15 et plus pour les modèles d'OpenAI et d'Anthropic de niveau équivalent.

Il y a quelque chose de particulier, et de délibéré, dans ce choix d'ouverture radicale. Zhipu AI est une entreprise cotée à la Bourse de Hong Kong (HKEX : 2513), avec une introduction en bourse achevée le 8 janvier 2026 qui a levé environ 558 millions de dollars. Ce n'est pas un projet académique à but non lucratif : elle a des investisseurs, des actionnaires, des attentes de rendement. Pourtant, elle distribue gratuitement, et sous licence très libre, ce qu'elle considère comme son modèle le plus avancé.

La logique, que nous avons déjà vue avec DeepSeek et Kimi, est celle de l'écosystème : plus il y a de développeurs dans le monde qui construisent sur GLM-5, plus l'adoption de la plateforme Z.ai, des services API et de la marque augmente. C'est un modèle d'affaires où l'ouverture du modèle est le produit marketing le plus efficace, et en même temps, dans le contexte géopolitique actuel, un outil d'influence douce sur l'écosystème mondial de l'intelligence artificielle.

Il faut toutefois soulever une question qui reste souvent au second plan lorsqu'on parle de modèles open source chinois : les implications en termes de sécurité et de conformité réglementaire. Zhipu AI opère sous la juridiction chinoise, avec toutes les obligations qui en découlent en matière de sécurité nationale et d'accès aux données. Le modèle en soi, une fois téléchargé, est indépendant de la société qui l'a créé, mais ceux qui utilisent l'API de Z.ai s'appuient sur une infrastructure soumise aux lois chinoises. Pour de nombreuses entreprises occidentales, en particulier dans les secteurs réglementés, ce n'est pas un détail négligeable. Pour les développeurs individuels ou les entreprises dans des contextes moins sensibles, la licence MIT garantit une porte de sortie : télécharger les poids, faire tourner le modèle en autonomie, sans dépendances externes.

La question du biais dans les données d'entraînement, inévitable pour tout modèle entraîné sur des corpus textuels humains, reste également une question ouverte. Zhipu n'a pas encore publié de rapport technique détaillé (l'équipe a annoncé qu'il était « en cours »), ce qui rend difficile l'évaluation indépendante des choix faits dans la sélection des données et dans la phase d'alignement sur les valeurs. Une omission qui n'est pas un détail marginal : c'est précisément sur ce point que des laboratoires américains comme Anthropic et OpenAI ont construit une part importante de leur réputation, avec une documentation publique étendue et des politiques explicites.

## Le tableau qui se dessine

En regardant les articles publiés sur ce portail ces dernières semaines, le schéma est sans équivoque. Dans un laps de temps compressé, DeepSeek, Moonshot (avec Kimi) et Zhipu (avec GLM) ont publié des modèles qui ne se contentent pas de « presque » égaler les meilleurs laboratoires américains : dans des benchmarks et des cas d'utilisation spécifiques, ils les surpassent, souvent à une fraction du coût. Ce n'est pas une coïncidence temporelle : c'est le signal d'un secteur qui a atteint une masse critique de compétences, de capitaux et, plus significativement, une capacité de développement sur du matériel domestique que les sanctions américaines n'ont pas réussi à arrêter, mais ont au contraire accéléré.

La réponse des marchés aux modèles chinois n'est pas une nouveauté de février 2026. Le précédent le plus retentissant remonte au 27 janvier 2025, lorsque l'annonce de DeepSeek R1 a fait fondre en une seule journée près de 600 milliards de dollars de capitalisation boursière de Nvidia, la chute la plus rapide de l'histoire du marché boursier américain. GLM-5 s'inscrit dans cette lignée : il n'a pas provoqué de choc comparable, mais il consolide le récit que le marché a déjà métabolisé, celui d'un écosystème chinois qui n'a pas besoin de ralentir.

Pour le secteur dans son ensemble, la leçon de GLM-5, comme de DeepSeek et Kimi avant lui, est que la course à l'intelligence artificielle n'est plus une compétition entre deux équipes seulement. Ceux qui conçoivent des infrastructures, évaluent des fournisseurs ou prennent des décisions d'investissement dans le secteur technologique doivent composer avec un écosystème véritablement multipolaire, où les variables géopolitiques, les choix de licence et les dépendances matérielles sont devenus partie intégrante de l'analyse technique.

GLM-5 est disponible librement sur [Hugging Face](https://huggingface.co/zai-org/GLM-5), testable via API sur [Z.ai](https://chat.z.ai) et consultable dans le détail technique sur le [répertoire GitHub](https://github.com/zai-org/GLM-5) officiel. Quiconque souhaite avoir une opinion éclairée dispose de tout le matériel nécessaire pour se la forger.
