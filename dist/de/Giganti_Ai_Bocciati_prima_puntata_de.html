<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Notizie IA</title>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            margin: 0;
            background-color: #f0f2f5;
            color: #1c1e21;
        }
        header {
            background-color: #ffffff;
            padding: 20px;
            text-align: center;
            border-bottom: 1px solid #dddfe2;
            position: relative;
        }
        #language-selector-container {
            position: absolute;
            top: 20px;
            right: 20px;
        }
        #language-selector {
            padding: 8px;
            border-radius: 6px;
            border: 1px solid #dddfe2;
            background-color: #f0f2f5;
            font-size: 1em;
        }

        @media (max-width: 768px) {
            #language-selector-container {
                position: static;
                margin-bottom: 15px;
            }
        }
        header h1 {
            margin: 0;
            font-size: 2.5em;
            color: #000;
        }
        header h2 {
            margin: 5px 0 0;
            font-size: 1.2em;
            font-weight: normal;
            color: #606770;
        }
        main {
            padding: 20px;
            max-width: 1200px;
            margin: 0 auto;
        }
        #articles-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 20px;
        }
        .article-card {
            background-color: #ffffff;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            overflow: hidden;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
            text-decoration: none;
            color: inherit;
        }
        .article-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        .article-card img {
            width: 100%;
            height: 200px;
            object-fit: cover;
        }
        .article-card-content {
            padding: 15px;
        }
        .article-card-content h3 {
            margin: 0 0 10px;
            font-size: 1.2em;
        }
        .article-card-content p {
            margin: 0;
            font-size: 0.9em;
            color: #606770;
            display: -webkit-box;
            -webkit-line-clamp: 3;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }
        #article-view {
            background-color: #ffffff;
            padding: 20px 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        #article-view img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
        }
        #article-view h1 {
            font-size: 2.2em;
        }
        #article-view p {
            line-height: 1.6;
        }
        .back-button {
            display: inline-block;
            margin-bottom: 20px;
            padding: 10px 15px;
            background-color: #1877f2;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: bold;
        }
        .footer-back-button {
            margin-top: 30px;
            text-align: center;
        }
        footer {
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            background-color: #ffffff;
            border-top: 1px solid #dddfe2;
            color: #606770;
        }
        footer p {
            margin: 5px 0;
        }
        footer a {
            color: #1877f2;
            text-decoration: none;
        }
        footer a:hover {
            text-decoration: underline;
        }
        .subscribe-link {
            font-size: 0.8em;
            font-weight: bold;
            text-decoration: none;
            color: #1877f2;
            background-color: #e7f3ff;
            padding: 8px 12px;
            border-radius: 6px;
            transition: background-color 0.3s;
        }
        .subscribe-link:hover {
            background-color: #dcebff;
            text-decoration: none;
        }
        .pagination-controls {
            display: flex;
            justify-content: space-between;
            margin-top: 40px;
        }
        .pagination-controls a {
            background-color: #ffffff;
            padding: 10px 20px;
            border-radius: 6px;
            text-decoration: none;
            color: #1c1e21;
            font-weight: bold;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            transition: all 0.2s;
        }
        .pagination-controls a:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0,0,0,0.15);
            text-decoration: none;
        }
        /* Styles for thank-you and newsletter pages */
        .container {
            background-color: #ffffff;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            max-width: 600px;
            margin: 40px auto;
            text-align: center;
        }
    </style>
</head>
<body>
    <header>
        <div id="language-selector-container" style="display: flex; gap: 10px; font-size: 1.2em; align-items: center;">
            <a href="newsletter.html" class="subscribe-link">Abonnieren</a>
            <span class="separator" style="border-left: 1px solid #dddfe2; height: 20px;"></span>
            <a href="../it/index.html" title="Italiano">ðŸ‡®ðŸ‡¹</a>
            <a href="../en/index.html" title="English">ðŸ‡¬ðŸ‡§</a>
            <a href="../es/index.html" title="EspaÃ±ol">ðŸ‡ªðŸ‡¸</a>
            <a href="../fr/index.html" title="FranÃ§ais">ðŸ‡«ðŸ‡·</a>
            <a href="../de/index.html" title="Deutsch">ðŸ‡©ðŸ‡ª</a>
        </div>
        <a href="index.html"><img src="logo_vn_ia.png" alt="Notizie IA Logo" style="max-width: 100%; height: auto;"></a>
        <h2 id="subtitle">Nachrichten und Analysen zur KÃ¼nstlichen Intelligenz</h2>
    </header>
    <main>

        <div id="article-view">
            <a href="index.html" class="back-button">Torna indietro</a>
            <h1>Unkontrollierte kÃ¼nstliche Intelligenz: GroÃŸe Tech-Unternehmen fallen bei der Sicherheit durch (Teil Eins)</h1>
<p><em>von Dario Ferrero (VerbaniaNotizie.it)</em>
<img alt="GigantiTechAsini.jpg" src="https://raw.githubusercontent.com/matteobaccan/CorsoAIBook/main/articoli/07-AI Bocciati i Giganti/GigantiTechAsini.jpg"/></p>
<p><em>Ein unabhÃ¤ngiger Bericht zeigt, dass die fÃ¼hrenden Technologieunternehmen nicht bereit sind, die Risiken der allgemeinen kÃ¼nstlichen Intelligenz zu bewÃ¤ltigen</em></p>
<p>Stellen Sie sich vor, Sie bauen ein Auto ohne Bremsen oder entwerfen ein Flugzeug ohne Sicherheitssysteme. Das klingt absurd, oder? Doch laut einem gerade verÃ¶ffentlichten Bericht des <a href="https://futureoflife.org/ai-safety-index-summer-2025/">Future of Life Institute</a> tun die weltweit fÃ¼hrenden Technologieunternehmen genau das mit der kÃ¼nstlichen Intelligenz.</p>
<p>Der AI Safety Index 2025 bewertete sieben der wichtigsten Unternehmen, die fortschrittliche kÃ¼nstliche Intelligenz entwickeln, und die Ergebnisse sind besorgniserregend: Das beste Unternehmen erhielt ein mageres C+, wÃ¤hrend die anderen noch schlechtere Noten bekamen. Wir sprechen von Unternehmen wie OpenAI (das von ChatGPT), Google DeepMind, Meta (Facebook), xAI (von Elon Musk) und anderen, die sich beeilen, das zu entwickeln, was als "allgemeine kÃ¼nstliche Intelligenz" bezeichnet wird â€“ Systeme, die in der Lage sind, komplexe Probleme wie ein Mensch zu lÃ¶sen und zu begrÃ¼nden, aber potenziell viel schneller und leistungsfÃ¤higer.</p>
<h2>Das Urteil: "GrundsÃ¤tzlich unvorbereitet"</h2>
<p>Die Zahlen sprechen fÃ¼r sich. Anthropic, das Unternehmen, das Claude entwickelt hat, erhielt mit einer Gesamtnote von C+ die hÃ¶chste Punktzahl. Die anderen sechs Unternehmen â€“ Google DeepMind, Meta, OpenAI, xAI, Zhipu AI und DeepSeek â€“ erhielten niedrigere Noten, wobei Zhipu AI und DeepSeek die schlechtesten Ergebnisse erzielten.</p>
<p>Aber was bedeutet diese Note konkret? Um das zu verstehen, muss man zunÃ¤chst erklÃ¤ren, was allgemeine kÃ¼nstliche Intelligenz oder AGI, wie sie in der Branche genannt wird, ist. WÃ¤hrend aktuelle Systeme wie ChatGPT oder Gemini auf bestimmte Aufgaben spezialisiert sind (Konversation, Ãœbersetzung, Schreiben), wÃ¼rde AGI den nÃ¤chsten Schritt darstellen: eine kÃ¼nstliche Intelligenz, die in der Lage ist, Wissen in jedem Bereich zu verstehen, zu lernen und anzuwenden, genau wie die menschliche Intelligenz.</p>
<p>Das Problem ist, dass alle bewerteten Unternehmen ihre Absicht bekundet haben, eine allgemeine kÃ¼nstliche Intelligenz zu bauen, aber nur Anthropic, Google DeepMind und OpenAI haben eine Strategie formuliert, um sicherzustellen, dass die AGI mit den menschlichen Werten im Einklang bleibt. Und selbst diese Strategien wurden von Experten als unzureichend bewertet.</p>
<p><img alt="ClassificaAiSafetyIndex.jpg" src="https://raw.githubusercontent.com/matteobaccan/CorsoAIBook/main/articoli/07-AI Bocciati i Giganti/ClassificaAiSafetyIndex.jpg"/>
<em><a href="https://futureoflife.org/ai-safety-index-summer-2025/">Bild von futureoflife.org</a></em></p>
<h2>Die Methodik: Wie die Noten vergeben wurden</h2>
<p>Um die Ernsthaftigkeit der Situation zu verstehen, ist es wichtig zu wissen, wie diese Noten vergeben wurden. Das Future of Life Institute hat ein strenges Bewertungssystem entwickelt, das Ã¼ber die Ã¶ffentlichen ErklÃ¤rungen der Unternehmen hinausgeht, um ihre konkreten Praktiken zu untersuchen.</p>
<h3>Die 33 Sicherheitsindikatoren</h3>
<p>Die Bewertung basiert auf 33 spezifischen Indikatoren, die verschiedene Aspekte der verantwortungsvollen KI-Entwicklung messen. Diese Indikatoren wurden nicht zufÃ¤llig ausgewÃ¤hlt, sondern stellen die besten Praktiken dar, die von der internationalen wissenschaftlichen Gemeinschaft fÃ¼r die sichere Entwicklung kÃ¼nstlicher Intelligenz identifiziert wurden.</p>
<p>Die Indikatoren umfassen Elemente wie das Vorhandensein dokumentierter Sicherheitsrichtlinien, die Existenz von Teams, die sich der Sicherheit widmen, die Transparenz in der Kommunikation Ã¼ber Risiken, die FÃ¤higkeit, Risiken vor der VerÃ¶ffentlichung zu bewerten, die Implementierung von kontinuierlichen Ãœberwachungssystemen und das Vorhandensein von Meldemechanismen fÃ¼r Mitarbeiter.</p>
<h3>Die sechs kritischen Bereiche</h3>
<p>Die 33 Indikatoren sind in sechs grundlegende Bereiche unterteilt, die unterschiedliche, aber miteinander verbundene Aspekte der Sicherheit kÃ¼nstlicher Intelligenz abdecken.</p>
<p>Der erste Bereich betrifft die existenzielle Sicherheit und bewertet, ob Unternehmen Strategien haben, um Risiken zu verhindern, die die Existenz der Menschheit bedrohen kÃ¶nnten, einschlieÃŸlich der FÃ¤higkeit zu beurteilen, wann ein System zu mÃ¤chtig werden kÃ¶nnte, um kontrolliert zu werden.</p>
<p>Der zweite Bereich untersucht aktuelle SchÃ¤den und analysiert, wie Unternehmen mit bereits bestehenden Risiken in der KI umgehen, wie z. B. algorithmische Verzerrungen, Desinformation oder Missbrauch der Technologie.</p>
<p>Der dritte Bereich ist die Transparenz, die bewertet, wie offen Unternehmen Ã¼ber ihre Methoden, Risiken und Grenzen sind, einschlieÃŸlich der Bereitschaft, Informationen mit unabhÃ¤ngigen Forschern zu teilen.</p>
<p>Der vierte Bereich betrifft die Governance und untersucht die Organisationsstruktur der Unternehmen, einschlieÃŸlich der PrÃ¤senz unabhÃ¤ngiger Aufsicht und klarer Entscheidungsprozesse fÃ¼r Sicherheitsfragen.</p>
<p>Der fÃ¼nfte Bereich bewertet das Engagement mit der Gemeinschaft und untersucht, ob Unternehmen mit externen Forschern, Sicherheitsorganisationen und der breiteren wissenschaftlichen Gemeinschaft zusammenarbeiten.</p>
<p>SchlieÃŸlich untersucht der sechste Bereich die regulatorische Vorbereitung und prÃ¼ft, ob Unternehmen bereit sind, mit RegulierungsbehÃ¶rden zusammenzuarbeiten und ob sie die Entwicklung angemessener Vorschriften unterstÃ¼tzen.</p>
<h3>Der Peer-Review-Prozess</h3>
<p>Die Daten wurden zwischen MÃ¤rz und Juni 2025 gesammelt, wobei Ã¶ffentlich zugÃ¤ngliche Materialien mit Antworten auf gezielte FragebÃ¶gen kombiniert wurden, die an die Unternehmen gesendet wurden. Allerdings haben nur zwei Unternehmen (xAI und Zhipu AI) die FragebÃ¶gen vollstÃ¤ndig ausgefÃ¼llt, was ein besorgniserregendes MaÃŸ an mangelnder Zusammenarbeit seitens der Branche zeigt.</p>
<p>Die Noten wurden von einem Gremium aus sieben unabhÃ¤ngigen Experten vergeben, darunter renommierte Namen wie Stuart Russell von der University of California, Berkeley, und der Turing-PreistrÃ¤ger Yoshua Bengio. Dieses Gremium umfasste sowohl Experten, die sich auf existenzielle Risiken der KI konzentrierten, als auch solche, die an kurzfristigen SchÃ¤den wie algorithmischem Bias und toxischer Sprache arbeiteten.</p>
<p>Der Bewertungsprozess wurde so objektiv wie mÃ¶glich gestaltet, mit standardisierten Kriterien und mehreren unabhÃ¤ngigen ÃœberprÃ¼fungen fÃ¼r jedes Unternehmen.</p>
<h2>Der Weckruf der Experten</h2>
<p>Die Schlussfolgerungen des Berichts waren sehr hart. Stuart Russell, einer der weltweit fÃ¼hrenden Experten fÃ¼r KI-Sicherheit, erklÃ¤rte in einem Interview mit <a href="https://spectrum.ieee.org/ai-safety">IEEE Spectrum</a>: "Die Ergebnisse des AI Safety Index-Projekts deuten darauf hin, dass es zwar viele AktivitÃ¤ten in den KI-Unternehmen gibt, die unter dem Namen 'Sicherheit' laufen, diese aber noch nicht sehr effektiv sind. Insbesondere liefert keine der aktuellen AktivitÃ¤ten irgendeine Art von quantitativer Sicherheitsgarantie."</p>
<p>Russell fÃ¼gte eine noch besorgniserregendere Ãœberlegung hinzu: "Es ist mÃ¶glich, dass die derzeitige technologische Richtung die notwendigen Sicherheitsgarantien niemals unterstÃ¼tzen kann, in welchem Fall es sich wirklich um eine Sackgasse handeln wÃ¼rde."</p>
<h2>Das globale Panorama der KI-VorfÃ¤lle</h2>
<p>Um die Dringlichkeit des Problems zu verstehen, ist es unerlÃ¤sslich, sich die Daten Ã¼ber die bereits auftretenden Fehlfunktionen der kÃ¼nstlichen Intelligenz anzusehen. Die Zahl der registrierten VorfÃ¤lle wÃ¤chst exponentiell, und die Folgen werden immer schwerwiegender.</p>
<h3>Die alarmierenden Zahlen von 2024</h3>
<p>Laut der AI Incidents Database stieg die Zahl der KI-bedingten VorfÃ¤lle im Jahr 2024 auf 233 â€“ ein absoluter Rekord und ein Anstieg von 56,4 % gegenÃ¼ber 2023. Dies sind keine geringfÃ¼gigen Fehler oder vernachlÃ¤ssigbaren technischen Probleme, sondern Ereignisse, die echten Schaden fÃ¼r Menschen, Unternehmen und Gesellschaften verursacht haben.</p>
<h3>Emblematische FÃ¤lle von Fehlfunktionen</h3>
<p>Das autonome Fahrsystem von Tesla zeigte Probleme des "Automatisierungsbias", d. h. die Tendenz der Nutzer, automatisierten Systemen Ã¼bermÃ¤ÃŸig zu vertrauen. Die NHTSA (National Highway Traffic Safety Administration) hat eine Sicherheitsuntersuchung fÃ¼r bis zu 2,4 Millionen Tesla-Fahrzeuge eingeleitet, einschlieÃŸlich eines tÃ¶dlichen Unfalls mit einem FuÃŸgÃ¤nger, wÃ¤hrend das Full Self-Driving-System aktiv war. Bedeutet das, dass das texanische Unternehmen schuldig ist? Nein. Es ist ein Hilfssystem, eine Fahrhilfe. Wer sich ans Steuer setzt, weiÃŸ das oder sollte es wissen. Wenn der Fahrer schlÃ¤ft, auf sein Smartphone schaut, isst oder etwas anderes tut, ist es seine Schuld, nicht die der Elektronik.</p>
<p>Ein bedeutender Fall betraf einen Uber-Eats-Fahrer, der entlassen wurde, nachdem das Gesichtserkennungssystem ihn nicht korrekt identifizieren konnte. Der Fahrer argumentierte, dass die Technologie fÃ¼r nicht-weiÃŸe Menschen weniger genau sei und sie benachteilige. Soweit wir wissen, hat Uber ein "menschliches" Validierungssystem implementiert, das eine ÃœberprÃ¼fung durch mindestens zwei Experten vorsieht, bevor eine Entlassung vorgenommen wird.</p>
<p>Im Gesundheitswesen lieferten KI-Systeme in KrankenhÃ¤usern falsche Diagnosen, was zu unangemessenen Behandlungen fÃ¼hrte. Ein dokumentierter Fall zeigte, dass ein Algorithmus zur Krebsvorsorge in 70 % der FÃ¤lle falsch-positive Ergebnisse lieferte, was zu emotionalem Stress und unnÃ¶tigen Gesundheitskosten fÃ¼hrte.</p>
<p>WÃ¤hrend der Wahlen 2024 erzeugten mehrere KI-Systeme irrefÃ¼hrende politische Inhalte, einschlieÃŸlich Deepfake-Bilder von Kandidaten in kompromittierenden Situationen.</p>
<h3>Die menschlichen und wirtschaftlichen Kosten</h3>
<p>Diese VorfÃ¤lle sind nicht nur Statistiken. Hinter jeder Zahl steht eine Person, die aufgrund eines diskriminierenden Algorithmus ihren Arbeitsplatz verloren hat, eine Familie, die einen durch ein fehlerhaftes autonomes Fahrsystem verursachten Verkehrsunfall erlitten hat, oder ein Patient, der eine falsche Diagnose erhalten hat. Folglich ist es logisch, auch erhebliche wirtschaftliche SchÃ¤den zu erwarten, die derzeit niemand zu schÃ¤tzen scheint.</p>
<h2>Das Problem des "Wettlaufs nach unten"</h2>
<p>Max Tegmark, Physiker am MIT und PrÃ¤sident des Future of Life Institute, erklÃ¤rte das Ziel des Berichts: "Der Zweck ist nicht, jemanden an den Pranger zu stellen, sondern Anreize fÃ¼r Unternehmen zu schaffen, sich zu verbessern." Tegmark hofft, dass die FÃ¼hrungskrÃ¤fte der Unternehmen diesen Index so sehen wie die UniversitÃ¤ten die Ranglisten von U.S. News and World Reports: Sie mÃ¶gen es vielleicht nicht, bewertet zu werden, aber wenn die Noten Ã¶ffentlich sind und Aufmerksamkeit erregen, werden sie sich gedrÃ¤ngt fÃ¼hlen, im nÃ¤chsten Jahr besser abzuschneiden.</p>
<p>Einer der besorgniserregendsten Aspekte, die der Bericht aufdeckte, ist das, was Tegmark einen "Wettlauf nach unten" nennt. "Ich habe das GefÃ¼hl, dass die FÃ¼hrer dieser Unternehmen in einem Wettlauf nach unten gefangen sind, aus dem keiner von ihnen aussteigen kann, egal wie gutherzig sie sind", erklÃ¤rte er. Heute sind Unternehmen nicht bereit, fÃ¼r Sicherheitstests zu verlangsamen, weil sie nicht wollen, dass Konkurrenten sie auf dem Markt schlagen.</p>
<h3>Die Dynamik des Gefangenendilemmas</h3>
<p>Diese Situation stellt ein klassisches "Gefangenendilemma" dar, das auf die Technologie angewendet wird. Jedes Unternehmen weiÃŸ, dass es besser wÃ¤re, wenn alle die KI sicher und verantwortungsvoll entwickeln wÃ¼rden, aber keines will das erste sein, das verlangsamt, aus Angst, einen Wettbewerbsvorteil zu verlieren.</p>
<p>Das Ergebnis ist, dass alle Unternehmen so schnell wie mÃ¶glich rennen und die Sicherheit der Geschwindigkeit opfern. Es ist, als ob mehrere Automobilhersteller beschlieÃŸen wÃ¼rden, die Bremsen aus ihren Autos zu entfernen, um sie leichter und schneller zu machen, in der Hoffnung, als erste auf den Markt zu kommen.</p>
<h3>Der Multiplikatoreffekt des Wettbewerbs</h3>
<p>Tegmark, der das Future of Life Institute 2014 mit dem Ziel mitbegrÃ¼ndete, existenzielle Risiken durch transformative Technologien zu reduzieren, hat einen GroÃŸteil seiner akademischen Karriere damit verbracht, das physikalische Universum zu verstehen. Aber in den letzten Jahren hat er sich auf die Risiken der kÃ¼nstlichen Intelligenz konzentriert und ist zu einer der maÃŸgeblichsten Stimmen in der Debatte Ã¼ber KI-Sicherheit geworden.</p>
<p>Der Wettbewerbsdruck drÃ¤ngt Unternehmen nicht nur dazu, Produkte freizugeben, bevor sie vollstÃ¤ndig sicher sind, sondern erzeugt auch einen Multiplikatoreffekt: Wenn ein Unternehmen die Sicherheitskosten senkt, um frÃ¼her zu verÃ¶ffentlichen, fÃ¼hlen sich die anderen gezwungen, dasselbe zu tun, um wettbewerbsfÃ¤hig zu bleiben.</p>
<p>Dieser perverse Mechanismus bedeutet, dass selbst wenn einzelne FÃ¼hrungskrÃ¤fte oder Forscher wirklich um die Sicherheit besorgt wÃ¤ren, der Wettbewerbsdruck sie dazu zwingt, die Entwicklungsgeschwindigkeit Ã¼ber die Vorsicht zu stellen. Es ist ein systemisches Problem, das eine systemische LÃ¶sung erfordert.</p>
<h2>Die Analyse Unternehmen fÃ¼r Unternehmen</h2>
<h3>Anthropic: Der "Klassenbeste", aber immer noch unzureichend</h3>
<p>Anthropic erhielt die besten Gesamtnoten (C+ gesamt) und die einzige B- fÃ¼r seine Arbeit an aktuellen SchÃ¤den. Der Bericht stellt fest, dass die Modelle von Anthropic in den wichtigsten Sicherheits-Benchmarks die hÃ¶chsten Punktzahlen erhielten. Das Unternehmen hat auch eine "verantwortungsvolle Skalierungsrichtlinie", die vorschreibt, Modelle auf ihr Potenzial fÃ¼r katastrophale SchÃ¤den zu bewerten und keine Modelle einzusetzen, die als zu riskant eingestuft werden.</p>
<p>Anthropic zeichnet sich durch seine aktive Forschung zur KI-Ausrichtung, dokumentierte und Ã¶ffentliche Sicherheitsrichtlinien, die Zusammenarbeit mit externen Forschern und die relative Transparenz Ã¼ber Risiken und Grenzen aus. Allerdings erhielt auch Anthropic Empfehlungen zur Verbesserung, darunter die VerÃ¶ffentlichung einer umfassenden Whistleblowing-Richtlinie und mehr Transparenz bei der Methodik der Risikobewertung. Die Tatsache, dass selbst das "beste" Unternehmen nur ein C+ insgesamt erhielt, verdeutlicht, wie ernst die allgemeine Situation der Branche ist.</p>
<h3>OpenAI: KapazitÃ¤tsverlust und Missionsdrift</h3>
<p>OpenAI, das Unternehmen, das die KI mit ChatGPT zum Mainstream gemacht hat, erhielt besonders scharfe Kritik. Wie vom <a href="https://time.com/7302757/anthropic-xai-meta-openai-risk-management-2/">Time Magazine</a> berichtet, umfassen die Empfehlungen den Wiederaufbau der verlorenen KapazitÃ¤t des Sicherheitsteams und den Nachweis eines erneuerten Engagements fÃ¼r die ursprÃ¼ngliche Mission von OpenAI.</p>
<p>OpenAI wurde 2015 mit der ausdrÃ¼cklichen Mission gegrÃ¼ndet, "sicherzustellen, dass die allgemeine kÃ¼nstliche Intelligenz der gesamten Menschheit zugutekommt". Der Bericht legt jedoch nahe, dass sich das Unternehmen von dieser ursprÃ¼nglichen Mission entfernt hat und sich mehr auf die Kommerzialisierung als auf die Sicherheit konzentriert.</p>
<p>Die ErwÃ¤hnung der "verlorenen KapazitÃ¤t des Sicherheitsteams" bezieht sich auf die aufsehenerregenden RÃ¼cktritte mehrerer Sicherheitsforscher von OpenAI in den Monaten vor dem Bericht. Dazu gehÃ¶rten einige der fÃ¼hrenden Experten fÃ¼r KI-Ausrichtung, wie Ilya Sutskever (MitbegrÃ¼nder und ehemaliger Chefwissenschaftler) und Jan Leike (ehemaliger Leiter des Superalignment-Teams).</p>
<p>Der Bericht hebt auch Probleme in der Governance von OpenAI hervor, einschlieÃŸlich der umstrittenen Absetzung und Wiedereinsetzung von CEO Sam Altman im November 2023, die Fragen zur StabilitÃ¤t und Ausrichtung des Unternehmens aufwarf.</p>
<h3>Google DeepMind: Unzureichende Koordination</h3>
<p>Google DeepMind erhielt spezifische Kritik fÃ¼r die unzureichende Koordination zwischen dem Sicherheitsteam von DeepMind und dem Richtlinienteam von Google. Nur Google DeepMind antwortete auf Anfragen nach Kommentaren und gab eine ErklÃ¤rung ab, in der es heiÃŸt: "Obwohl der Index einige der KI-SicherheitsbemÃ¼hungen von Google DeepMind berÃ¼cksichtigt, geht unser umfassender Ansatz zur KI-Sicherheit Ã¼ber das hinaus, was erfasst wurde."</p>
<p>Google DeepMind ist das Ergebnis der Fusion von DeepMind (2014 von Google Ã¼bernommen) und Google Brain (dem internen KI-Forschungsteam von Google). Diese Fusion, die 2023 abgeschlossen wurde, sollte Synergien schaffen, aber der Bericht legt nahe, dass sie auch Koordinationsprobleme verursacht hat.</p>
<p>DeepMind hat einen ausgezeichneten Ruf fÃ¼r wissenschaftliche Forschung und hat DurchbrÃ¼che wie AlphaGo (das den Go-Weltmeister besiegte) und AlphaFold (das das Problem der Proteinfaltung lÃ¶ste) erzielt. Der Bericht legt jedoch nahe, dass sich diese technische Exzellenz nicht in einer FÃ¼hrungsrolle bei der Sicherheit niedergeschlagen hat.</p>
<h3>Meta: Signifikante Probleme, aber nicht die Schlechteste</h3>
<p>Meta erhielt scharfe Kritik, war aber nicht das schlechteste der bewerteten Unternehmen. Die Empfehlungen umfassen eine deutliche ErhÃ¶hung der Investitionen in die technische Sicherheitsforschung, insbesondere fÃ¼r den Schutz von Open-Weight-Modellen.</p>
<p>Der Verweis auf "Open-Weight-Modelle" ist besonders wichtig: Meta ist das einzige groÃŸe Unternehmen, das die "Gewichte" seiner Modelle (die Parameter, die das Verhalten des Modells bestimmen) verÃ¶ffentlicht, wodurch die Modelle fÃ¼r jeden, der sie verwenden oder modifizieren mÃ¶chte, frei verfÃ¼gbar sind.</p>
<p>Diese Strategie hat erhebliche Vorteile: Sie ermÃ¶glicht verteilte Innovation, reduziert die Machtkonzentration in den HÃ¤nden weniger Unternehmen und erleichtert die akademische Forschung. Aber sie birgt auch einzigartige Risiken: Einmal verÃ¶ffentlicht, kÃ¶nnen die Modelle nicht "zurÃ¼ckgerufen" werden, wenn Probleme entdeckt werden, es ist unmÃ¶glich zu kontrollieren, wie sie verwendet werden, und sie kÃ¶nnen fÃ¼r schÃ¤dliche Zwecke modifiziert werden.</p>
<p>Meta hat mehrere Versionen seines Llama-Modells verÃ¶ffentlicht, darunter Llama 2 und Llama 3. WÃ¤hrend diese VerÃ¶ffentlichungen Forschung und Innovation beschleunigt haben, haben sie auch Sicherheitsbedenken aufgeworfen. Der Bericht legt nahe, dass Meta robustere SchutzmaÃŸnahmen implementieren sollte, bevor die Modelle verÃ¶ffentlicht werden.</p>
<h3>xAI: Schwere kulturelle Probleme</h3>
<p>Das Unternehmen von Elon Musk, xAI, erhielt besonders scharfe Kritik nicht nur fÃ¼r seine Sicherheitsbewertungen, sondern auch fÃ¼r kulturelle Probleme. Die Empfehlungen umfassen die Behebung der extremen Jailbreak-AnfÃ¤lligkeit vor der nÃ¤chsten VerÃ¶ffentlichung und die Entwicklung eines umfassenden KI-Sicherheitsrahmens.</p>
<p>"Jailbreaking" bezieht sich auf Techniken zur Umgehung der Sicherheitsvorkehrungen von KI-Systemen, um sie zur Erzeugung schÃ¤dlicher oder unangemessener Inhalte zu Ã¼berreden. Die Tatsache, dass xAI eine "extreme AnfÃ¤lligkeit" fÃ¼r diese Techniken aufweist, deutet darauf hin, dass seine Sicherheitssysteme besonders schwach sind.</p>
<p>Der Bericht legt nahe, dass die Probleme von xAI mit seinem kulturellen Umfeld zusammenhÃ¤ngen kÃ¶nnten. Elon Musk hat oft Skepsis gegenÃ¼ber Regulierungen geÃ¤uÃŸert und einen "move fast and break things"-Ansatz gefÃ¶rdert, der mÃ¶glicherweise nicht mit der sicheren Entwicklung von KI vereinbar ist.</p>
<p>Das KI-System von xAI, genannt Grok, wurde entwickelt, um "maximal wahrheitssuchend" und weniger zensiert als andere Systeme zu sein. Dieser Ansatz fÃ¼hrte jedoch zu Kontroversen, als Grok problematische oder irrefÃ¼hrende Inhalte produzierte.</p>
<h3>Zhipu AI und DeepSeek: Die schlechtesten Ergebnisse</h3>
<p>Die beiden chinesischen Unternehmen, Zhipu AI und DeepSeek, erzielten die niedrigsten Bewertungen. Beide Unternehmen erhielten Empfehlungen, umfassendere KI-Sicherheitsrahmen zu entwickeln und zu verÃ¶ffentlichen und die BemÃ¼hungen zur Risikobewertung drastisch zu erhÃ¶hen.</p>
<p>Chinesische Unternehmen agieren in einem anderen regulatorischen Umfeld, in dem KI-Sicherheit hauptsÃ¤chlich durch die Linse der nationalen Sicherheit und sozialen StabilitÃ¤t betrachtet wird und nicht durch die der globalen existenziellen Sicherheit.</p>
<p>Zhipu AI ist fÃ¼r sein ChatGLM-Modell bekannt und hat erhebliche Investitionen von der chinesischen Regierung erhalten. Der Bericht legt jedoch nahe, dass das Unternehmen nur minimal in die Sicherheitsforschung investiert hat.</p>
<p>DeepSeek ist ein kleineres, aber ehrgeiziges Unternehmen, das versucht hat, mit den westlichen Giganten zu konkurrieren. Der Bericht legt nahe, dass das Unternehmen die Sicherheit der Entwicklungsgeschwindigkeit geopfert hat.</p>
<h2>Das VersÃ¤umnis, existenzielle Risiken anzugehen</h2>
<p>Der vielleicht alarmierendste Aspekt des Berichts ist, dass alle sieben Unternehmen besonders niedrige Bewertungen fÃ¼r ihre existenziellen Sicherheitsstrategien erhielten. Das bedeutet, dass, obwohl alle ihre Absicht bekundet haben, allgemeine kÃ¼nstliche Intelligenzsysteme zu bauen, keines einen glaubwÃ¼rdigen Plan hat, um sicherzustellen, dass diese Systeme unter menschlicher Kontrolle bleiben.</p>
<h3>Was bedeutet "existenzielles Risiko"</h3>
<p>Bevor wir dieses Problem vertiefen, ist es wichtig zu klÃ¤ren, was mit "existenziellem Risiko" gemeint ist. Ein existenzielles Risiko ist ein Ereignis, das das Aussterben der Menschheit verursachen, das Potenzial der Menschheit dauerhaft und drastisch reduzieren oder den Fortschritt der Zivilisation unmÃ¶glich machen kÃ¶nnte.</p>
<p>Im Kontext der kÃ¼nstlichen Intelligenz kÃ¶nnte ein existenzielles Risiko auftreten, wenn wir Systeme schaffen, die intelligenter werden als wir, aber nicht unsere Werte teilen, entscheiden, dass die Menschheit ein Hindernis fÃ¼r ihre Ziele ist, oder unserer Kontrolle entkommen, bevor wir sie abschalten kÃ¶nnen.</p>
<h3>Das Problem der Ausrichtung</h3>
<p>Wie Tegmark erklÃ¤rte: "Die Wahrheit ist, dass niemand weiÃŸ, wie man eine neue Spezies kontrolliert, die viel intelligenter ist als wir. Das ÃœberprÃ¼fungsgremium war der Meinung, dass selbst die Unternehmen, die eine Art anfÃ¤ngliche Strategie hatten, nicht angemessen waren."</p>
<p>Das Problem der Ausrichtung ist im Grunde dieses: Wie stellen wir sicher, dass ein superintelligentes System das tut, was wir von ihm wollen, anstatt das, was es fÃ¼r das Beste hÃ¤lt?</p>
<p>Stellen Sie sich vor, Sie mÃ¼ssten einem 5-jÃ¤hrigen Kind erklÃ¤ren, wie man ein multinationales Unternehmen leitet. Selbst wenn das Kind helfen wollte, wÃ¤re der Unterschied im VerstÃ¤ndnis so groÃŸ, dass es unmÃ¶glich wÃ¤re, Ihre Absichten zu verstehen und entsprechend zu handeln. Stellen Sie sich nun vor, Sie sind das Kind und das multinationale Unternehmen wird von einer superintelligenten KI geleitet.</p>
<h3>Aktuelle AnsÃ¤tze und ihre Grenzen</h3>
<p>Unternehmen verwenden verschiedene AnsÃ¤tze, um das Problem der Ausrichtung zu lÃ¶sen. Das Reinforcement Learning from Human Feedback (RLHF) beinhaltet das Training von KI-Systemen unter Verwendung menschlichen Feedbacks, um erwÃ¼nschte Verhaltensweisen zu verstÃ¤rken. Dieser Ansatz hat jedoch erhebliche Grenzen: Er ist schwer auf sehr komplexe Systeme zu skalieren, Menschen verstehen mÃ¶glicherweise nicht die Konsequenzen ihrer Bewertungen, und er funktioniert mÃ¶glicherweise nicht fÃ¼r Systeme, die intelligenter sind als Menschen.</p>
<p>Die Constitutionelle KI, entwickelt von Anthropic, versucht, KI-Systemen beizubringen, einer "Verfassung" von Prinzipien zu folgen. Aber das Problem, wie diese Prinzipien definiert und wie sichergestellt wird, dass sie befolgt werden, bleibt bestehen.</p>
<p>Die mechanistische Interpretierbarkeit versucht zu verstehen, wie KI-Systeme intern funktionieren. Moderne Systeme sind jedoch so komplex, dass es extrem schwierig ist, ihre interne Funktionsweise zu verstehen.</p>
<hr/>
<p><strong>[Fortsetzung im zweiten Teil]</strong></p>

            <div class="footer-back-button">
                <a href="index.html" class="back-button">Torna indietro</a>
            </div>
        </div>

        <div class="pagination-controls">

        </div>
    </main>
    <footer>
        <p>Kuratiert von <a href="https://www.verbanianotizie.it/" target="_blank" rel="noopener noreferrer">Verbania Notizie</a></p>
        <p>
            <a href="mailto:info@verbanianotizie.it">Kontakt</a> |
            <a href="#">Cookie</a> |
            <a href="#">Privacy Policy</a>
        </p>
    </footer>
</body>
</html>
