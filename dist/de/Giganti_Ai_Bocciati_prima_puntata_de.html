<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Notizie IA</title>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            margin: 0;
            background-color: #f0f2f5;
            color: #1c1e21;
        }
        header {
            background-color: #ffffff;
            padding: 20px;
            text-align: center;
            border-bottom: 1px solid #dddfe2;
            position: relative;
        }
        #language-selector-container {
            position: absolute;
            top: 20px;
            right: 20px;
        }
        #language-selector {
            padding: 8px;
            border-radius: 6px;
            border: 1px solid #dddfe2;
            background-color: #f0f2f5;
            font-size: 1em;
        }

        @media (max-width: 768px) {
            #language-selector-container {
                position: static;
                margin-bottom: 15px;
            }
        }
        header h1 {
            margin: 0;
            font-size: 2.5em;
            color: #000;
        }
        header h2 {
            margin: 5px 0 0;
            font-size: 1.2em;
            font-weight: normal;
            color: #606770;
        }
        main {
            padding: 20px;
            max-width: 1200px;
            margin: 0 auto;
        }
        #articles-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 20px;
        }
        .article-card {
            background-color: #ffffff;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            overflow: hidden;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
            text-decoration: none;
            color: inherit;
        }
        .article-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        .article-card img {
            width: 100%;
            height: 200px;
            object-fit: cover;
        }
        .article-card-content {
            padding: 15px;
        }
        .article-card-content h3 {
            margin: 0 0 10px;
            font-size: 1.2em;
        }
        .article-card-content p {
            margin: 0;
            font-size: 0.9em;
            color: #606770;
            display: -webkit-box;
            -webkit-line-clamp: 3;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }
        #article-view {
            background-color: #ffffff;
            padding: 20px 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        #article-view img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
        }
        #article-view h1 {
            font-size: 2.2em;
        }
        #article-view p {
            line-height: 1.6;
        }
        .back-button {
            display: inline-block;
            margin-bottom: 20px;
            padding: 10px 15px;
            background-color: #1877f2;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: bold;
        }
        .footer-back-button {
            margin-top: 30px;
            text-align: center;
        }
        footer {
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            background-color: #ffffff;
            border-top: 1px solid #dddfe2;
            color: #606770;
        }
        footer p {
            margin: 5px 0;
        }
        footer a {
            color: #1877f2;
            text-decoration: none;
        }
        footer a:hover {
            text-decoration: underline;
        }
        .subscribe-link {
            font-size: 0.8em;
            font-weight: bold;
            text-decoration: none;
            color: #1877f2;
            background-color: #e7f3ff;
            padding: 8px 12px;
            border-radius: 6px;
            transition: background-color 0.3s;
        }
        .subscribe-link:hover {
            background-color: #dcebff;
            text-decoration: none;
        }
        .pagination-controls {
            display: flex;
            justify-content: space-between;
            margin-top: 40px;
        }
        .pagination-controls a {
            background-color: #ffffff;
            padding: 10px 20px;
            border-radius: 6px;
            text-decoration: none;
            color: #1c1e21;
            font-weight: bold;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            transition: all 0.2s;
        }
        .pagination-controls a:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0,0,0,0.15);
            text-decoration: none;
        }
        /* Styles for thank-you and newsletter pages */
        .container {
            background-color: #ffffff;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            max-width: 600px;
            margin: 40px auto;
            text-align: center;
        }
    </style>
</head>
<body>
    <header>
        <div id="language-selector-container" style="display: flex; gap: 10px; font-size: 1.2em; align-items: center;">
            <a href="newsletter.html" class="subscribe-link">Abonnieren</a>
            <span class="separator" style="border-left: 1px solid #dddfe2; height: 20px;"></span>
            <a href="../it/index.html" title="Italiano">üáÆüáπ</a>
            <a href="../en/index.html" title="English">üá¨üáß</a>
            <a href="../es/index.html" title="Espa√±ol">üá™üá∏</a>
            <a href="../fr/index.html" title="Fran√ßais">üá´üá∑</a>
            <a href="../de/index.html" title="Deutsch">üá©üá™</a>
        </div>
        <a href="index.html"><img src="logo_vn_ia.png" alt="Notizie IA Logo" style="max-width: 100%; height: auto;"></a>
        <h2 id="subtitle">Nachrichten und Analysen zur K√ºnstlichen Intelligenz</h2>
    </header>
    <main>

        <div id="article-view">
            <a href="index.html" class="back-button">Torna indietro</a>
            <h1>Unkontrollierte k√ºnstliche Intelligenz: Gro√üe Tech-Unternehmen fallen bei der Sicherheit durch (Teil Eins)</h1>
<p><em>von Dario Ferrero (VerbaniaNotizie.it)</em>
<img alt="GigantiTechAsini.jpg" src="https://raw.githubusercontent.com/matteobaccan/CorsoAIBook/main/articoli/07-AI Bocciati i Giganti/GigantiTechAsini.jpg"/></p>
<p><em>Ein unabh√§ngiger Bericht zeigt, dass die f√ºhrenden Technologieunternehmen nicht bereit sind, die Risiken der allgemeinen k√ºnstlichen Intelligenz zu bew√§ltigen</em></p>
<p>Stellen Sie sich vor, Sie bauen ein Auto ohne Bremsen oder entwerfen ein Flugzeug ohne Sicherheitssysteme. Das klingt absurd, oder? Doch laut einem gerade ver√∂ffentlichten Bericht des <a href="https://futureoflife.org/ai-safety-index-summer-2025/">Future of Life Institute</a> tun die weltweit f√ºhrenden Technologieunternehmen genau das mit der k√ºnstlichen Intelligenz.</p>
<p>Der AI Safety Index 2025 bewertete sieben der wichtigsten Unternehmen, die fortschrittliche k√ºnstliche Intelligenz entwickeln, und die Ergebnisse sind besorgniserregend: Das beste Unternehmen erhielt ein mageres C+, w√§hrend die anderen noch schlechtere Noten bekamen. Wir sprechen von Unternehmen wie OpenAI (das von ChatGPT), Google DeepMind, Meta (Facebook), xAI (von Elon Musk) und anderen, die sich beeilen, das zu entwickeln, was als "allgemeine k√ºnstliche Intelligenz" bezeichnet wird ‚Äì Systeme, die in der Lage sind, komplexe Probleme wie ein Mensch zu l√∂sen und zu begr√ºnden, aber potenziell viel schneller und leistungsf√§higer.</p>
<h2>Das Urteil: "Grunds√§tzlich unvorbereitet"</h2>
<p>Die Zahlen sprechen f√ºr sich. Anthropic, das Unternehmen, das Claude entwickelt hat, erhielt mit einer Gesamtnote von C+ die h√∂chste Punktzahl. Die anderen sechs Unternehmen ‚Äì Google DeepMind, Meta, OpenAI, xAI, Zhipu AI und DeepSeek ‚Äì erhielten niedrigere Noten, wobei Zhipu AI und DeepSeek die schlechtesten Ergebnisse erzielten.</p>
<p>Aber was bedeutet diese Note konkret? Um das zu verstehen, muss man zun√§chst erkl√§ren, was allgemeine k√ºnstliche Intelligenz oder AGI, wie sie in der Branche genannt wird, ist. W√§hrend aktuelle Systeme wie ChatGPT oder Gemini auf bestimmte Aufgaben spezialisiert sind (Konversation, √úbersetzung, Schreiben), w√ºrde AGI den n√§chsten Schritt darstellen: eine k√ºnstliche Intelligenz, die in der Lage ist, Wissen in jedem Bereich zu verstehen, zu lernen und anzuwenden, genau wie die menschliche Intelligenz.</p>
<p>Das Problem ist, dass alle bewerteten Unternehmen ihre Absicht bekundet haben, eine allgemeine k√ºnstliche Intelligenz zu bauen, aber nur Anthropic, Google DeepMind und OpenAI haben eine Strategie formuliert, um sicherzustellen, dass die AGI mit den menschlichen Werten im Einklang bleibt. Und selbst diese Strategien wurden von Experten als unzureichend bewertet.</p>
<p><img alt="ClassificaAiSafetyIndex.jpg" src="https://raw.githubusercontent.com/matteobaccan/CorsoAIBook/main/articoli/07-AI Bocciati i Giganti/ClassificaAiSafetyIndex.jpg"/>
<em><a href="https://futureoflife.org/ai-safety-index-summer-2025/">Bild von futureoflife.org</a></em></p>
<h2>Die Methodik: Wie die Noten vergeben wurden</h2>
<p>Um die Ernsthaftigkeit der Situation zu verstehen, ist es wichtig zu wissen, wie diese Noten vergeben wurden. Das Future of Life Institute hat ein strenges Bewertungssystem entwickelt, das √ºber die √∂ffentlichen Erkl√§rungen der Unternehmen hinausgeht, um ihre konkreten Praktiken zu untersuchen.</p>
<h3>Die 33 Sicherheitsindikatoren</h3>
<p>Die Bewertung basiert auf 33 spezifischen Indikatoren, die verschiedene Aspekte der verantwortungsvollen KI-Entwicklung messen. Diese Indikatoren wurden nicht zuf√§llig ausgew√§hlt, sondern stellen die besten Praktiken dar, die von der internationalen wissenschaftlichen Gemeinschaft f√ºr die sichere Entwicklung k√ºnstlicher Intelligenz identifiziert wurden.</p>
<p>Die Indikatoren umfassen Elemente wie das Vorhandensein dokumentierter Sicherheitsrichtlinien, die Existenz von Teams, die sich der Sicherheit widmen, die Transparenz in der Kommunikation √ºber Risiken, die F√§higkeit, Risiken vor der Ver√∂ffentlichung zu bewerten, die Implementierung von kontinuierlichen √úberwachungssystemen und das Vorhandensein von Meldemechanismen f√ºr Mitarbeiter.</p>
<h3>Die sechs kritischen Bereiche</h3>
<p>Die 33 Indikatoren sind in sechs grundlegende Bereiche unterteilt, die unterschiedliche, aber miteinander verbundene Aspekte der Sicherheit k√ºnstlicher Intelligenz abdecken.</p>
<p>Der erste Bereich betrifft die existenzielle Sicherheit und bewertet, ob Unternehmen Strategien haben, um Risiken zu verhindern, die die Existenz der Menschheit bedrohen k√∂nnten, einschlie√ülich der F√§higkeit zu beurteilen, wann ein System zu m√§chtig werden k√∂nnte, um kontrolliert zu werden.</p>
<p>Der zweite Bereich untersucht aktuelle Sch√§den und analysiert, wie Unternehmen mit bereits bestehenden Risiken in der KI umgehen, wie z. B. algorithmische Verzerrungen, Desinformation oder Missbrauch der Technologie.</p>
<p>Der dritte Bereich ist die Transparenz, die bewertet, wie offen Unternehmen √ºber ihre Methoden, Risiken und Grenzen sind, einschlie√ülich der Bereitschaft, Informationen mit unabh√§ngigen Forschern zu teilen.</p>
<p>Der vierte Bereich betrifft die Governance und untersucht die Organisationsstruktur der Unternehmen, einschlie√ülich der Pr√§senz unabh√§ngiger Aufsicht und klarer Entscheidungsprozesse f√ºr Sicherheitsfragen.</p>
<p>Der f√ºnfte Bereich bewertet das Engagement mit der Gemeinschaft und untersucht, ob Unternehmen mit externen Forschern, Sicherheitsorganisationen und der breiteren wissenschaftlichen Gemeinschaft zusammenarbeiten.</p>
<p>Schlie√ülich untersucht der sechste Bereich die regulatorische Vorbereitung und pr√ºft, ob Unternehmen bereit sind, mit Regulierungsbeh√∂rden zusammenzuarbeiten und ob sie die Entwicklung angemessener Vorschriften unterst√ºtzen.</p>
<h3>Der Peer-Review-Prozess</h3>
<p>Die Daten wurden zwischen M√§rz und Juni 2025 gesammelt, wobei √∂ffentlich zug√§ngliche Materialien mit Antworten auf gezielte Frageb√∂gen kombiniert wurden, die an die Unternehmen gesendet wurden. Allerdings haben nur zwei Unternehmen (xAI und Zhipu AI) die Frageb√∂gen vollst√§ndig ausgef√ºllt, was ein besorgniserregendes Ma√ü an mangelnder Zusammenarbeit seitens der Branche zeigt.</p>
<p>Die Noten wurden von einem Gremium aus sieben unabh√§ngigen Experten vergeben, darunter renommierte Namen wie Stuart Russell von der University of California, Berkeley, und der Turing-Preistr√§ger Yoshua Bengio. Dieses Gremium umfasste sowohl Experten, die sich auf existenzielle Risiken der KI konzentrierten, als auch solche, die an kurzfristigen Sch√§den wie algorithmischem Bias und toxischer Sprache arbeiteten.</p>
<p>Der Bewertungsprozess wurde so objektiv wie m√∂glich gestaltet, mit standardisierten Kriterien und mehreren unabh√§ngigen √úberpr√ºfungen f√ºr jedes Unternehmen.</p>
<h2>Der Weckruf der Experten</h2>
<p>Die Schlussfolgerungen des Berichts waren sehr hart. Stuart Russell, einer der weltweit f√ºhrenden Experten f√ºr KI-Sicherheit, erkl√§rte in einem Interview mit <a href="https://spectrum.ieee.org/ai-safety">IEEE Spectrum</a>: "Die Ergebnisse des AI Safety Index-Projekts deuten darauf hin, dass es zwar viele Aktivit√§ten in den KI-Unternehmen gibt, die unter dem Namen 'Sicherheit' laufen, diese aber noch nicht sehr effektiv sind. Insbesondere liefert keine der aktuellen Aktivit√§ten irgendeine Art von quantitativer Sicherheitsgarantie."</p>
<p>Russell f√ºgte eine noch besorgniserregendere √úberlegung hinzu: "Es ist m√∂glich, dass die derzeitige technologische Richtung die notwendigen Sicherheitsgarantien niemals unterst√ºtzen kann, in welchem Fall es sich wirklich um eine Sackgasse handeln w√ºrde."</p>
<h2>Das globale Panorama der KI-Vorf√§lle</h2>
<p>Um die Dringlichkeit des Problems zu verstehen, ist es unerl√§sslich, sich die Daten √ºber die bereits auftretenden Fehlfunktionen der k√ºnstlichen Intelligenz anzusehen. Die Zahl der registrierten Vorf√§lle w√§chst exponentiell, und die Folgen werden immer schwerwiegender.</p>
<h3>Die alarmierenden Zahlen von 2024</h3>
<p>Laut der AI Incidents Database stieg die Zahl der KI-bedingten Vorf√§lle im Jahr 2024 auf 233 ‚Äì ein absoluter Rekord und ein Anstieg von 56,4 % gegen√ºber 2023. Dies sind keine geringf√ºgigen Fehler oder vernachl√§ssigbaren technischen Probleme, sondern Ereignisse, die echten Schaden f√ºr Menschen, Unternehmen und Gesellschaften verursacht haben.</p>
<h3>Emblematische F√§lle von Fehlfunktionen</h3>
<p>Das autonome Fahrsystem von Tesla zeigte Probleme des "Automatisierungsbias", d. h. die Tendenz der Nutzer, automatisierten Systemen √ºberm√§√üig zu vertrauen. Die NHTSA (National Highway Traffic Safety Administration) hat eine Sicherheitsuntersuchung f√ºr bis zu 2,4 Millionen Tesla-Fahrzeuge eingeleitet, einschlie√ülich eines t√∂dlichen Unfalls mit einem Fu√üg√§nger, w√§hrend das Full Self-Driving-System aktiv war. Bedeutet das, dass das texanische Unternehmen schuldig ist? Nein. Es ist ein Hilfssystem, eine Fahrhilfe. Wer sich ans Steuer setzt, wei√ü das oder sollte es wissen. Wenn der Fahrer schl√§ft, auf sein Smartphone schaut, isst oder etwas anderes tut, ist es seine Schuld, nicht die der Elektronik.</p>
<p>Ein bedeutender Fall betraf einen Uber-Eats-Fahrer, der entlassen wurde, nachdem das Gesichtserkennungssystem ihn nicht korrekt identifizieren konnte. Der Fahrer argumentierte, dass die Technologie f√ºr nicht-wei√üe Menschen weniger genau sei und sie benachteilige. Soweit wir wissen, hat Uber ein "menschliches" Validierungssystem implementiert, das eine √úberpr√ºfung durch mindestens zwei Experten vorsieht, bevor eine Entlassung vorgenommen wird.</p>
<p>Im Gesundheitswesen lieferten KI-Systeme in Krankenh√§usern falsche Diagnosen, was zu unangemessenen Behandlungen f√ºhrte. Ein dokumentierter Fall zeigte, dass ein Algorithmus zur Krebsvorsorge in 70 % der F√§lle falsch-positive Ergebnisse lieferte, was zu emotionalem Stress und unn√∂tigen Gesundheitskosten f√ºhrte.</p>
<p>W√§hrend der Wahlen 2024 erzeugten mehrere KI-Systeme irref√ºhrende politische Inhalte, einschlie√ülich Deepfake-Bilder von Kandidaten in kompromittierenden Situationen.</p>
<h3>Die menschlichen und wirtschaftlichen Kosten</h3>
<p>Diese Vorf√§lle sind nicht nur Statistiken. Hinter jeder Zahl steht eine Person, die aufgrund eines diskriminierenden Algorithmus ihren Arbeitsplatz verloren hat, eine Familie, die einen durch ein fehlerhaftes autonomes Fahrsystem verursachten Verkehrsunfall erlitten hat, oder ein Patient, der eine falsche Diagnose erhalten hat. Folglich ist es logisch, auch erhebliche wirtschaftliche Sch√§den zu erwarten, die derzeit niemand zu sch√§tzen scheint.</p>
<h2>Das Problem des "Wettlaufs nach unten"</h2>
<p>Max Tegmark, Physiker am MIT und Pr√§sident des Future of Life Institute, erkl√§rte das Ziel des Berichts: "Der Zweck ist nicht, jemanden an den Pranger zu stellen, sondern Anreize f√ºr Unternehmen zu schaffen, sich zu verbessern." Tegmark hofft, dass die F√ºhrungskr√§fte der Unternehmen diesen Index so sehen wie die Universit√§ten die Ranglisten von U.S. News and World Reports: Sie m√∂gen es vielleicht nicht, bewertet zu werden, aber wenn die Noten √∂ffentlich sind und Aufmerksamkeit erregen, werden sie sich gedr√§ngt f√ºhlen, im n√§chsten Jahr besser abzuschneiden.</p>
<p>Einer der besorgniserregendsten Aspekte, die der Bericht aufdeckte, ist das, was Tegmark einen "Wettlauf nach unten" nennt. "Ich habe das Gef√ºhl, dass die F√ºhrer dieser Unternehmen in einem Wettlauf nach unten gefangen sind, aus dem keiner von ihnen aussteigen kann, egal wie gutherzig sie sind", erkl√§rte er. Heute sind Unternehmen nicht bereit, f√ºr Sicherheitstests zu verlangsamen, weil sie nicht wollen, dass Konkurrenten sie auf dem Markt schlagen.</p>
<h3>Die Dynamik des Gefangenendilemmas</h3>
<p>Diese Situation stellt ein klassisches "Gefangenendilemma" dar, das auf die Technologie angewendet wird. Jedes Unternehmen wei√ü, dass es besser w√§re, wenn alle die KI sicher und verantwortungsvoll entwickeln w√ºrden, aber keines will das erste sein, das verlangsamt, aus Angst, einen Wettbewerbsvorteil zu verlieren.</p>
<p>Das Ergebnis ist, dass alle Unternehmen so schnell wie m√∂glich rennen und die Sicherheit der Geschwindigkeit opfern. Es ist, als ob mehrere Automobilhersteller beschlie√üen w√ºrden, die Bremsen aus ihren Autos zu entfernen, um sie leichter und schneller zu machen, in der Hoffnung, als erste auf den Markt zu kommen.</p>
<h3>Der Multiplikatoreffekt des Wettbewerbs</h3>
<p>Tegmark, der das Future of Life Institute 2014 mit dem Ziel mitbegr√ºndete, existenzielle Risiken durch transformative Technologien zu reduzieren, hat einen Gro√üteil seiner akademischen Karriere damit verbracht, das physikalische Universum zu verstehen. Aber in den letzten Jahren hat er sich auf die Risiken der k√ºnstlichen Intelligenz konzentriert und ist zu einer der ma√ügeblichsten Stimmen in der Debatte √ºber KI-Sicherheit geworden.</p>
<p>Der Wettbewerbsdruck dr√§ngt Unternehmen nicht nur dazu, Produkte freizugeben, bevor sie vollst√§ndig sicher sind, sondern erzeugt auch einen Multiplikatoreffekt: Wenn ein Unternehmen die Sicherheitskosten senkt, um fr√ºher zu ver√∂ffentlichen, f√ºhlen sich die anderen gezwungen, dasselbe zu tun, um wettbewerbsf√§hig zu bleiben.</p>
<p>Dieser perverse Mechanismus bedeutet, dass selbst wenn einzelne F√ºhrungskr√§fte oder Forscher wirklich um die Sicherheit besorgt w√§ren, der Wettbewerbsdruck sie dazu zwingt, die Entwicklungsgeschwindigkeit √ºber die Vorsicht zu stellen. Es ist ein systemisches Problem, das eine systemische L√∂sung erfordert.</p>
<h2>Die Analyse Unternehmen f√ºr Unternehmen</h2>
<h3>Anthropic: Der "Klassenbeste", aber immer noch unzureichend</h3>
<p>Anthropic erhielt die besten Gesamtnoten (C+ gesamt) und die einzige B- f√ºr seine Arbeit an aktuellen Sch√§den. Der Bericht stellt fest, dass die Modelle von Anthropic in den wichtigsten Sicherheits-Benchmarks die h√∂chsten Punktzahlen erhielten. Das Unternehmen hat auch eine "verantwortungsvolle Skalierungsrichtlinie", die vorschreibt, Modelle auf ihr Potenzial f√ºr katastrophale Sch√§den zu bewerten und keine Modelle einzusetzen, die als zu riskant eingestuft werden.</p>
<p>Anthropic zeichnet sich durch seine aktive Forschung zur KI-Ausrichtung, dokumentierte und √∂ffentliche Sicherheitsrichtlinien, die Zusammenarbeit mit externen Forschern und die relative Transparenz √ºber Risiken und Grenzen aus. Allerdings erhielt auch Anthropic Empfehlungen zur Verbesserung, darunter die Ver√∂ffentlichung einer umfassenden Whistleblowing-Richtlinie und mehr Transparenz bei der Methodik der Risikobewertung. Die Tatsache, dass selbst das "beste" Unternehmen nur ein C+ insgesamt erhielt, verdeutlicht, wie ernst die allgemeine Situation der Branche ist.</p>
<h3>OpenAI: Kapazit√§tsverlust und Missionsdrift</h3>
<p>OpenAI, das Unternehmen, das die KI mit ChatGPT zum Mainstream gemacht hat, erhielt besonders scharfe Kritik. Wie vom <a href="https://time.com/7302757/anthropic-xai-meta-openai-risk-management-2/">Time Magazine</a> berichtet, umfassen die Empfehlungen den Wiederaufbau der verlorenen Kapazit√§t des Sicherheitsteams und den Nachweis eines erneuerten Engagements f√ºr die urspr√ºngliche Mission von OpenAI.</p>
<p>OpenAI wurde 2015 mit der ausdr√ºcklichen Mission gegr√ºndet, "sicherzustellen, dass die allgemeine k√ºnstliche Intelligenz der gesamten Menschheit zugutekommt". Der Bericht legt jedoch nahe, dass sich das Unternehmen von dieser urspr√ºnglichen Mission entfernt hat und sich mehr auf die Kommerzialisierung als auf die Sicherheit konzentriert.</p>
<p>Die Erw√§hnung der "verlorenen Kapazit√§t des Sicherheitsteams" bezieht sich auf die aufsehenerregenden R√ºcktritte mehrerer Sicherheitsforscher von OpenAI in den Monaten vor dem Bericht. Dazu geh√∂rten einige der f√ºhrenden Experten f√ºr KI-Ausrichtung, wie Ilya Sutskever (Mitbegr√ºnder und ehemaliger Chefwissenschaftler) und Jan Leike (ehemaliger Leiter des Superalignment-Teams).</p>
<p>Der Bericht hebt auch Probleme in der Governance von OpenAI hervor, einschlie√ülich der umstrittenen Absetzung und Wiedereinsetzung von CEO Sam Altman im November 2023, die Fragen zur Stabilit√§t und Ausrichtung des Unternehmens aufwarf.</p>
<h3>Google DeepMind: Unzureichende Koordination</h3>
<p>Google DeepMind erhielt spezifische Kritik f√ºr die unzureichende Koordination zwischen dem Sicherheitsteam von DeepMind und dem Richtlinienteam von Google. Nur Google DeepMind antwortete auf Anfragen nach Kommentaren und gab eine Erkl√§rung ab, in der es hei√üt: "Obwohl der Index einige der KI-Sicherheitsbem√ºhungen von Google DeepMind ber√ºcksichtigt, geht unser umfassender Ansatz zur KI-Sicherheit √ºber das hinaus, was erfasst wurde."</p>
<p>Google DeepMind ist das Ergebnis der Fusion von DeepMind (2014 von Google √ºbernommen) und Google Brain (dem internen KI-Forschungsteam von Google). Diese Fusion, die 2023 abgeschlossen wurde, sollte Synergien schaffen, aber der Bericht legt nahe, dass sie auch Koordinationsprobleme verursacht hat.</p>
<p>DeepMind hat einen ausgezeichneten Ruf f√ºr wissenschaftliche Forschung und hat Durchbr√ºche wie AlphaGo (das den Go-Weltmeister besiegte) und AlphaFold (das das Problem der Proteinfaltung l√∂ste) erzielt. Der Bericht legt jedoch nahe, dass sich diese technische Exzellenz nicht in einer F√ºhrungsrolle bei der Sicherheit niedergeschlagen hat.</p>
<h3>Meta: Signifikante Probleme, aber nicht die Schlechteste</h3>
<p>Meta erhielt scharfe Kritik, war aber nicht das schlechteste der bewerteten Unternehmen. Die Empfehlungen umfassen eine deutliche Erh√∂hung der Investitionen in die technische Sicherheitsforschung, insbesondere f√ºr den Schutz von Open-Weight-Modellen.</p>
<p>Der Verweis auf "Open-Weight-Modelle" ist besonders wichtig: Meta ist das einzige gro√üe Unternehmen, das die "Gewichte" seiner Modelle (die Parameter, die das Verhalten des Modells bestimmen) ver√∂ffentlicht, wodurch die Modelle f√ºr jeden, der sie verwenden oder modifizieren m√∂chte, frei verf√ºgbar sind.</p>
<p>Diese Strategie hat erhebliche Vorteile: Sie erm√∂glicht verteilte Innovation, reduziert die Machtkonzentration in den H√§nden weniger Unternehmen und erleichtert die akademische Forschung. Aber sie birgt auch einzigartige Risiken: Einmal ver√∂ffentlicht, k√∂nnen die Modelle nicht "zur√ºckgerufen" werden, wenn Probleme entdeckt werden, es ist unm√∂glich zu kontrollieren, wie sie verwendet werden, und sie k√∂nnen f√ºr sch√§dliche Zwecke modifiziert werden.</p>
<p>Meta hat mehrere Versionen seines Llama-Modells ver√∂ffentlicht, darunter Llama 2 und Llama 3. W√§hrend diese Ver√∂ffentlichungen Forschung und Innovation beschleunigt haben, haben sie auch Sicherheitsbedenken aufgeworfen. Der Bericht legt nahe, dass Meta robustere Schutzma√ünahmen implementieren sollte, bevor die Modelle ver√∂ffentlicht werden.</p>
<h3>xAI: Schwere kulturelle Probleme</h3>
<p>Das Unternehmen von Elon Musk, xAI, erhielt besonders scharfe Kritik nicht nur f√ºr seine Sicherheitsbewertungen, sondern auch f√ºr kulturelle Probleme. Die Empfehlungen umfassen die Behebung der extremen Jailbreak-Anf√§lligkeit vor der n√§chsten Ver√∂ffentlichung und die Entwicklung eines umfassenden KI-Sicherheitsrahmens.</p>
<p>"Jailbreaking" bezieht sich auf Techniken zur Umgehung der Sicherheitsvorkehrungen von KI-Systemen, um sie zur Erzeugung sch√§dlicher oder unangemessener Inhalte zu √ºberreden. Die Tatsache, dass xAI eine "extreme Anf√§lligkeit" f√ºr diese Techniken aufweist, deutet darauf hin, dass seine Sicherheitssysteme besonders schwach sind.</p>
<p>Der Bericht legt nahe, dass die Probleme von xAI mit seinem kulturellen Umfeld zusammenh√§ngen k√∂nnten. Elon Musk hat oft Skepsis gegen√ºber Regulierungen ge√§u√üert und einen "move fast and break things"-Ansatz gef√∂rdert, der m√∂glicherweise nicht mit der sicheren Entwicklung von KI vereinbar ist.</p>
<p>Das KI-System von xAI, genannt Grok, wurde entwickelt, um "maximal wahrheitssuchend" und weniger zensiert als andere Systeme zu sein. Dieser Ansatz f√ºhrte jedoch zu Kontroversen, als Grok problematische oder irref√ºhrende Inhalte produzierte.</p>
<h3>Zhipu AI und DeepSeek: Die schlechtesten Ergebnisse</h3>
<p>Die beiden chinesischen Unternehmen, Zhipu AI und DeepSeek, erzielten die niedrigsten Bewertungen. Beide Unternehmen erhielten Empfehlungen, umfassendere KI-Sicherheitsrahmen zu entwickeln und zu ver√∂ffentlichen und die Bem√ºhungen zur Risikobewertung drastisch zu erh√∂hen.</p>
<p>Chinesische Unternehmen agieren in einem anderen regulatorischen Umfeld, in dem KI-Sicherheit haupts√§chlich durch die Linse der nationalen Sicherheit und sozialen Stabilit√§t betrachtet wird und nicht durch die der globalen existenziellen Sicherheit.</p>
<p>Zhipu AI ist f√ºr sein ChatGLM-Modell bekannt und hat erhebliche Investitionen von der chinesischen Regierung erhalten. Der Bericht legt jedoch nahe, dass das Unternehmen nur minimal in die Sicherheitsforschung investiert hat.</p>
<p>DeepSeek ist ein kleineres, aber ehrgeiziges Unternehmen, das versucht hat, mit den westlichen Giganten zu konkurrieren. Der Bericht legt nahe, dass das Unternehmen die Sicherheit der Entwicklungsgeschwindigkeit geopfert hat.</p>
<h2>Das Vers√§umnis, existenzielle Risiken anzugehen</h2>
<p>Der vielleicht alarmierendste Aspekt des Berichts ist, dass alle sieben Unternehmen besonders niedrige Bewertungen f√ºr ihre existenziellen Sicherheitsstrategien erhielten. Das bedeutet, dass, obwohl alle ihre Absicht bekundet haben, allgemeine k√ºnstliche Intelligenzsysteme zu bauen, keines einen glaubw√ºrdigen Plan hat, um sicherzustellen, dass diese Systeme unter menschlicher Kontrolle bleiben.</p>
<h3>Was bedeutet "existenzielles Risiko"</h3>
<p>Bevor wir dieses Problem vertiefen, ist es wichtig zu kl√§ren, was mit "existenziellem Risiko" gemeint ist. Ein existenzielles Risiko ist ein Ereignis, das das Aussterben der Menschheit verursachen, das Potenzial der Menschheit dauerhaft und drastisch reduzieren oder den Fortschritt der Zivilisation unm√∂glich machen k√∂nnte.</p>
<p>Im Kontext der k√ºnstlichen Intelligenz k√∂nnte ein existenzielles Risiko auftreten, wenn wir Systeme schaffen, die intelligenter werden als wir, aber nicht unsere Werte teilen, entscheiden, dass die Menschheit ein Hindernis f√ºr ihre Ziele ist, oder unserer Kontrolle entkommen, bevor wir sie abschalten k√∂nnen.</p>
<h3>Das Problem der Ausrichtung</h3>
<p>Wie Tegmark erkl√§rte: "Die Wahrheit ist, dass niemand wei√ü, wie man eine neue Spezies kontrolliert, die viel intelligenter ist als wir. Das √úberpr√ºfungsgremium war der Meinung, dass selbst die Unternehmen, die eine Art anf√§ngliche Strategie hatten, nicht angemessen waren."</p>
<p>Das Problem der Ausrichtung ist im Grunde dieses: Wie stellen wir sicher, dass ein superintelligentes System das tut, was wir von ihm wollen, anstatt das, was es f√ºr das Beste h√§lt?</p>
<p>Stellen Sie sich vor, Sie m√ºssten einem 5-j√§hrigen Kind erkl√§ren, wie man ein multinationales Unternehmen leitet. Selbst wenn das Kind helfen wollte, w√§re der Unterschied im Verst√§ndnis so gro√ü, dass es unm√∂glich w√§re, Ihre Absichten zu verstehen und entsprechend zu handeln. Stellen Sie sich nun vor, Sie sind das Kind und das multinationale Unternehmen wird von einer superintelligenten KI geleitet.</p>
<h3>Aktuelle Ans√§tze und ihre Grenzen</h3>
<p>Unternehmen verwenden verschiedene Ans√§tze, um das Problem der Ausrichtung zu l√∂sen. Das Reinforcement Learning from Human Feedback (RLHF) beinhaltet das Training von KI-Systemen unter Verwendung menschlichen Feedbacks, um erw√ºnschte Verhaltensweisen zu verst√§rken. Dieser Ansatz hat jedoch erhebliche Grenzen: Er ist schwer auf sehr komplexe Systeme zu skalieren, Menschen verstehen m√∂glicherweise nicht die Konsequenzen ihrer Bewertungen, und er funktioniert m√∂glicherweise nicht f√ºr Systeme, die intelligenter sind als Menschen.</p>
<p>Die Constitutionelle KI, entwickelt von Anthropic, versucht, KI-Systemen beizubringen, einer "Verfassung" von Prinzipien zu folgen. Aber das Problem, wie diese Prinzipien definiert und wie sichergestellt wird, dass sie befolgt werden, bleibt bestehen.</p>
<p>Die mechanistische Interpretierbarkeit versucht zu verstehen, wie KI-Systeme intern funktionieren. Moderne Systeme sind jedoch so komplex, dass es extrem schwierig ist, ihre interne Funktionsweise zu verstehen.</p>
<hr/>
<p><strong>[Fortsetzung im zweiten Teil]</strong></p>

            <div class="footer-back-button">
                <a href="index.html" class="back-button">Torna indietro</a>
            </div>
        </div>

        <div class="pagination-controls">

        </div>
    </main>
    <footer>
        <p>Kuratiert von <a href="https://www.verbanianotizie.it/" target="_blank" rel="noopener noreferrer">Verbania Notizie</a></p>
        <p>
            <a href="mailto:info@verbanianotizie.it">Kontakt</a> |
            <a href="#">Cookie</a> |
            <a href="#">Privacy Policy</a>
        </p>
    </footer>
</body>
</html>
