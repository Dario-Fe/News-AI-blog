<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Notizie IA</title>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            margin: 0;
            background-color: #f0f2f5;
            color: #1c1e21;
        }
        header {
            background-color: #ffffff;
            padding: 20px;
            text-align: center;
            border-bottom: 1px solid #dddfe2;
            position: relative;
        }
        #language-selector-container {
            position: absolute;
            top: 20px;
            right: 20px;
        }
        #language-selector {
            padding: 8px;
            border-radius: 6px;
            border: 1px solid #dddfe2;
            background-color: #f0f2f5;
            font-size: 1em;
        }

        @media (max-width: 768px) {
            #language-selector-container {
                position: static;
                margin-bottom: 15px;
            }
        }
        header h1 {
            margin: 0;
            font-size: 2.5em;
            color: #000;
        }
        header h2 {
            margin: 5px 0 0;
            font-size: 1.2em;
            font-weight: normal;
            color: #606770;
        }
        main {
            padding: 20px;
            max-width: 1200px;
            margin: 0 auto;
        }
        #articles-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 20px;
        }
        .article-card {
            background-color: #ffffff;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            overflow: hidden;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
            text-decoration: none;
            color: inherit;
        }
        .article-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        .article-card img {
            width: 100%;
            height: 200px;
            object-fit: cover;
        }
        .article-card-content {
            padding: 15px;
        }
        .article-card-content h3 {
            margin: 0 0 10px;
            font-size: 1.2em;
        }
        .article-card-content p {
            margin: 0;
            font-size: 0.9em;
            color: #606770;
            display: -webkit-box;
            -webkit-line-clamp: 3;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }
        #article-view {
            background-color: #ffffff;
            padding: 20px 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        #article-view img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
        }
        #article-view h1 {
            font-size: 2.2em;
        }
        #article-view p {
            line-height: 1.6;
        }
        .back-button {
            display: inline-block;
            margin-bottom: 20px;
            padding: 10px 15px;
            background-color: #1877f2;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: bold;
        }
        .footer-back-button {
            margin-top: 30px;
            text-align: center;
        }
        footer {
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            background-color: #ffffff;
            border-top: 1px solid #dddfe2;
            color: #606770;
        }
        footer p {
            margin: 5px 0;
        }
        footer a {
            color: #1877f2;
            text-decoration: none;
        }
        footer a:hover {
            text-decoration: underline;
        }
        .subscribe-link {
            font-size: 0.8em;
            font-weight: bold;
            text-decoration: none;
            color: #1877f2;
            background-color: #e7f3ff;
            padding: 8px 12px;
            border-radius: 6px;
            transition: background-color 0.3s;
        }
        .subscribe-link:hover {
            background-color: #dcebff;
            text-decoration: none;
        }
        .pagination-controls {
            display: flex;
            justify-content: space-between;
            margin-top: 40px;
        }
        .pagination-controls a {
            background-color: #ffffff;
            padding: 10px 20px;
            border-radius: 6px;
            text-decoration: none;
            color: #1c1e21;
            font-weight: bold;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            transition: all 0.2s;
        }
        .pagination-controls a:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0,0,0,0.15);
            text-decoration: none;
        }
        /* Styles for thank-you and newsletter pages */
        .container {
            background-color: #ffffff;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            max-width: 600px;
            margin: 40px auto;
            text-align: center;
        }
    </style>
</head>
<body>
    <header>
        <div id="language-selector-container" style="display: flex; gap: 10px; font-size: 1.2em; align-items: center;">
            <a href="newsletter.html" class="subscribe-link">Suscr√≠bete</a>
            <span class="separator" style="border-left: 1px solid #dddfe2; height: 20px;"></span>
            <a href="../it/index.html" title="Italiano">üáÆüáπ</a>
            <a href="../en/index.html" title="English">üá¨üáß</a>
            <a href="../es/index.html" title="Espa√±ol">üá™üá∏</a>
        </div>
        <a href="index.html"><img src="logo_vn_ia.png" alt="Notizie IA Logo" style="max-width: 100%; height: auto;"></a>
        <h2 id="subtitle">Noticias y an√°lisis sobre Inteligencia Artificial</h2>
    </header>
    <main>

        <div id="article-view">
            <a href="index.html" class="back-button">Torna indietro</a>
            <h1>Inteligencia Artificial sin Control: Las Grandes Empresas Tecnol√≥gicas Reprueban en Seguridad (Primera Parte)</h1>
<p><em>por Dario Ferrero (VerbaniaNotizie.it)</em>
<img alt="GigantiTechAsini.jpg" src="https://raw.githubusercontent.com/matteobaccan/CorsoAIBook/main/articoli/07-AI Bocciati i Giganti/GigantiTechAsini.jpg"/></p>
<p><em>Un informe independiente revela que las principales empresas tecnol√≥gicas no est√°n preparadas para gestionar los riesgos de la inteligencia artificial general.</em></p>
<p>Imagina construir un coche sin frenos, o dise√±ar un avi√≥n sin sistemas de seguridad. Suena absurdo, ¬øverdad? Sin embargo, seg√∫n un informe reci√©n publicado por el <a href="https://futureoflife.org/ai-safety-index-summer-2025/">Future of Life Institute</a>, esto es exactamente lo que las principales empresas tecnol√≥gicas del mundo est√°n haciendo con la inteligencia artificial.</p>
<p>El AI Safety Index 2025 evalu√≥ a siete de las empresas m√°s importantes que desarrollan inteligencia artificial avanzada, y los resultados son alarmantes: la mejor calificada obtuvo una m√≠sera C+, mientras que las dem√°s recibieron calificaciones a√∫n peores. Estamos hablando de empresas como OpenAI (la creadora de ChatGPT), Google DeepMind, Meta (Facebook), xAI (la empresa de Elon Musk) y otras que compiten por desarrollar lo que se conoce como "inteligencia artificial general" (AGI), sistemas capaces de razonar y resolver problemas complejos como lo har√≠a un ser humano, pero potencialmente mucho m√°s r√°pido y con m√°s potencia.</p>
<h2>El Veredicto: "Fundamentalmente no Preparadas"</h2>
<p>Los n√∫meros hablan por s√≠ solos. Anthropic, la empresa que cre√≥ Claude, obtuvo la puntuaci√≥n m√°s alta con una calificaci√≥n general de C+. Las otras seis empresas ‚ÄîGoogle DeepMind, Meta, OpenAI, xAI, Zhipu AI y DeepSeek‚Äî recibieron calificaciones inferiores, siendo Zhipu AI y DeepSeek las que obtuvieron los peores resultados.</p>
<p>Pero, ¬øqu√© significa realmente esta calificaci√≥n? Para entenderlo, primero debemos explicar qu√© es la inteligencia artificial general, o AGI. Mientras que los sistemas actuales como ChatGPT o Gemini est√°n especializados en tareas espec√≠ficas (conversaci√≥n, traducci√≥n, escritura), la AGI representar√≠a el siguiente paso: una inteligencia artificial capaz de comprender, aprender y aplicar el conocimiento en cualquier campo, al igual que la inteligencia humana.</p>
<p>El problema es que todas las empresas evaluadas han declarado su intenci√≥n de construir una inteligencia artificial general, pero solo Anthropic, Google DeepMind y OpenAI han articulado una estrategia para garantizar que la AGI se mantenga alineada con los valores humanos. E incluso estas estrategias han sido consideradas inadecuadas por los expertos.</p>
<p><img alt="ClassificaAiSafetyIndex.jpg" src="https://raw.githubusercontent.com/matteobaccan/CorsoAIBook/main/articoli/07-AI Bocciati i Giganti/ClassificaAiSafetyIndex.jpg"/>
<em><a href="https://futureoflife.org/ai-safety-index-summer-2025/">Imagen de futureoflife.org</a></em></p>
<h2>La Metodolog√≠a: C√≥mo se Asignaron las Calificaciones</h2>
<p>Para comprender la gravedad de la situaci√≥n, es importante saber c√≥mo se asignaron estas calificaciones. El Future of Life Institute desarroll√≥ un riguroso sistema de evaluaci√≥n que va m√°s all√° de las declaraciones p√∫blicas para examinar las pr√°cticas concretas de estas empresas.</p>
<h3>Los 33 Indicadores de Seguridad</h3>
<p>La evaluaci√≥n se basa en 33 indicadores espec√≠ficos que miden diferentes aspectos del desarrollo responsable de la IA. Estos indicadores no fueron elegidos al azar, sino que representan las mejores pr√°cticas identificadas por la comunidad cient√≠fica internacional para el desarrollo seguro de la inteligencia artificial.</p>
<p>Los indicadores incluyen elementos como la presencia de pol√≠ticas de seguridad documentadas, la existencia de equipos de seguridad dedicados, la transparencia en las comunicaciones de riesgos, la capacidad de evaluar los riesgos antes del lanzamiento, la implementaci√≥n de sistemas de monitoreo continuo y la presencia de mecanismos de denuncia para los empleados.</p>
<h3>Los Seis Dominios Cr√≠ticos</h3>
<p>Los 33 indicadores se organizan en seis dominios fundamentales que cubren aspectos diferentes pero interconectados de la seguridad de la inteligencia artificial.</p>
<p>El primer dominio se refiere a la seguridad existencial y eval√∫a si las empresas tienen estrategias para prevenir riesgos que podr√≠an amenazar la existencia de la humanidad, incluida la capacidad de evaluar cu√°ndo un sistema podr√≠a volverse demasiado poderoso para ser controlado.</p>
<p>El segundo dominio examina los da√±os actuales, analizando c√≥mo las empresas abordan los riesgos de la IA existentes, como el sesgo algor√≠tmico, la desinformaci√≥n o el uso indebido de la tecnolog√≠a.</p>
<p>El tercer dominio es la transparencia, que eval√∫a cu√°n abiertas son las empresas sobre sus m√©todos, riesgos y limitaciones, incluida su disposici√≥n a compartir informaci√≥n con investigadores independientes.</p>
<p>El cuarto dominio se refiere a la gobernanza y examina la estructura organizativa de las empresas, incluida la presencia de supervisi√≥n independiente y procesos de toma de decisiones claros para cuestiones de seguridad.</p>
<p>El quinto dominio eval√∫a la participaci√≥n de la comunidad, examinando si las empresas colaboran con investigadores externos, organizaciones de seguridad y la comunidad cient√≠fica en general.</p>
<p>Finalmente, el sexto dominio examina la preparaci√≥n regulatoria, verificando si las empresas est√°n listas para trabajar con los reguladores y si apoyan el desarrollo de regulaciones apropiadas.</p>
<h3>El Proceso de Revisi√≥n por Pares</h3>
<p>Los datos se recopilaron entre marzo y junio de 2025, combinando materiales disponibles p√∫blicamente con respuestas a cuestionarios espec√≠ficos enviados a las empresas. Sin embargo, solo dos empresas (xAI y Zhipu AI) completaron los cuestionarios en su totalidad, lo que pone de manifiesto un preocupante nivel de falta de colaboraci√≥n por parte de la industria.</p>
<p>Las calificaciones fueron asignadas por un panel de siete expertos independientes, que inclu√≠a nombres de prestigio como Stuart Russell de la Universidad de California, Berkeley, y el ganador del Premio Turing, Yoshua Bengio. Este panel inclu√≠a tanto a expertos que se centraron en los riesgos existenciales de la IA como a aquellos que trabajaron en da√±os a corto plazo como el sesgo algor√≠tmico y el lenguaje t√≥xico.</p>
<p>El proceso de evaluaci√≥n fue dise√±ado para ser lo m√°s objetivo posible, con criterios estandarizados y m√∫ltiples revisiones independientes para cada empresa.</p>
<h2>El Grito de Alarma de los Expertos</h2>
<p>Las conclusiones del informe fueron duras. Stuart Russell, uno de los principales expertos mundiales en seguridad de la IA, declar√≥ en una entrevista con <a href="https://spectrum.ieee.org/ai-safety">IEEE Spectrum</a>: "Los resultados del proyecto AI Safety Index sugieren que, si bien hay mucha actividad en las empresas de IA bajo el nombre de 'seguridad', todav√≠a no es muy efectiva. En particular, ninguna de las actividades actuales proporciona ning√∫n tipo de garant√≠a cuantitativa de seguridad".</p>
<p>Russell a√±adi√≥ una consideraci√≥n a√∫n m√°s preocupante: "Es posible que la direcci√≥n tecnol√≥gica actual nunca pueda soportar las garant√≠as de seguridad necesarias, en cuyo caso ser√≠a un callej√≥n sin salida".</p>
<h2>El Panorama Global de los Incidentes de IA</h2>
<p>Para comprender la urgencia del problema, es esencial observar los datos sobre los fallos de la inteligencia artificial que ya est√°n ocurriendo. El n√∫mero de incidentes registrados est√° creciendo exponencialmente, y las consecuencias son cada vez m√°s graves.</p>
<h3>Las Cifras Alarmantes de 2024</h3>
<p>Seg√∫n la Base de Datos de Incidentes de IA, el n√∫mero de incidentes relacionados con la IA aument√≥ a 233 en 2024, un m√°ximo hist√≥rico y un aumento del 56,4 % con respecto a 2023. No se trata de errores menores o problemas t√©cnicos insignificantes, sino de eventos que han causado un da√±o real a personas, empresas y la sociedad.</p>
<h3>Casos Emblem√°ticos de Fallos</h3>
<p>El sistema de conducci√≥n aut√≥noma de Tesla ha mostrado problemas de "sesgo de automatizaci√≥n", es decir, la tendencia de los usuarios a confiar excesivamente en los sistemas automatizados. La NHTSA (Administraci√≥n Nacional de Seguridad del Tr√°fico en las Carreteras) ha abierto una investigaci√≥n de seguridad sobre hasta 2,4 millones de veh√≠culos de Tesla, incluido un accidente mortal con un peat√≥n mientras el sistema de Conducci√≥n Aut√≥noma Total estaba activo. ¬øSignifica esto que la empresa con sede en Texas es culpable? No. Es un sistema que ayuda, una ayuda a la conducci√≥n. Quien se pone al volante lo sabe, o deber√≠a saberlo. Si el conductor est√° durmiendo, mirando su tel√©fono, comiendo o haciendo otra cosa, es su culpa, no de la electr√≥nica.</p>
<p>Un caso significativo involucr√≥ a un conductor de Uber Eats que fue despedido despu√©s de que el sistema de reconocimiento facial no lo identificara correctamente. El conductor argument√≥ que la tecnolog√≠a es menos precisa para las personas no blancas, lo que las pone en desventaja. Por lo que sabemos, Uber ha implementado un sistema de validaci√≥n "humano" que implica la revisi√≥n por parte de al menos dos expertos antes de proceder con un despido.</p>
<p>En el sector de la salud, los sistemas de IA utilizados en los hospitales han proporcionado diagn√≥sticos incorrectos, lo que ha llevado a una atenci√≥n inadecuada. Un caso documentado mostr√≥ que un algoritmo de detecci√≥n de c√°ncer produjo falsos positivos en el 70 % de los casos, causando angustia emocional y costes sanitarios innecesarios.</p>
<p>Durante las elecciones de 2024, varios sistemas de IA generaron contenido pol√≠tico enga√±oso, incluidas im√°genes deepfake de candidatos en situaciones comprometedoras.</p>
<h3>El Costo Humano y Econ√≥mico</h3>
<p>Estos incidentes no son solo estad√≠sticas. Detr√°s de cada n√∫mero hay una persona que perdi√≥ su trabajo debido a un algoritmo discriminatorio, una familia que sufri√≥ un accidente de tr√°fico causado por un sistema de conducci√≥n aut√≥noma defectuoso o un paciente que recibi√≥ un diagn√≥stico incorrecto. En consecuencia, es l√≥gico prever da√±os econ√≥micos significativos, que nadie parece haber estimado por el momento.</p>
<h2>El Problema de la "Carrera hacia el Abismo"</h2>
<p>Max Tegmark, f√≠sico del MIT y presidente del Future of Life Institute, explic√≥ el objetivo del informe: "El prop√≥sito no es avergonzar a nadie, sino proporcionar incentivos para que las empresas mejoren". Tegmark espera que los ejecutivos de las empresas vean este √≠ndice como las universidades ven las clasificaciones de U.S. News and World Reports: puede que no les guste ser evaluados, pero si las calificaciones son p√∫blicas y llaman la atenci√≥n, se sentir√°n obligados a hacerlo mejor el pr√≥ximo a√±o.</p>
<p>Uno de los aspectos m√°s preocupantes que surgieron del informe es lo que Tegmark llama una "carrera hacia el abismo". "Siento que los l√≠deres de estas empresas est√°n atrapados en una carrera hacia el abismo de la que ninguno de ellos puede escapar, sin importar cu√°n bien intencionados sean", explic√≥. Hoy en d√≠a, las empresas no est√°n dispuestas a ralentizar las pruebas de seguridad porque no quieren que los competidores les ganen en el mercado.</p>
<h3>La Din√°mica del Dilema del Prisionero</h3>
<p>Esta situaci√≥n representa un cl√°sico "dilema del prisionero" aplicado a la tecnolog√≠a. Todas las empresas saben que ser√≠a mejor si todas desarrollaran la IA de forma segura y responsable, pero ninguna quiere ser la primera en ralentizar, por temor a perder su ventaja competitiva.</p>
<p>El resultado es que todas las empresas terminan corriendo lo m√°s r√°pido posible, sacrificando la seguridad por la velocidad. Es como si varios fabricantes de autom√≥viles decidieran eliminar los frenos de sus coches para hacerlos m√°s ligeros y r√°pidos, con la esperanza de llegar primero al mercado.</p>
<h3>El Efecto Multiplicador de la Competencia</h3>
<p>Tegmark, cofundador del Future of Life Institute en 2014 con el objetivo de reducir los riesgos existenciales de las tecnolog√≠as transformadoras, ha dedicado gran parte de su carrera acad√©mica a comprender el universo f√≠sico. Pero en los √∫ltimos a√±os, se ha centrado en los riesgos de la inteligencia artificial, convirti√©ndose en una de las voces m√°s autorizadas en el debate sobre la seguridad de la IA.</p>
<p>La presi√≥n competitiva no solo empuja a las empresas a lanzar productos antes de que sean completamente seguros, sino que tambi√©n crea un efecto multiplicador: si una empresa recorta los costes de seguridad para lanzar antes, las dem√°s se sienten obligadas a hacer lo mismo para seguir siendo competitivas.</p>
<p>Este mecanismo perverso significa que, aunque los ejecutivos o investigadores individuales estuvieran genuinamente preocupados por la seguridad, la presi√≥n competitiva los empuja a priorizar la velocidad de desarrollo sobre la prudencia. Es un problema sist√©mico que requiere una soluci√≥n sist√©mica.</p>
<h2>An√°lisis Empresa por Empresa</h2>
<h3>Anthropic: El "Mejor de la Clase" pero A√∫n Insuficiente</h3>
<p>Anthropic recibi√≥ las mejores puntuaciones generales (C+ en general), recibiendo el √∫nico B- por su trabajo sobre los da√±os actuales. El informe se√±ala que los modelos de Anthropic han recibido las puntuaciones m√°s altas en los principales puntos de referencia de seguridad. La empresa tambi√©n tiene una "pol√≠tica de escalamiento responsable" que exige evaluar los modelos por su potencial para causar un da√±o catastr√≥fico y no implementar modelos considerados demasiado arriesgados.</p>
<p>Anthropic se destaca por su investigaci√≥n activa sobre la alineaci√≥n de la IA, pol√≠ticas de seguridad documentadas y p√∫blicas, colaboraci√≥n con investigadores externos y una relativa transparencia sobre los riesgos y limitaciones. Sin embargo, incluso Anthropic recibi√≥ recomendaciones de mejora, como la publicaci√≥n de una pol√≠tica integral de denuncia de irregularidades y una mayor transparencia sobre su metodolog√≠a de evaluaci√≥n de riesgos. El hecho de que incluso la "mejor" empresa solo haya recibido una C+ en general ilustra la gravedad de la situaci√≥n general del sector.</p>
<h3>OpenAI: P√©rdida de Capacidad y Deriva de la Misi√≥n</h3>
<p>OpenAI, la empresa que populariz√≥ la IA con ChatGPT, recibi√≥ cr√≠ticas particularmente severas. Seg√∫n lo informado por <a href="https://time.com/7302757/anthropic-xai-meta-openai-risk-management-2/">Time Magazine</a>, las recomendaciones incluyen reconstruir la capacidad perdida del equipo de seguridad y demostrar un compromiso renovado con la misi√≥n original de OpenAI.</p>
<p>OpenAI fue fundada en 2015 con la misi√≥n expl√≠cita de "garantizar que la inteligencia artificial general beneficie a toda la humanidad". Sin embargo, el informe sugiere que la empresa se ha desviado de esta misi√≥n original, centr√°ndose m√°s en la comercializaci√≥n que en la seguridad.</p>
<p>La menci√≥n de la "capacidad perdida del equipo de seguridad" se refiere a las renuncias de alto perfil de varios investigadores de seguridad de OpenAI en los meses previos al informe. Entre ellos se encontraban algunos de los principales expertos en alineaci√≥n de la IA, como Ilya Sutskever (cofundador y ex cient√≠fico jefe) y Jan Leike (ex jefe del equipo de superalineaci√≥n).</p>
<p>El informe tambi√©n destaca problemas en la gobernanza de OpenAI, incluida la controvertida destituci√≥n y reincorporaci√≥n del CEO Sam Altman en noviembre de 2023, que plante√≥ dudas sobre la estabilidad y la direcci√≥n de la empresa.</p>
<h3>Google DeepMind: Coordinaci√≥n Insuficiente</h3>
<p>Google DeepMind recibi√≥ cr√≠ticas espec√≠ficas por la insuficiente coordinaci√≥n entre el equipo de seguridad de DeepMind y el equipo de pol√≠ticas de Google. Solo Google DeepMind respondi√≥ a las solicitudes de comentarios, proporcionando una declaraci√≥n que dec√≠a: "Si bien el √≠ndice incorpora algunos de los esfuerzos de seguridad de la IA de Google DeepMind, nuestro enfoque integral de la seguridad de la IA se extiende m√°s all√° de lo que se captur√≥".</p>
<p>Google DeepMind es el resultado de la fusi√≥n entre DeepMind (adquirida por Google en 2014) y Google Brain (el equipo de investigaci√≥n de IA interno de Google). Esta fusi√≥n, completada en 2023, ten√≠a la intenci√≥n de crear sinergias, pero el informe sugiere que tambi√©n ha creado problemas de coordinaci√≥n.</p>
<p>DeepMind tiene una excelente reputaci√≥n en investigaci√≥n cient√≠fica, habiendo logrado avances como AlphaGo (que venci√≥ al campe√≥n mundial de Go) y AlphaFold (que resolvi√≥ el problema del plegamiento de prote√≠nas). Sin embargo, el informe sugiere que esta excelencia t√©cnica no se ha traducido en un liderazgo en seguridad.</p>
<h3>Meta: Problemas Significativos pero no el Peor</h3>
<p>Meta recibi√≥ cr√≠ticas severas, but no fue la peor entre las empresas evaluadas. Las recomendaciones incluyen aumentar significativamente la inversi√≥n en investigaci√≥n de seguridad t√©cnica, especialmente para la protecci√≥n de los modelos de peso abierto.</p>
<p>La referencia a los "modelos de peso abierto" es particularmente importante: Meta es la √∫nica gran empresa que publica los "pesos" de sus modelos (los par√°metros que determinan el comportamiento del modelo), lo que hace que los modelos est√©n disponibles gratuitamente para que cualquiera los use o modifique.</p>
<p>Esta estrategia tiene ventajas significativas: permite la innovaci√≥n distribuida, reduce la concentraci√≥n de poder en manos de unas pocas empresas y facilita la investigaci√≥n acad√©mica. Pero tambi√©n conlleva riesgos √∫nicos: una vez publicados, los modelos no se pueden "retirar" si se descubren problemas, es imposible controlar c√≥mo se utilizan y se pueden modificar con fines maliciosos.</p>
<p>Meta ha lanzado varias versiones de su modelo Llama, incluidas Llama 2 y Llama 3. Si bien estos lanzamientos han acelerado la investigaci√≥n y la innovaci√≥n, tambi√©n han generado preocupaciones sobre la seguridad. El informe sugiere que Meta deber√≠a implementar protecciones m√°s s√≥lidas antes de lanzar los modelos.</p>
<h3>xAI: Graves Problemas Culturales</h3>
<p>La empresa de Elon Musk, xAI, recibi√≥ cr√≠ticas particularmente severas no solo por sus puntuaciones de seguridad, sino tambi√©n por problemas culturales. Las recomendaciones incluyen abordar la vulnerabilidad extrema al jailbreak antes del pr√≥ximo lanzamiento y desarrollar un marco integral de seguridad de la IA.</p>
<p>El "jailbreaking" se refiere a las t√©cnicas para eludir las protecciones de seguridad de los sistemas de IA, convenci√©ndolos de que produzcan contenido da√±ino o inapropiado. El hecho de que xAI tenga una "vulnerabilidad extrema" a estas t√©cnicas sugiere que sus sistemas de seguridad son particularmente d√©biles.</p>
<p>El informe sugiere que los problemas de xAI pueden estar relacionados con su entorno cultural. Elon Musk a menudo ha expresado escepticismo hacia las regulaciones y ha promovido un enfoque de "moverse r√°pido y romper cosas" que puede no ser compatible con el desarrollo seguro de la IA.</p>
<p>El sistema de IA de xAI, llamado Grok, fue dise√±ado para ser "m√°ximamente buscador de la verdad" y menos censurado que otros sistemas. Sin embargo, este enfoque ha generado controversia cuando Grok ha producido contenido problem√°tico o enga√±oso.</p>
<h3>Zhipu AI y DeepSeek: Los Peores Resultados</h3>
<p>Las dos empresas chinas, Zhipu AI y DeepSeek, recibieron las puntuaciones m√°s bajas en la evaluaci√≥n. Ambas empresas recibieron recomendaciones para desarrollar y publicar marcos de seguridad de la IA m√°s completos y para aumentar dr√°sticamente sus esfuerzos de evaluaci√≥n de riesgos.</p>
<p>Las empresas chinas operan en un entorno regulatorio diferente, donde la seguridad de la IA se ve principalmente a trav√©s del prisma de la seguridad nacional y la estabilidad social en lugar de la seguridad existencial global.</p>
<p>Zhipu AI es conocida por su modelo ChatGLM y ha recibido una importante inversi√≥n del gobierno chino. Sin embargo, el informe sugiere que la empresa ha invertido m√≠nimamente en investigaci√≥n de seguridad.</p>
<p>DeepSeek es una empresa m√°s peque√±a pero ambiciosa que ha intentado competir con los gigantes occidentales. El informe sugiere que la empresa ha sacrificado la seguridad por la velocidad de desarrollo.</p>
<h2>El Fracaso en Abordar los Riesgos Existenciales</h2>
<p>Quiz√°s el aspecto m√°s alarmante del informe es que las siete empresas obtuvieron puntuaciones particularmente bajas en sus estrategias de seguridad existencial. Esto significa que, a pesar de que todas han declarado su intenci√≥n de construir sistemas de inteligencia artificial general, ninguna tiene un plan cre√≠ble para garantizar que estos sistemas permanezcan bajo control humano.</p>
<h3>¬øQu√© Significa "Riesgo Existencial"?</h3>
<p>Antes de profundizar en este problema, es importante aclarar qu√© se entiende por "riesgo existencial". Un riesgo existencial es un evento que podr√≠a causar la extinci√≥n de la humanidad, reducir permanente y dr√°sticamente el potencial de la humanidad o imposibilitar el progreso de la civilizaci√≥n.</p>
<p>En el contexto de la inteligencia artificial, un riesgo existencial podr√≠a ocurrir si creamos sistemas que se vuelven m√°s inteligentes que nosotros pero no comparten nuestros valores, deciden que la humanidad es un obst√°culo para sus objetivos o escapan a nuestro control antes de que podamos apagarlos.</p>
<h3>El Problema de la Alineaci√≥n</h3>
<p>Como explic√≥ Tegmark: "La verdad es que nadie sabe c√≥mo controlar una nueva especie que es mucho m√°s inteligente que nosotros. El panel de revisi√≥n consider√≥ que incluso las empresas que ten√≠an alguna forma de estrategia inicial, no era adecuada".</p>
<p>El problema de la alineaci√≥n es fundamentalmente este: ¬øc√≥mo podemos estar seguros de que un sistema superinteligente har√° lo que queremos que haga, en lugar de lo que cree que es mejor?</p>
<p>Imagina tener que explicarle a un ni√±o de 5 a√±os c√≥mo dirigir una corporaci√≥n multinacional. Incluso si el ni√±o quisiera ayudar, la diferencia de comprensi√≥n es tan grande que le ser√≠a imposible entender tus intenciones y actuar en consecuencia. Ahora imagina que t√∫ eres el ni√±o y la multinacional est√° dirigida por una IA superinteligente.</p>
<h3>Enfoques Actuales y sus Limitaciones</h3>
<p>Las empresas est√°n utilizando varios enfoques para tratar de resolver el problema de la alineaci√≥n. El Aprendizaje por Refuerzo a partir de la Retroalimentaci√≥n Humana (RLHF) implica entrenar sistemas de IA utilizando la retroalimentaci√≥n humana para reforzar comportamientos deseables. Sin embargo, este enfoque tiene limitaciones significativas: es dif√≠cil de escalar a sistemas muy complejos, los humanos pueden no comprender las consecuencias de sus evaluaciones y puede que no funcione para sistemas que son m√°s inteligentes que los humanos.</p>
<p>La IA Constitucional, desarrollada por Anthropic, busca ense√±ar a los sistemas de IA a seguir una "constituci√≥n" de principios. Pero sigue existiendo el problema de c√≥mo definir estos principios y c√≥mo garantizar que se sigan.</p>
<p>La interpretabilidad mecanicista busca comprender c√≥mo funcionan internamente los sistemas de IA. Sin embargo, los sistemas modernos son tan complejos que es extremadamente dif√≠cil comprender su funcionamiento interno.</p>
<hr/>
<p><strong>[Continuar√° en la segunda parte]</strong></p>

            <div class="footer-back-button">
                <a href="index.html" class="back-button">Torna indietro</a>
            </div>
        </div>

        <div class="pagination-controls">

        </div>
    </main>
    <footer>
        <p>A cargo de <a href="https://www.verbanianotizie.it/" target="_blank" rel="noopener noreferrer">Verbania Notizie</a></p>
        <p>
            <a href="mailto:info@verbanianotizie.it">Contacto</a> |
            <a href="#">Cookie</a> |
            <a href="#">Privacy Policy</a>
        </p>
    </footer>
</body>
</html>
