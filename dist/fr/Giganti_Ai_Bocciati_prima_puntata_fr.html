<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Notizie IA</title>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            margin: 0;
            background-color: #f0f2f5;
            color: #1c1e21;
        }
        header {
            background-color: #ffffff;
            padding: 20px;
            text-align: center;
            border-bottom: 1px solid #dddfe2;
            position: relative;
        }
        #language-selector-container {
            position: absolute;
            top: 20px;
            right: 20px;
        }
        #language-selector {
            padding: 8px;
            border-radius: 6px;
            border: 1px solid #dddfe2;
            background-color: #f0f2f5;
            font-size: 1em;
        }

        @media (max-width: 768px) {
            #language-selector-container {
                position: static;
                margin-bottom: 15px;
            }
        }
        header h1 {
            margin: 0;
            font-size: 2.5em;
            color: #000;
        }
        header h2 {
            margin: 5px 0 0;
            font-size: 1.2em;
            font-weight: normal;
            color: #606770;
        }
        main {
            padding: 20px;
            max-width: 1200px;
            margin: 0 auto;
        }
        #articles-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 20px;
        }
        .article-card {
            background-color: #ffffff;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            overflow: hidden;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
            text-decoration: none;
            color: inherit;
        }
        .article-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        .article-card img {
            width: 100%;
            height: 200px;
            object-fit: cover;
        }
        .article-card-content {
            padding: 15px;
        }
        .article-card-content h3 {
            margin: 0 0 10px;
            font-size: 1.2em;
        }
        .article-card-content p {
            margin: 0;
            font-size: 0.9em;
            color: #606770;
            display: -webkit-box;
            -webkit-line-clamp: 3;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }
        #article-view {
            background-color: #ffffff;
            padding: 20px 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        #article-view img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
        }
        #article-view h1 {
            font-size: 2.2em;
        }
        #article-view p {
            line-height: 1.6;
        }
        .back-button {
            display: inline-block;
            margin-bottom: 20px;
            padding: 10px 15px;
            background-color: #1877f2;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: bold;
        }
        .footer-back-button {
            margin-top: 30px;
            text-align: center;
        }
        footer {
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            background-color: #ffffff;
            border-top: 1px solid #dddfe2;
            color: #606770;
        }
        footer p {
            margin: 5px 0;
        }
        footer a {
            color: #1877f2;
            text-decoration: none;
        }
        footer a:hover {
            text-decoration: underline;
        }
        .subscribe-link {
            font-size: 0.8em;
            font-weight: bold;
            text-decoration: none;
            color: #1877f2;
            background-color: #e7f3ff;
            padding: 8px 12px;
            border-radius: 6px;
            transition: background-color 0.3s;
        }
        .subscribe-link:hover {
            background-color: #dcebff;
            text-decoration: none;
        }
        .pagination-controls {
            display: flex;
            justify-content: space-between;
            margin-top: 40px;
        }
        .pagination-controls a {
            background-color: #ffffff;
            padding: 10px 20px;
            border-radius: 6px;
            text-decoration: none;
            color: #1c1e21;
            font-weight: bold;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            transition: all 0.2s;
        }
        .pagination-controls a:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0,0,0,0.15);
            text-decoration: none;
        }
        /* Styles for thank-you and newsletter pages */
        .container {
            background-color: #ffffff;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            max-width: 600px;
            margin: 40px auto;
            text-align: center;
        }
    </style>
</head>
<body>
    <header>
        <div id="language-selector-container" style="display: flex; gap: 10px; font-size: 1.2em; align-items: center;">
            <a href="newsletter.html" class="subscribe-link">S'abonner</a>
            <span class="separator" style="border-left: 1px solid #dddfe2; height: 20px;"></span>
            <a href="../it/index.html" title="Italiano">üáÆüáπ</a>
            <a href="../en/index.html" title="English">üá¨üáß</a>
            <a href="../es/index.html" title="Espa√±ol">üá™üá∏</a>
            <a href="../fr/index.html" title="Fran√ßais">üá´üá∑</a>
            <a href="../de/index.html" title="Deutsch">üá©üá™</a>
        </div>
        <a href="index.html"><img src="logo_vn_ia.png" alt="Notizie IA Logo" style="max-width: 100%; height: auto;"></a>
        <h2 id="subtitle">Actualit√©s et analyses sur l'intelligence artificielle</h2>
    </header>
    <main>

        <div id="article-view">
            <a href="index.html" class="back-button">Torna indietro</a>
            <h1>L'intelligence artificielle sans contr√¥le : les g√©ants de la tech recal√©s en mati√®re de s√©curit√© (Premi√®re Partie)</h1>
<p><em>par Dario Ferrero (VerbaniaNotizie.it)</em>
<img alt="GigantiTechAsini.jpg" src="https://raw.githubusercontent.com/matteobaccan/CorsoAIBook/main/articoli/07-AI Bocciati i Giganti/GigantiTechAsini.jpg"/></p>
<p><em>Un rapport ind√©pendant r√©v√®le que les principales entreprises technologiques ne sont pas pr√™tes √† g√©rer les risques de l'intelligence artificielle g√©n√©rale</em></p>
<p>Imaginez construire une voiture sans freins, ou concevoir un avion sans syst√®mes de s√©curit√©. Cela semble absurde, n'est-ce pas ? Pourtant, selon un rapport r√©cemment publi√© par le <a href="https://futureoflife.org/ai-safety-index-summer-2025/">Future of Life Institute</a>, c'est exactement ce que font les principales entreprises technologiques mondiales avec l'intelligence artificielle.</p>
<p>L'AI Safety Index 2025 a √©valu√© sept des plus importantes entreprises d√©veloppant une intelligence artificielle avanc√©e, et les r√©sultats sont pr√©occupants : le meilleur a obtenu un maigre C+, tandis que les autres ont re√ßu des notes encore pires. Nous parlons d'entreprises comme OpenAI (celle de ChatGPT), Google DeepMind, Meta (Facebook), xAI (d'Elon Musk), et d'autres qui se pr√©cipitent pour d√©velopper ce qu'on appelle "l'intelligence artificielle g√©n√©rale" - des syst√®mes capables de raisonner et de r√©soudre des probl√®mes complexes comme le ferait un √™tre humain, mais potentiellement beaucoup plus rapidement et puissamment.</p>
<h2>Le verdict : "Fondamentalement non pr√©par√©es"</h2>
<p>Les chiffres parlent d'eux-m√™mes. Anthropic, l'entreprise qui a cr√©√© Claude, a obtenu le score le plus √©lev√© avec une note globale de C+. Les six autres entreprises - Google DeepMind, Meta, OpenAI, xAI, Zhipu AI et DeepSeek - ont re√ßu des notes inf√©rieures, Zhipu AI et DeepSeek obtenant les pires r√©sultats.</p>
<p>Mais que signifie concr√®tement cette note ? Pour le comprendre, il faut d'abord expliquer ce qu'est l'intelligence artificielle g√©n√©rale, ou AGI comme on l'appelle dans le secteur. Si les syst√®mes actuels comme ChatGPT ou Gemini sont sp√©cialis√©s dans des t√¢ches sp√©cifiques (conversation, traduction, √©criture), l'AGI repr√©senterait l'√©tape suivante : une intelligence artificielle capable de comprendre, d'apprendre et d'appliquer des connaissances dans n'importe quel domaine, tout comme le fait l'intelligence humaine.</p>
<p>Le probl√®me est que toutes les entreprises √©valu√©es ont d√©clar√© leur intention de construire une intelligence artificielle g√©n√©rale, mais seules Anthropic, Google DeepMind et OpenAI ont articul√© une strat√©gie pour garantir que l'AGI reste align√©e sur les valeurs humaines. Et m√™me ces strat√©gies ont √©t√© jug√©es inad√©quates par les experts.</p>
<p><img alt="ClassificaAiSafetyIndex.jpg" src="https://raw.githubusercontent.com/matteobaccan/CorsoAIBook/main/articoli/07-AI Bocciati i Giganti/ClassificaAiSafetyIndex.jpg"/>
<em><a href="https://futureoflife.org/ai-safety-index-summer-2025/">Image tir√©e de futureoflife.org</a></em></p>
<h2>La m√©thodologie : comment les notes ont √©t√© attribu√©es</h2>
<p>Pour comprendre la gravit√© de la situation, il est important de savoir comment ces notes ont √©t√© attribu√©es. Le Future of Life Institute a d√©velopp√© un syst√®me d'√©valuation rigoureux qui va au-del√† des d√©clarations publiques des entreprises pour examiner leurs pratiques concr√®tes.</p>
<h3>Les 33 indicateurs de s√©curit√©</h3>
<p>L'√©valuation est bas√©e sur 33 indicateurs sp√©cifiques qui mesurent diff√©rents aspects du d√©veloppement responsable de l'IA. Ces indicateurs n'ont pas √©t√© choisis au hasard, mais repr√©sentent les meilleures pratiques identifi√©es par la communaut√© scientifique internationale pour le d√©veloppement s√ªr de l'intelligence artificielle.</p>
<p>Les indicateurs incluent des √©l√©ments tels que la pr√©sence de politiques de s√©curit√© document√©es, l'existence d'√©quipes d√©di√©es √† la s√©curit√©, la transparence dans les communications sur les risques, la capacit√© √† √©valuer les risques avant la publication, la mise en ≈ìuvre de syst√®mes de surveillance continue et la pr√©sence de m√©canismes de signalement pour les employ√©s.</p>
<h3>Les six domaines critiques</h3>
<p>Les 33 indicateurs sont organis√©s en six domaines fondamentaux qui couvrent des aspects diff√©rents mais interconnect√©s de la s√©curit√© de l'intelligence artificielle.</p>
<p>Le premier domaine concerne la s√©curit√© existentielle et √©value si les entreprises ont des strat√©gies pour pr√©venir les risques qui pourraient menacer l'existence de l'humanit√©, y compris la capacit√© d'√©valuer quand un syst√®me pourrait devenir trop puissant pour √™tre contr√¥l√©.</p>
<p>Le deuxi√®me domaine examine les dommages actuels, en analysant comment les entreprises traitent les risques d√©j√† pr√©sents dans l'IA tels que les biais algorithmiques, la d√©sinformation ou l'utilisation abusive de la technologie.</p>
<p>Le troisi√®me domaine est la transparence, qui √©value √† quel point les entreprises sont ouvertes sur leurs m√©thodes, risques et limitations, y compris la volont√© de partager des informations avec des chercheurs ind√©pendants.</p>
<p>Le quatri√®me domaine concerne la gouvernance et examine la structure organisationnelle des entreprises, y compris la pr√©sence d'une supervision ind√©pendante et de processus d√©cisionnels clairs pour les questions de s√©curit√©.</p>
<p>Le cinqui√®me domaine √©value l'engagement avec la communaut√©, en examinant si les entreprises collaborent avec des chercheurs externes, des organisations de s√©curit√© et la communaut√© scientifique au sens large.</p>
<p>Enfin, le sixi√®me domaine examine la pr√©paration r√©glementaire, en v√©rifiant si les entreprises sont pr√™tes √† travailler avec les r√©gulateurs et si elles soutiennent le d√©veloppement de r√©glementations appropri√©es.</p>
<h3>Le processus d'√©valuation par les pairs</h3>
<p>Les donn√©es ont √©t√© collect√©es entre mars et juin 2025, combinant des documents accessibles au public avec des r√©ponses √† des questionnaires cibl√©s envoy√©s aux entreprises. Cependant, seules deux entreprises (xAI et Zhipu AI) ont enti√®rement rempli les questionnaires, soulignant un niveau de non-collaboration pr√©occupant de la part du secteur.</p>
<p>Les notes ont √©t√© attribu√©es par un panel de sept experts ind√©pendants, comprenant des noms prestigieux comme Stuart Russell de l'Universit√© de Californie √† Berkeley, et le laur√©at du prix Turing Yoshua Bengio. Ce panel comprenait √† la fois des experts ax√©s sur les risques existentiels de l'IA et ceux qui ont travaill√© sur les dommages √† court terme tels que les biais algorithmiques et le langage toxique.</p>
<p>Le processus d'√©valuation a √©t√© con√ßu pour √™tre aussi objectif que possible, avec des crit√®res standardis√©s et de multiples examens ind√©pendants pour chaque entreprise.</p>
<h2>Le cri d'alarme des experts</h2>
<p>Les conclusions du rapport ont √©t√© tr√®s dures. Stuart Russell, l'un des plus grands experts mondiaux en s√©curit√© de l'IA, a d√©clar√© dans une interview √† <a href="https://spectrum.ieee.org/ai-safety">IEEE Spectrum</a> : "Les r√©sultats du projet AI Safety Index sugg√®rent que, bien qu'il y ait beaucoup d'activit√© dans les entreprises d'IA sous le nom de 's√©curit√©', elle n'est pas encore tr√®s efficace. En particulier, aucune des activit√©s actuelles ne fournit de garantie quantitative de s√©curit√©."</p>
<p>Russell a ajout√© une consid√©ration encore plus inqui√©tante : "Il est possible que la direction technologique actuelle ne puisse jamais supporter les garanties de s√©curit√© n√©cessaires, auquel cas ce serait vraiment une impasse."</p>
<h2>Le panorama mondial des incidents li√©s √† l'IA</h2>
<p>Pour comprendre l'urgence du probl√®me, il est essentiel d'examiner les donn√©es sur les dysfonctionnements de l'intelligence artificielle qui se produisent d√©j√†. Le nombre d'incidents enregistr√©s augmente de mani√®re exponentielle, et les cons√©quences deviennent de plus en plus graves.</p>
<h3>Les chiffres alarmants de 2024</h3>
<p>Selon l'AI Incidents Database, le nombre d'incidents li√©s √† l'IA est pass√© √† 233 en 2024 - un record absolu et une augmentation de 56,4 % par rapport √† 2023. Il ne s'agit pas d'erreurs mineures ou de probl√®mes techniques n√©gligeables, mais d'√©v√©nements qui ont caus√© des dommages r√©els √† des personnes, des entreprises et des soci√©t√©s.</p>
<h3>Cas embl√©matiques de dysfonctionnements</h3>
<p>Le syst√®me de conduite autonome de Tesla a montr√© des probl√®mes de "biais d'automatisation", c'est-√†-dire la tendance des utilisateurs √† faire trop confiance aux syst√®mes automatis√©s. La NHTSA (National Highway Traffic Safety Administration) a ouvert une enqu√™te de s√©curit√© sur jusqu'√† 2,4 millions de v√©hicules Tesla, incluant un accident mortel avec un pi√©ton alors que le syst√®me Full Self-Driving √©tait actif. Cela signifie-t-il que l'entreprise texane est coupable ? Non. C'est un syst√®me d'aide, une assistance √† la conduite. Celui qui prend le volant le sait, ou doit le savoir. Si le conducteur dort, regarde son smartphone, mange ou fait autre chose, c'est de sa faute, pas de l'√©lectronique.</p>
<p>Un cas significatif a concern√© un livreur d'Uber Eats qui a √©t√© licenci√© apr√®s que le syst√®me de reconnaissance faciale n'ait pas r√©ussi √† l'identifier correctement. Le chauffeur a soutenu que la technologie est moins pr√©cise pour les personnes non blanches, les d√©savantageant. D'apr√®s ce que nous savons, Uber a mis en place un syst√®me de validation "humaine" qui pr√©voit l'examen par au moins deux experts avant de proc√©der √† un licenciement.</p>
<p>Dans le secteur de la sant√©, des syst√®mes d'IA utilis√©s dans les h√¥pitaux ont fourni des diagnostics erron√©s, entra√Ænant des traitements inappropri√©s. Un cas document√© a vu un algorithme de d√©pistage du cancer produire des faux positifs dans 70 % des cas, causant un stress √©motionnel et des co√ªts de sant√© inutiles.</p>
<p>Lors des √©lections de 2024, plusieurs syst√®mes d'IA ont g√©n√©r√© des contenus politiques trompeurs, y compris des images deepfake de candidats dans des situations compromettantes.</p>
<h3>Le co√ªt humain et √©conomique</h3>
<p>Ces incidents ne sont pas que des statistiques. Derri√®re chaque chiffre, il y a une personne qui a perdu son emploi √† cause d'un algorithme discriminatoire, une famille qui a subi un accident de la route caus√© par un syst√®me de conduite autonome d√©fectueux, ou un patient qui a re√ßu un diagnostic erron√©. Par cons√©quent, il est logique de pr√©voir √©galement des dommages √©conomiques consid√©rables, que personne ne semble avoir estim√©s pour le moment.</p>
<h2>Le probl√®me de la "course vers le bas"</h2>
<p>Max Tegmark, physicien au MIT et pr√©sident du Future of Life Institute, a expliqu√© l'objectif du rapport : "Le but n'est pas de faire honte √† qui que ce soit, mais de fournir des incitations aux entreprises pour qu'elles s'am√©liorent". Tegmark esp√®re que les dirigeants des entreprises verront cet indice comme les universit√©s voient les classements de U.S. News and World Reports : ils pourraient ne pas aimer √™tre √©valu√©s, mais si les notes sont publiques et attirent l'attention, ils se sentiront pouss√©s √† faire mieux l'ann√©e prochaine.</p>
<p>L'un des aspects les plus pr√©occupants r√©v√©l√©s par le rapport est ce que Tegmark appelle une "course vers le bas". "Je sens que les dirigeants de ces entreprises sont pi√©g√©s dans une course vers le bas dont aucun d'eux ne peut sortir, peu importe √† quel point ils sont bien intentionn√©s", a-t-il expliqu√©. Aujourd'hui, les entreprises ne sont pas dispos√©es √† ralentir pour des tests de s√©curit√© car elles ne veulent pas que leurs concurrents les devancent sur le march√©.</p>
<h3>La dynamique du dilemme du prisonnier</h3>
<p>Cette situation repr√©sente un "dilemme du prisonnier" classique appliqu√© √† la technologie. Chaque entreprise sait qu'il vaudrait mieux que toutes d√©veloppent l'IA de mani√®re s√ªre et responsable, mais aucune ne veut √™tre la premi√®re √† ralentir, craignant de perdre un avantage concurrentiel.</p>
<p>Le r√©sultat est que toutes les entreprises finissent par courir aussi vite que possible, sacrifiant la s√©curit√© pour la vitesse. C'est comme si plusieurs constructeurs automobiles d√©cidaient de supprimer les freins de leurs voitures pour les rendre plus l√©g√®res et plus rapides, dans l'espoir d'arriver les premiers sur le march√©.</p>
<h3>L'effet multiplicateur de la concurrence</h3>
<p>Tegmark, qui a cofond√© le Future of Life Institute en 2014 dans le but de r√©duire les risques existentiels d√©coulant des technologies transformatrices, a consacr√© une grande partie de sa carri√®re acad√©mique √† essayer de comprendre l'univers physique. Mais ces derni√®res ann√©es, il s'est concentr√© sur les risques de l'intelligence artificielle, devenant l'une des voix les plus influentes dans le d√©bat sur la s√©curit√© de l'IA.</p>
<p>La pression concurrentielle ne pousse pas seulement les entreprises √† lancer des produits avant qu'ils ne soient compl√®tement s√ªrs, mais elle cr√©e √©galement un effet multiplicateur : si une entreprise r√©duit les co√ªts de s√©curit√© pour lancer plus t√¥t, les autres se sentent oblig√©es de faire de m√™me pour rester comp√©titives.</p>
<p>Ce m√©canisme pervers signifie que, m√™me si les dirigeants ou les chercheurs individuels √©taient sinc√®rement pr√©occup√©s par la s√©curit√©, la pression concurrentielle les pousse √† privil√©gier la vitesse de d√©veloppement par rapport √† la prudence. C'est un probl√®me syst√©mique qui n√©cessite une solution syst√©mique.</p>
<h2>L'analyse entreprise par entreprise</h2>
<h3>Anthropic : Le "meilleur de la classe" mais encore insuffisant</h3>
<p>Anthropic a obtenu les meilleures notes globales (C+ global), recevant le seul B- pour son travail sur les dommages actuels. Le rapport note que les mod√®les d'Anthropic ont re√ßu les scores les plus √©lev√©s dans les principaux benchmarks de s√©curit√©. L'entreprise dispose √©galement d'une "politique de mise √† l'√©chelle responsable" qui l'oblige √† √©valuer les mod√®les pour leur potentiel √† causer des dommages catastrophiques et √† ne pas d√©ployer de mod√®les jug√©s trop risqu√©s.</p>
<p>Anthropic se distingue par sa recherche active sur l'alignement de l'IA, ses politiques de s√©curit√© document√©es et publiques, sa collaboration avec des chercheurs externes et sa transparence relative sur les risques et les limitations. Cependant, m√™me Anthropic a re√ßu des recommandations d'am√©lioration, notamment la publication d'une politique compl√®te de d√©nonciation et une plus grande transparence sur la m√©thodologie d'√©valuation des risques. Le fait que m√™me la "meilleure" entreprise n'ait re√ßu qu'un C+ global illustre la gravit√© de la situation g√©n√©rale du secteur.</p>
<h3>OpenAI : Perte de capacit√© et d√©rive de mission</h3>
<p>OpenAI, l'entreprise qui a rendu l'IA grand public avec ChatGPT, a re√ßu des critiques particuli√®rement s√©v√®res. Comme le rapporte <a href="https://time.com/7302757/anthropic-xai-meta-openai-risk-management-2/">Time Magazine</a>, les recommandations incluent la reconstruction de la capacit√© de l'√©quipe de s√©curit√© perdue et la d√©monstration d'un engagement renouvel√© envers la mission originale d'OpenAI.</p>
<p>OpenAI a √©t√© fond√©e en 2015 avec la mission explicite de "garantir que l'intelligence artificielle g√©n√©rale profite √† toute l'humanit√©". Cependant, le rapport sugg√®re que l'entreprise s'est √©loign√©e de cette mission originale, se concentrant davantage sur la commercialisation que sur la s√©curit√©.</p>
<p>La mention de la "capacit√© de l'√©quipe de s√©curit√© perdue" fait r√©f√©rence aux d√©missions tr√®s m√©diatis√©es de plusieurs chercheurs en s√©curit√© d'OpenAI dans les mois pr√©c√©dant le rapport. Parmi eux figuraient certains des plus grands experts de l'alignement de l'IA, comme Ilya Sutskever (co-fondateur et ancien scientifique en chef) et Jan Leike (ancien chef de l'√©quipe de superalignement).</p>
<p>Le rapport met √©galement en √©vidence des probl√®mes de gouvernance chez OpenAI, notamment la destitution et la r√©int√©gration controvers√©es du PDG Sam Altman en novembre 2023, qui ont soulev√© des questions sur la stabilit√© et la direction de l'entreprise.</p>
<h3>Google DeepMind : Coordination insuffisante</h3>
<p>Google DeepMind a re√ßu des critiques sp√©cifiques pour une coordination insuffisante entre l'√©quipe de s√©curit√© de DeepMind et l'√©quipe politique de Google. Seul Google DeepMind a r√©pondu aux demandes de commentaires, fournissant une d√©claration affirmant : "Bien que l'indice int√®gre certains des efforts de s√©curit√© de l'IA de Google DeepMind, notre approche compl√®te de la s√©curit√© de l'IA s'√©tend au-del√† de ce qui a √©t√© captur√©."</p>
<p>Google DeepMind est le r√©sultat de la fusion entre DeepMind (acquise par Google en 2014) et Google Brain (l'√©quipe de recherche interne en IA de Google). Cette fusion, achev√©e en 2023, devait cr√©er des synergies, mais le rapport sugg√®re qu'elle a √©galement cr√©√© des probl√®mes de coordination.</p>
<p>DeepMind jouit d'une excellente r√©putation pour la recherche scientifique, ayant r√©alis√© des perc√©es comme AlphaGo (qui a battu le champion du monde de Go) et AlphaFold (qui a r√©solu le probl√®me du repliement des prot√©ines). Cependant, le rapport sugg√®re que cette excellence technique ne s'est pas traduite par un leadership en mati√®re de s√©curit√©.</p>
<h3>Meta : Probl√®mes significatifs mais pas la pire</h3>
<p>Meta a re√ßu des critiques s√©v√®res, mais n'a pas √©t√© la pire des entreprises √©valu√©es. Les recommandations incluent une augmentation significative des investissements dans la recherche sur la s√©curit√© technique, en particulier pour les protections des mod√®les √† poids ouverts (open-weight).</p>
<p>La r√©f√©rence aux "mod√®les √† poids ouverts" est particuli√®rement importante : Meta est la seule grande entreprise qui publie les "poids" de ses mod√®les (les param√®tres qui d√©terminent le comportement du mod√®le), rendant les mod√®les librement disponibles pour quiconque souhaite les utiliser ou les modifier.</p>
<p>Cette strat√©gie pr√©sente des avantages significatifs : elle permet l'innovation distribu√©e, r√©duit la concentration du pouvoir entre les mains de quelques entreprises et facilite la recherche universitaire. Mais elle comporte √©galement des risques uniques : une fois publi√©s, les mod√®les ne peuvent pas √™tre "rappel√©s" si des probl√®mes sont d√©couverts, il est impossible de contr√¥ler comment ils sont utilis√©s et ils peuvent √™tre modifi√©s √† des fins malveillantes.</p>
<p>Meta a publi√© plusieurs versions de son mod√®le Llama, y compris Llama 2 et Llama 3. Bien que ces publications aient acc√©l√©r√© la recherche et l'innovation, elles ont √©galement soulev√© des pr√©occupations en mati√®re de s√©curit√©. Le rapport sugg√®re que Meta devrait mettre en ≈ìuvre des protections plus robustes avant de publier les mod√®les.</p>
<h3>xAI : Probl√®mes culturels graves</h3>
<p>L'entreprise d'Elon Musk, xAI, a re√ßu des critiques particuli√®rement s√©v√®res non seulement pour ses scores de s√©curit√© mais aussi pour des probl√®mes culturels. Les recommandations incluent de s'attaquer √† l'extr√™me vuln√©rabilit√© au jailbreak avant la prochaine version et de d√©velopper un cadre complet de s√©curit√© de l'IA.</p>
<p>Le "jailbreaking" fait r√©f√©rence √† des techniques pour contourner les protections de s√©curit√© des syst√®mes d'IA, les convainquant de produire des contenus nuisibles ou inappropri√©s. Le fait que xAI ait une "extr√™me vuln√©rabilit√©" √† ces techniques sugg√®re que ses syst√®mes de s√©curit√© sont particuli√®rement faibles.</p>
<p>Le rapport sugg√®re que les probl√®mes de xAI pourraient √™tre li√©s √† son environnement culturel. Elon Musk a souvent exprim√© son scepticisme √† l'√©gard des r√©glementations et a promu une approche "avancer vite et casser des choses" qui pourrait ne pas √™tre compatible avec le d√©veloppement s√ªr de l'IA.</p>
<p>Le syst√®me d'IA de xAI, appel√© Grok, a √©t√© con√ßu pour √™tre "maximalement en qu√™te de v√©rit√©" et moins censur√© que d'autres syst√®mes. Cependant, cette approche a conduit √† des controverses lorsque Grok a produit des contenus probl√©matiques ou trompeurs.</p>
<h3>Zhipu AI et DeepSeek : Les pires r√©sultats</h3>
<p>Les deux entreprises chinoises, Zhipu AI et DeepSeek, ont obtenu les scores les plus bas de l'√©valuation. Les deux entreprises ont re√ßu des recommandations pour d√©velopper et publier des cadres de s√©curit√© de l'IA plus complets et augmenter consid√©rablement les efforts d'√©valuation des risques.</p>
<p>Les entreprises chinoises op√®rent dans un environnement r√©glementaire diff√©rent, o√π la s√©curit√© de l'IA est principalement consid√©r√©e sous l'angle de la s√©curit√© nationale et de la stabilit√© sociale plut√¥t que de la s√©curit√© existentielle mondiale.</p>
<p>Zhipu AI est connue pour son mod√®le ChatGLM et a re√ßu des investissements importants du gouvernement chinois. Cependant, le rapport sugg√®re que l'entreprise a investi un minimum dans la recherche sur la s√©curit√©.</p>
<p>DeepSeek est une entreprise plus petite mais ambitieuse, qui a cherch√© √† concurrencer les g√©ants occidentaux. Le rapport sugg√®re que l'entreprise a sacrifi√© la s√©curit√© pour la vitesse de d√©veloppement.</p>
<h2>L'√©chec √† faire face aux risques existentiels</h2>
<p>L'aspect peut-√™tre le plus alarmant du rapport est que les sept entreprises ont obtenu des scores particuli√®rement bas dans leurs strat√©gies de s√©curit√© existentielle. Cela signifie que, bien qu'elles aient toutes d√©clar√© leur intention de construire des syst√®mes d'intelligence artificielle g√©n√©rale, aucune n'a de plan cr√©dible pour s'assurer que ces syst√®mes restent sous contr√¥le humain.</p>
<h3>Que signifie "risque existentiel"</h3>
<p>Avant d'approfondir ce probl√®me, il est important de clarifier ce que l'on entend par "risque existentiel". Un risque existentiel est un √©v√©nement qui pourrait causer l'extinction de l'humanit√©, r√©duire de mani√®re permanente et drastique le potentiel de l'humanit√© ou rendre impossible le progr√®s de la civilisation.</p>
<p>Dans le contexte de l'intelligence artificielle, un risque existentiel pourrait survenir si nous cr√©ions des syst√®mes qui deviennent plus intelligents que nous mais ne partagent pas nos valeurs, d√©cident que l'humanit√© est un obstacle √† leurs objectifs ou √©chappent √† notre contr√¥le avant que nous puissions les √©teindre.</p>
<h3>Le probl√®me de l'alignement</h3>
<p>Comme l'a expliqu√© Tegmark : "La v√©rit√© est que personne ne sait comment contr√¥ler une nouvelle esp√®ce qui est beaucoup plus intelligente que nous. Le panel d'examen a estim√© que m√™me les entreprises qui avaient une forme de strat√©gie initiale n'√©taient pas ad√©quates."</p>
<p>Le probl√®me de l'alignement est fondamentalement le suivant : comment pouvons-nous √™tre s√ªrs qu'un syst√®me super-intelligent fasse ce que nous voulons qu'il fasse, plut√¥t que ce qu'il pense √™tre le mieux ?</p>
<p>Imaginez devoir expliquer √† un enfant de 5 ans comment g√©rer une multinationale. M√™me si l'enfant voulait aider, la diff√©rence de compr√©hension est si grande qu'il lui serait impossible de comprendre vos intentions et d'agir en cons√©quence. Maintenant, imaginez que vous √™tes l'enfant et que la multinationale est g√©r√©e par une IA super-intelligente.</p>
<h3>Les approches actuelles et leurs limites</h3>
<p>Les entreprises utilisent diff√©rentes approches pour tenter de r√©soudre le probl√®me de l'alignement. L'apprentissage par renforcement √† partir de la r√©troaction humaine (RLHF) implique l'entra√Ænement de syst√®mes d'IA en utilisant les commentaires humains pour renforcer les comportements souhaitables. Cependant, cette approche a des limites importantes : elle est difficile √† appliquer √† des syst√®mes tr√®s complexes, les humains pourraient ne pas comprendre les cons√©quences de leurs √©valuations et elle pourrait ne pas fonctionner pour des syst√®mes plus intelligents que les humains.</p>
<p>L'IA Constitutionnelle, d√©velopp√©e par Anthropic, tente d'enseigner aux syst√®mes d'IA √† suivre une "constitution" de principes. Mais le probl√®me de la d√©finition de ces principes et de la mani√®re de s'assurer qu'ils sont suivis demeure.</p>
<p>L'interpr√©tabilit√© m√©caniciste cherche √† comprendre comment les syst√®mes d'IA fonctionnent en interne. Cependant, les syst√®mes modernes sont si complexes qu'il est extr√™mement difficile de comprendre leur fonctionnement interne.</p>
<hr/>
<p><strong>[√Ä suivre dans la deuxi√®me partie]</strong></p>

            <div class="footer-back-button">
                <a href="index.html" class="back-button">Torna indietro</a>
            </div>
        </div>

        <div class="pagination-controls">

        </div>
    </main>
    <footer>
        <p>√âdit√© par <a href="https://www.verbanianotizie.it/" target="_blank" rel="noopener noreferrer">Verbania Notizie</a></p>
        <p>
            <a href="mailto:info@verbanianotizie.it">Contacts</a> |
            <a href="#">Cookie</a> |
            <a href="#">Privacy Policy</a>
        </p>
    </footer>
</body>
</html>
