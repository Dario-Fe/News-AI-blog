<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Notizie IA</title>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            margin: 0;
            background-color: #f0f2f5;
            color: #1c1e21;
        }
        header {
            background-color: #ffffff;
            padding: 20px;
            text-align: center;
            border-bottom: 1px solid #dddfe2;
            position: relative;
        }
        #language-selector-container {
            position: absolute;
            top: 20px;
            right: 20px;
        }
        #language-selector {
            padding: 8px;
            border-radius: 6px;
            border: 1px solid #dddfe2;
            background-color: #f0f2f5;
            font-size: 1em;
        }

        @media (max-width: 768px) {
            #language-selector-container {
                position: static;
                margin-bottom: 15px;
            }
        }
        header h1 {
            margin: 0;
            font-size: 2.5em;
            color: #000;
        }
        header h2 {
            margin: 5px 0 0;
            font-size: 1.2em;
            font-weight: normal;
            color: #606770;
        }
        main {
            padding: 20px;
            max-width: 1200px;
            margin: 0 auto;
        }
        #articles-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 20px;
        }
        .article-card {
            background-color: #ffffff;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            overflow: hidden;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
            text-decoration: none;
            color: inherit;
        }
        .article-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        .article-card img {
            width: 100%;
            height: 200px;
            object-fit: cover;
        }
        .article-card-content {
            padding: 15px;
        }
        .article-card-content h3 {
            margin: 0 0 10px;
            font-size: 1.2em;
        }
        .article-card-content p {
            margin: 0;
            font-size: 0.9em;
            color: #606770;
            display: -webkit-box;
            -webkit-line-clamp: 3;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }
        #article-view {
            background-color: #ffffff;
            padding: 20px 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        #article-view img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
        }
        #article-view h1 {
            font-size: 2.2em;
        }
        #article-view p {
            line-height: 1.6;
        }
        .back-button {
            display: inline-block;
            margin-bottom: 20px;
            padding: 10px 15px;
            background-color: #1877f2;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: bold;
        }
        .footer-back-button {
            margin-top: 30px;
            text-align: center;
        }
        footer {
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            background-color: #ffffff;
            border-top: 1px solid #dddfe2;
            color: #606770;
        }
        footer p {
            margin: 5px 0;
        }
        footer a {
            color: #1877f2;
            text-decoration: none;
        }
        footer a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <header>
        <div id="language-selector-container" style="display: flex; gap: 10px; font-size: 1.2em;">
            <a href="../it/index.html" title="Italiano">üáÆüáπ</a>
            <a href="../en/index.html" title="English">üá¨üáß</a>
            <a href="../es/index.html" title="Espa√±ol">üá™üá∏</a>
        </div>
        <a href="../it/index.html"><img src="../logo_vn_ia.png" alt="Notizie IA Logo" style="max-width: 100%; height: auto;"></a>
        <h2 id="subtitle">Notizie ed analisi sull'Intelligenza Artificiale</h2>
    </header>
    <main>

        <div id="article-view">
            <a href="index.html" class="back-button">Torna indietro</a>
            <h1>L'intelligenza artificiale senza controllo: le grandi aziende tech bocciate in sicurezza (Prima Puntata)</h1>
<p><em>di Dario Ferrero (VerbaniaNotizie.it)</em>
<img alt="GigantiTechAsini.jpg" src="https://raw.githubusercontent.com/matteobaccan/CorsoAIBook/main/articoli/07-AI Bocciati i Giganti/GigantiTechAsini.jpg"/></p>
<p><em>Un rapporto indipendente rivela che le principali aziende tecnologiche non sono pronte a gestire i rischi dell'intelligenza artificiale generale</em></p>
<p>Immaginate di costruire un'automobile senza freni, o di progettare un aereo senza sistemi di sicurezza. Sembra assurdo, vero? Eppure, secondo un rapporto appena pubblicato dal <a href="https://futureoflife.org/ai-safety-index-summer-2025/">Future of Life Institute</a>, √® esattamente quello che stanno facendo le principali aziende tecnologiche mondiali con l'intelligenza artificiale.</p>
<p>L'AI Safety Index 2025 ha valutato sette delle pi√π importanti aziende che sviluppano intelligenza artificiale avanzata, e i risultati sono preoccupanti: il migliore ha ottenuto un misero C+, mentre le altre hanno ricevuto voti ancora peggiori. Stiamo parlando di aziende come OpenAI (quella di ChatGPT), Google DeepMind, Meta (Facebook), xAI (di Elon Musk), e altre che stanno correndo a sviluppare quella che viene chiamata "intelligenza artificiale generale" - sistemi capaci di ragionare e risolvere problemi complessi come farebbe un essere umano, ma potenzialmente molto pi√π velocemente e potentemente.</p>
<h2>Il verdetto: "Fondamentalmente impreparate"</h2>
<p>I numeri parlano chiaro. Anthropic, l'azienda che ha creato Claude, ha ottenuto il punteggio pi√π alto con un voto complessivo di C+. Le altre sei aziende - Google DeepMind, Meta, OpenAI, xAI, Zhipu AI e DeepSeek - hanno ricevuto inferiori, con Zhipu AI e DeepSeek che hanno ottenuto i risultati peggiori.</p>
<p>Ma cosa significa concretamente questo voto? Per capirlo, bisogna prima spiegare cos'√® l'intelligenza artificiale generale, o AGI come viene chiamata nel settore. Se i sistemi attuali come ChatGPT o Gemini sono specializzati in compiti specifici (conversazione, traduzione, scrittura), l'AGI rappresenterebbe il passo successivo: un'intelligenza artificiale capace di comprendere, apprendere e applicare la conoscenza in qualsiasi campo, proprio come fa l'intelligenza umana.</p>
<p>Il problema √® che tutte le aziende valutate hanno dichiarato l'intenzione di costruire un'intelligenza artificiale generale, ma solo Anthropic, Google DeepMind e OpenAI hanno articolato una strategia per garantire che l'AGI rimanga allineata con i valori umani. E anche queste strategie sono state giudicate inadeguate dagli esperti.</p>
<p><img alt="ClassificaAiSafetyIndex.jpg" src="https://raw.githubusercontent.com/matteobaccan/CorsoAIBook/main/articoli/07-AI Bocciati i Giganti/ClassificaAiSafetyIndex.jpg"/>
<em><a href="https://futureoflife.org/ai-safety-index-summer-2025/">Immagine tratta da futureoflife.org</a></em></p>
<h2>La metodologia: come sono stati assegnati i voti</h2>
<p>Per comprendere la gravit√† della situazione, √® importante sapere come sono stati assegnati questi voti. Il Future of Life Institute ha sviluppato un sistema di valutazione rigoroso che va oltre le dichiarazioni pubbliche delle aziende per esaminare le loro pratiche concrete.</p>
<h3>I 33 indicatori di sicurezza</h3>
<p>La valutazione si basa su 33 indicatori specifici che misurano diversi aspetti dello sviluppo responsabile dell'IA. Questi indicatori non sono stati scelti a caso, ma rappresentano le migliori pratiche identificate dalla comunit√† scientifica internazionale per lo sviluppo sicuro dell'intelligenza artificiale.</p>
<p>Gli indicatori includono elementi come la presenza di politiche di sicurezza documentate, l'esistenza di team dedicati alla sicurezza, la trasparenza nelle comunicazioni sui rischi, la capacit√† di valutare i rischi prima del rilascio, l'implementazione di sistemi di monitoraggio continuo e la presenza di meccanismi di segnalazione per i dipendenti.</p>
<h3>I sei domini critici</h3>
<p>I 33 indicatori sono organizzati in sei domini fondamentali che coprono aspetti diversi ma interconnessi della sicurezza dell'intelligenza artificiale.</p>
<p>Il primo dominio riguarda la sicurezza esistenziale e valuta se le aziende hanno strategie per prevenire rischi che potrebbero minacciare l'esistenza dell'umanit√†, inclusa la capacit√† di valutare quando un sistema potrebbe diventare troppo potente per essere controllato.</p>
<p>Il secondo dominio esamina i danni attuali, analizzando come le aziende affrontano i rischi gi√† presenti nell'IA come i pregiudizi algoritmici, la disinformazione o l'uso improprio della tecnologia.</p>
<p>Il terzo dominio √® la trasparenza, che valuta quanto le aziende sono aperte riguardo ai loro metodi, rischi e limitazioni, includendo la disponibilit√† a condividere informazioni con ricercatori indipendenti.</p>
<p>Il quarto dominio riguarda la governance ed esamina la struttura organizzativa delle aziende, inclusa la presenza di supervisione indipendente e processi decisionali chiari per questioni di sicurezza.</p>
<p>Il quinto dominio valuta l'impegno con la comunit√†, esaminando se le aziende collaborano con ricercatori esterni, organizzazioni di sicurezza e la comunit√† scientifica pi√π ampia.</p>
<p>Infine, il sesto dominio esamina la preparazione normativa, verificando se le aziende sono pronte a lavorare con i regolatori e se supportano lo sviluppo di normative appropriate.</p>
<h3>Il processo di peer review</h3>
<p>I dati sono stati raccolti tra marzo e giugno 2025, combinando materiali pubblicamente disponibili con risposte a questionari mirati inviati alle aziende. Tuttavia, solo due aziende (xAI e Zhipu AI) hanno completato completamente i questionari, evidenziando un preoccupante livello di non-collaborazione da parte del settore.</p>
<p>I voti sono stati assegnati da un panel di sette esperti indipendenti, tra cui nomi prestigiosi come Stuart Russell dell'Universit√† di California, Berkeley, e il vincitore del Premio Turing Yoshua Bengio. Questo panel includeva sia esperti che si sono concentrati sui rischi esistenziali dell'IA sia quelli che hanno lavorato sui danni a breve termine come il bias algoritmico e il linguaggio tossico.</p>
<p>Il processo di valutazione √® stato progettato per essere il pi√π oggettivo possibile, con criteri standardizzati e multiple revisioni indipendenti per ogni azienda.</p>
<h2>Il grido d'allarme degli esperti</h2>
<p>Le conclusioni del rapporto sono state durissime. Stuart Russell, uno dei massimi esperti mondiali di sicurezza dell'IA, ha dichiarato in un'intervista a <a href="https://spectrum.ieee.org/ai-safety">IEEE Spectrum</a>: "I risultati del progetto AI Safety Index suggeriscono che, sebbene ci sia molta attivit√† nelle aziende di IA che va sotto il nome di 'sicurezza', non √® ancora molto efficace. In particolare, nessuna delle attivit√† attuali fornisce alcun tipo di garanzia quantitativa di sicurezza".</p>
<p>Russell ha aggiunto una considerazione ancora pi√π preoccupante: "√à possibile che l'attuale direzione tecnologica non possa mai supportare le necessarie garanzie di sicurezza, nel qual caso si tratterebbe davvero di un vicolo cieco".</p>
<h2>Il panorama globale degli incidenti IA</h2>
<p>Per comprendere l'urgenza del problema, √® essenziale guardare ai dati sui malfunzionamenti dell'intelligenza artificiale che stanno gi√† accadendo. Il numero di incidenti registrati sta crescendo in modo esponenziale, e le conseguenze stanno diventando sempre pi√π gravi.</p>
<h3>I numeri allarmanti del 2024</h3>
<p>Secondo l'AI Incidents Database, il numero di incidenti legati all'IA √® aumentato a 233 nel 2024 - un record assoluto e un aumento del 56,4% rispetto al 2023. Questi non sono errori minori o problemi tecnici trascurabili, ma eventi che hanno causato danni reali a persone, aziende e societ√†.</p>
<h3>Casi emblematici di malfunzionamenti</h3>
<p>Il sistema di guida autonoma Tesla ha mostrato problemi di "automation bias", ovvero la tendenza degli utenti a fidarsi eccessivamente dei sistemi automatizzati. La NHTSA (National Highway Traffic Safety Administration) ha aperto un'indagine di sicurezza per fino a 2,4 milioni di veicoli Tesla, includendo un incidente fatale con un pedone mentre era attivo il sistema Full Self-Driving. Questo significa che l'azienda texana √® colpevole? No. √à un sistema che aiuta, un ausilio alla guida. Chi si mette al volante lo sa, o lo deve sapere. Se il conducente dorme, guarda lo smartphone, mangia o fa altro, √® colpa sua, non dell'elettronica.</p>
<p>Un caso significativo ha riguardato un autista di Uber Eats che √® stato licenziato dopo che il sistema di riconoscimento facciale non √® riuscito a identificarlo correttamente. Il conducente ha sostenuto che la tecnologia √® meno accurata per le persone non bianche, mettendole in svantaggio. Da quello che ci risulta, Uber ha implementato un sistema di validazione "umana" che prevede la revisione da parte di almeno due esperti prima di procedere con un licenziamento.</p>
<p>Nel settore sanitario, sistemi di IA utilizzati in ospedali hanno fornito diagnosi errate, portando a cure inappropriate. Un caso documentato ha visto un algoritmo di screening per il cancro produrre falsi positivi nel 70% dei casi, causando stress emotivo e costi sanitari inutili.</p>
<p>Durante le elezioni del 2024, diversi sistemi di IA hanno generato contenuti politici fuorvianti, incluse immagini deepfake di candidati in situazioni compromettenti.</p>
<h3>Il costo umano ed economico</h3>
<p>Questi incidenti non sono solo statistiche. Dietro ogni numero c'√® una persona che ha perso il lavoro a causa di un algoritmo discriminatorio, una famiglia che ha subito un incidente stradale causato da un sistema di guida autonoma difettoso, o un paziente che ha ricevuto una diagnosi errata. Di conseguenza √® logico prevedere anche notevoli danni economici, che al momento nessuno sembra aver stimato.</p>
<h2>Il problema della "corsa al ribasso"</h2>
<p>Max Tegmark, fisico del MIT e presidente del Future of Life Institute, ha spiegato l'obiettivo del rapporto: "Lo scopo non √® quello di svergognare nessuno, ma di fornire incentivi alle aziende per migliorare". Tegmark spera che i dirigenti delle aziende vedano questo indice come le universit√† vedono le classifiche di U.S. News and World Reports: potrebbero non gradire essere valutate, ma se i voti sono pubblici e stanno attirando l'attenzione, si sentiranno spinte a fare meglio l'anno prossimo.</p>
<p>Uno degli aspetti pi√π preoccupanti emersi dal rapporto √® quello che Tegmark definisce una "corsa al ribasso". "Sento che i leader di queste aziende sono intrappolati in una corsa al ribasso da cui nessuno di loro pu√≤ uscire, non importa quanto siano di buon cuore", ha spiegato. Oggi, le aziende non sono disposte a rallentare per i test di sicurezza perch√© non vogliono che i concorrenti le battano sul mercato.</p>
<h3>La dinamica del prisoner's dilemma</h3>
<p>Questa situazione rappresenta un classico "dilemma del prigioniero" applicato alla tecnologia. Ogni azienda sa che sarebbe meglio se tutte sviluppassero l'IA in modo sicuro e responsabile, ma nessuna vuole essere la prima a rallentare, temendo di perdere vantaggio competitivo.</p>
<p>Il risultato √® che tutte le aziende finiscono per correre il pi√π velocemente possibile, sacrificando la sicurezza per la velocit√†. √à come se diverse aziende automobilistiche decidessero di eliminare i freni dalle loro auto per renderle pi√π leggere e veloci, nella speranza di arrivare per prime al mercato.</p>
<h3>L'effetto moltiplicatore della concorrenza</h3>
<p>Tegmark, che ha co-fondato il Future of Life Institute nel 2014 con l'obiettivo di ridurre i rischi esistenziali derivanti da tecnologie trasformative, ha dedicato gran parte della sua carriera accademica a cercare di capire l'universo fisico. Ma negli ultimi anni si √® concentrato sui rischi dell'intelligenza artificiale, diventando una delle voci pi√π autorevoli nel dibattito sulla sicurezza dell'IA.</p>
<p>La pressione competitiva non solo spinge le aziende a rilasciare prodotti prima che siano completamente sicuri, ma crea anche un effetto moltiplicatore: se un'azienda taglia i costi della sicurezza per rilasciare prima, le altre si sentono costrette a fare lo stesso per rimanere competitive.</p>
<p>Questo meccanismo perverso significa che, anche se i singoli dirigenti o ricercatori fossero genuinamente preoccupati per la sicurezza, la pressione competitiva li spinge a mettere la velocit√† di sviluppo davanti alla prudenza. √à un problema sistemico che richiede una soluzione sistemica.</p>
<h2>L'analisi azienda per azienda</h2>
<h3>Anthropic: Il "migliore della classe" ma ancora insufficiente</h3>
<p>Anthropic ha ottenuto i migliori punteggi complessivi (C+ globale), ricevendo l'unico B- per il suo lavoro sui danni attuali. Il rapporto nota che i modelli di Anthropic hanno ricevuto i punteggi pi√π alti nei principali benchmark di sicurezza. L'azienda ha anche una "politica di scaling responsabile" che impone di valutare i modelli per il loro potenziale di causare danni catastrofici e di non implementare modelli giudicati troppo rischiosi.</p>
<p>Anthropic si distingue per la sua ricerca attiva sull'allineamento dell'IA, le politiche di sicurezza documentate e pubbliche, la collaborazione con ricercatori esterni e la trasparenza relativa sui rischi e limitazioni. Tuttavia, anche Anthropic ha ricevuto raccomandazioni per migliorare, tra cui pubblicare una politica completa di whistleblowing e diventare pi√π trasparente sulla metodologia di valutazione del rischio. Il fatto che anche l'azienda "migliore" abbia ricevuto solo un C+ complessivo illustra quanto sia grave la situazione generale del settore.</p>
<h3>OpenAI: Perdita di capacit√† e mission drift</h3>
<p>OpenAI, l'azienda che ha reso l'IA mainstream con ChatGPT, ha ricevuto critiche particolarmente severe. Come riportato da <a href="https://time.com/7302757/anthropic-xai-meta-openai-risk-management-2/">Time Magazine</a>, le raccomandazioni includono ricostruire la capacit√† del team di sicurezza perduta e dimostrare un rinnovato impegno verso la missione originale di OpenAI.</p>
<p>OpenAI √® stata fondata nel 2015 con la missione esplicita di "assicurare che l'intelligenza artificiale generale benefici tutta l'umanit√†". Tuttavia, il rapporto suggerisce che l'azienda si √® allontanata da questa missione originale, concentrandosi maggiormente sulla commercializzazione che sulla sicurezza.</p>
<p>La menzione della "capacit√† del team di sicurezza perduta" si riferisce alle dimissioni di alto profilo di diversi ricercatori di sicurezza da OpenAI nei mesi precedenti al rapporto. Questi includevano alcuni dei principali esperti di allineamento dell'IA, come Ilya Sutskever (co-fondatore e ex-chief scientist) e Jan Leike (ex-capo del team di superallineamento).</p>
<p>Il rapporto evidenzia anche problemi nella governance di OpenAI, inclusa la controversa rimozione e reintegrazione del CEO Sam Altman nel novembre 2023, che ha sollevato domande sulla stabilit√† e direzione dell'azienda.</p>
<h3>Google DeepMind: Coordinamento insufficiente</h3>
<p>Google DeepMind ha ricevuto critiche specifiche per il coordinamento insufficiente tra il team di sicurezza di DeepMind e il team di policy di Google. Solo Google DeepMind ha risposto alle richieste di commento, fornendo una dichiarazione che afferma: "Sebbene l'indice incorpori alcuni degli sforzi di sicurezza dell'IA di Google DeepMind, il nostro approccio completo alla sicurezza dell'IA si estende oltre quello che √® stato catturato".</p>
<p>Google DeepMind √® il risultato della fusione tra DeepMind (acquisita da Google nel 2014) e Google Brain (il team di ricerca IA interno di Google). Questa fusione, completata nel 2023, doveva creare sinergie, ma il rapporto suggerisce che ha anche creato problemi di coordinamento.</p>
<p>DeepMind ha una reputazione eccellente per la ricerca scientifica, avendo raggiunto breakthrough come AlphaGo (che ha battuto il campione mondiale di Go) e AlphaFold (che ha risolto il problema del folding delle proteine). Tuttavia, il rapporto suggerisce che questa eccellenza tecnica non si √® tradotta in leadership nella sicurezza.</p>
<h3>Meta: Problemi significativi ma non la peggiore</h3>
<p>Meta ha ricevuto critiche severe, ma non √® risultata la peggiore tra le aziende valutate. Le raccomandazioni includono aumentare significativamente gli investimenti nella ricerca sulla sicurezza tecnica, specialmente per le protezioni dei modelli open-weight.</p>
<p>Il riferimento ai "modelli open-weight" √® particolarmente importante: Meta √® l'unica grande azienda che rilascia i "pesi" dei suoi modelli (i parametri che determinano il comportamento del modello), rendendo i modelli liberamente disponibili per chiunque li voglia usare o modificare.</p>
<p>Questa strategia presenta vantaggi significativi: permette l'innovazione distribuita, riduce la concentrazione del potere nelle mani di poche aziende e facilita la ricerca accademica. Ma comporta anche rischi unici: una volta rilasciati, i modelli non possono essere "richiamati" se si scoprono problemi, √® impossibile controllare come vengono utilizzati e possono essere modificati per scopi dannosi.</p>
<p>Meta ha rilasciato diverse versioni del suo modello Llama, inclusa Llama 2 e Llama 3. Mentre questi rilasci hanno accelerato la ricerca e l'innovazione, hanno anche sollevato preoccupazioni sulla sicurezza. Il rapporto suggerisce che Meta dovrebbe implementare protezioni pi√π robuste prima di rilasciare i modelli.</p>
<h3>xAI: Problemi culturali gravi</h3>
<p>L'azienda di Elon Musk, xAI, ha ricevuto critiche particolarmente severe non solo per i suoi punteggi di sicurezza ma anche per problemi culturali. Le raccomandazioni includono affrontare l'estrema vulnerabilit√† del jailbreak prima del prossimo rilascio e sviluppare un framework completo di sicurezza dell'IA.</p>
<p>Il "jailbreaking" si riferisce a tecniche per aggirare le protezioni di sicurezza dei sistemi di IA, convincendoli a produrre contenuti dannosi o inappropriati. Il fatto che xAI abbia una "estrema vulnerabilit√†" a queste tecniche suggerisce che i suoi sistemi di sicurezza sono particolarmente deboli.</p>
<p>Il rapporto suggerisce che i problemi di xAI potrebbero essere legati al suo ambiente culturale. Elon Musk ha spesso espresso scetticismo verso le regolamentazioni e ha promosso un approccio "move fast and break things" che potrebbe non essere compatibile con lo sviluppo sicuro dell'IA.</p>
<p>Il sistema di IA di xAI, chiamato Grok, √® stato progettato per essere "maximally truth-seeking" e meno censurato rispetto ad altri sistemi. Tuttavia, questo approccio ha portato a controversie quando Grok ha prodotto contenuti problematici o fuorvianti.</p>
<h3>Zhipu AI e DeepSeek: I risultati peggiori</h3>
<p>Le due aziende cinesi, Zhipu AI e DeepSeek, hanno ottenuto i punteggi pi√π bassi nella valutazione. Entrambe le aziende hanno ricevuto raccomandazioni per sviluppare e pubblicare framework di sicurezza dell'IA pi√π completi e aumentare drasticamente gli sforzi di valutazione dei rischi.</p>
<p>Le aziende cinesi operano in un ambiente normativo diverso, dove la sicurezza dell'IA √® vista principalmente attraverso la lente della sicurezza nazionale e della stabilit√† sociale piuttosto che della sicurezza esistenziale globale.</p>
<p>Zhipu AI √® nota per il suo modello ChatGLM e ha ricevuto significativi investimenti dal governo cinese. Tuttavia, il rapporto suggerisce che l'azienda ha investito minimamente nella ricerca sulla sicurezza.</p>
<p>DeepSeek √® un'azienda pi√π piccola ma ambiziosa, che ha cercato di competere con i giganti occidentali. Il rapporto suggerisce che l'azienda ha sacrificato la sicurezza per la velocit√† di sviluppo.</p>
<h2>Il fallimento nell'affrontare i rischi esistenziali</h2>
<p>Forse l'aspetto pi√π allarmante del rapporto √® che tutte e sette le aziende hanno ottenuto punteggi particolarmente bassi nelle loro strategie di sicurezza esistenziale. Questo significa che, nonostante tutte abbiano dichiarato l'intenzione di costruire sistemi di intelligenza artificiale generale, nessuna ha un piano credibile per assicurarsi che questi sistemi rimangano sotto controllo umano.</p>
<h3>Cosa significa "rischio esistenziale"</h3>
<p>Prima di approfondire questo problema, √® importante chiarire cosa si intende per "rischio esistenziale". Un rischio esistenziale √® un evento che potrebbe causare l'estinzione dell'umanit√†, ridurre permanentemente e drasticamente il potenziale dell'umanit√† o rendere impossibile il progresso della civilt√†.</p>
<p>Nel contesto dell'intelligenza artificiale, un rischio esistenziale potrebbe verificarsi se creassimo sistemi che diventano pi√π intelligenti di noi ma non condividono i nostri valori, decidono che l'umanit√† √® un ostacolo ai loro obiettivi o sfuggono al nostro controllo prima che possiamo spegnerli.</p>
<h3>Il problema dell'allineamento</h3>
<p>Come ha spiegato Tegmark: "La verit√† √® che nessuno sa come controllare una nuova specie che √® molto pi√π intelligente di noi. Il panel di revisione ha sentito che anche le aziende che avevano una qualche forma di strategia iniziale, non erano adeguate".</p>
<p>Il problema dell'allineamento √® fondamentalmente questo: come facciamo a essere sicuri che un sistema super-intelligente faccia quello che vogliamo che faccia, piuttosto che quello che pensa sia meglio?</p>
<p>Immaginate di dover spiegare a un bambino di 5 anni come gestire una multinazionale. Anche se il bambino volesse aiutare, la differenza di comprensione √® cos√¨ grande che sarebbe impossibile per lui capire le vostre intenzioni e agire di conseguenza. Ora immaginate che il bambino siate voi e la multinazionale sia gestita da un'IA super-intelligente.</p>
<h3>Gli approcci attuali e i loro limiti</h3>
<p>Le aziende stanno usando diversi approcci per cercare di risolvere il problema dell'allineamento. Il Reinforcement Learning from Human Feedback (RLHF) coinvolge l'addestramento di sistemi di IA usando feedback umano per rinforzare comportamenti desiderabili. Tuttavia, questo approccio ha limiti significativi: √® difficile scalare a sistemi molto complessi, gli umani potrebbero non capire le conseguenze delle loro valutazioni e potrebbe non funzionare per sistemi pi√π intelligenti degli umani.</p>
<p>Il Constitutional AI, sviluppato da Anthropic, cerca di insegnare ai sistemi di IA a seguire una "costituzione" di principi. Ma rimane il problema di come definire questi principi e come assicurarsi che siano seguiti.</p>
<p>L'interpretabilit√† meccanicistica cerca di capire come funzionano internamente i sistemi di IA. Tuttavia, i sistemi moderni sono cos√¨ complessi che √® estremamente difficile comprendere il loro funzionamento interno.</p>
<hr/>
<p><strong>[Continua nella seconda puntata]</strong></p>

            <div class="footer-back-button">
                <a href="index.html" class="back-button">Torna indietro</a>
            </div>
        </div>

    </main>
    <footer>
        <p>A cura di Verbania Notizie</p>
        <p>
            <a href="mailto:info@verbanianotizie.it">Contatti</a> |
            <a href="#">Cookie</a> |
            <a href="#">Privacy Policy</a>
        </p>
    </footer>
</body>
</html>
