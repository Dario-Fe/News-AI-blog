How AI is redefining the programmer's craft: A conversation with Enrico Papalini

papalini-interview.jpg

Enrico Papalini has spent over twenty years writing and orchestrating software systems in contexts where you can't afford to push code that "probably works." As Head of Engineering Excellence and Innovation at Borsa Italiana, part of the Euronext group, he works on infrastructure where a bug doesn't just mean a Jira ticket, but potentially millions of euros in blocked transactions. Before that: Microsoft, Intesa Sanpaolo, startups, and large corporations. His LinkedIn profile tells the story of someone who has had to balance innovation and reliability at every stage of his career.

He's not the type to theorize from the outside. He's someone who has had to answer questions like: "Can we deploy this to production?" when "this" involves generative artificial intelligence and the system manages critical infrastructure.

Now he has written a book that summarizes this experience: *Intelligenza Artificiale e Ingegneria del Software: Cosa debbono fare le imprese*, also available in English as *Non-Deterministic Software Engineering*. The book is based on DX research on over 180 companies, integrates DORA metrics adapted for AI development, and analyzes case studies of those who have already gone through it: OpenAI, Shopify, Google. To write it, he dialogued with Martin Fowler, Kent Beck, and Addy Osmani.
The Broken Silent Pact

While tutorials on "10x your coding with AI" continue to be released, Papalini has chosen to approach the problem from another angle. I ask him what prompted him to write this particular book, right now.

"Everyone talks about speed, but the real revolution is something else," he replies. "It's a change in the nature of the tools we use. For forty years, we took one thing for granted: you write code, and it does exactly what you wrote. Always. It's on this certainty that we've built everything—how we test, how we debug, how we work in teams. Generative AI breaks this silent pact. Not because it's faulty, but because it has a probabilistic nature. You ask it the same thing twice, it gives you different answers. Sometimes brilliant, sometimes wrong with disarming confidence."

The metaphor he uses is striking: "Companies that think they can replace programmers with AI and look smugly at how many more lines of code they can produce are missing the point: they are introducing a random variable into the heart of their systems. It's a bit like a civil engineer building a bridge with materials that *might* support the expected weight. It works great when a few cars pass, but when the first truck comes, it collapses."

He wrote the book, he says, "because it seemed to me that there was no guide for those who have to navigate this change without crashing, but also without giving up the benefits, which are real."
From Determinism to Tolerance

The conceptual heart of the book is in its English title: *Non-Deterministic Software Engineering*. It's a deliberate oxymoron. Software engineering has always been the art of building deterministic systems: input A always produces output B. Papalini is proposing that we welcome tools into our workflows that, by their nature, do not respect this rule.

I ask him how the quality control paradigm changes when we move from a world where code did exactly what was written, to a world where we welcome probabilistic systems into our IDEs.

"Everything changes, and at the same time, nothing. I know, it sounds like a paradox," he begins. "Everything changes because 'it works' no longer means 'it's correct.' The code generated by AI compiles, passes the tests you've written, and looks professional. But it could hide vulnerabilities, handle edge cases poorly, or be written in a way that no one will understand in six months. Nothing changes because the fundamentals of software engineering remain the same: testing, reviews, design thinking. In fact, they become more important than ever."

The key, according to Papalini, lies in adopting an approach that has so far been foreign to developers: "The real novelty is that we must learn to think in terms of 'tolerances.' Martin Fowler often uses this analogy: his wife is a structural engineer, and she never designs to the exact limit. She always calculates a safety margin. We developers have never had to do that because our 'materials' were perfectly predictable. Now they are not. And those who don't build these margins will sooner or later see their 'bridge' collapse."
The Illusion of Speed

One of the most unsettling passages in the book concerns productivity data. Papalini cites a METR study showing that developers can feel 20% faster with artificial intelligence, while real tests on measurable tasks indicate that in some cases they are 19% slower.

I ask him how this discrepancy is possible, and above all, how we can truly measure whether we are working better or just feeling more productive.

"You know what struck me most about that study? It wasn't the fact that they were 19% slower with AI. It's that the developers *believed* they were faster. And they continued to believe it even after seeing the results," Papalini recounts. "Why does this happen? Because AI reduces perceived effort. You feel more fluid, less stuck. It's like having a colleague who is always available to help, who doesn't judge you and doesn't make you wait. Psychologically, it's powerful. But 'I feel productive' and 'I am producing value' are two very different things."

The solution he proposes also applies at the individual level: "To avoid falling for the hype, a company should establish a baseline before adopting the tools. Without a 'before,' you can never prove an 'after.' When the CEO asks what AI has brought, you need numbers, not feelings. But the numbers must be the right ones: stop measuring lines of code or how many AI suggestions are accepted. Measure what matters: features released, bugs in production, time to resolve incidents. And one fundamental thing: measure how motivated the developers remain."
The "It Works" Trap

Andrej Karpathy popularized the term "vibe coding": programming by following the intuition of the moment, letting AI suggest directions that "feel right." It's fascinating and deeply dangerous.

Papalini dedicates several pages to what he calls the "It Works Trap." I ask him what the long-term risks are of a codebase written primarily by following the vibe, without rigorous validation.

"Let me tell you a story," he begins. "Martin Fowler had used AI to generate a visualization in SVG vector format, nothing complex. It worked perfectly. Then he wanted to make a trivial change: move a label by a few pixels. He opened the file and found what he called 'crazy stuff'—code that worked, yes, but was structured in a completely alien way, impossible to touch without breaking everything. The only option? Throw it away and regenerate from scratch."

The anecdote perfectly captures the problem: "This is the real cost and risk of vibe coding on a corporate scale. You create systems that work but that no one understands. And enterprise software has to live for years, sometimes decades. It needs to be modified, extended, debugged at three in the morning when something explodes."

The solution he proposes is simple in its formulation but requires discipline: "The rule we must follow is simple: never commit code that you can't explain to a colleague. If it's generated by AI and I don't understand how it works, it's not ready for production."
The Price of Unreliability

In the book, Papalini introduces the concept of the "unreliability tax." I ask him to quantify, from the point of view of someone who writes and reviews code every day, what the real cost is of AI-generated code that seems correct but hides vulnerabilities.

"The numbers are sobering," he begins. "Research shows that a significant percentage of AI-generated code contains vulnerabilities; some sources say almost half. And the insidious thing is that they are 'plausible' vulnerabilities: the code looks professional, uses recognizable patterns. It's just that it's missing input validation at that critical point, or it uses a deprecated cryptographic function."

These are not obvious errors: "The direct cost is remediation time. But the real cost is what you don't see right away: the vulnerability that goes unnoticed for months, until someone finds it. At that point, you're not paying for development hours, you're paying for incident response, potential data breaches, damage to your reputation."

Papalini also identifies a more personal cost: "There's also a more subtle tax: the loss of trust in the system. When the team starts to distrust its own code, everything slows down. Every change becomes a risk. And the same goes for customers: there's a risk they'll switch to a more reliable competitor."

His recommendation is practical: "The sensible investment is in prevention: automatic security scanning, targeted training, mandatory human review for anything that touches authentication or sensitive data. It costs less than cleaning up the mess afterward and losing customers."
The Grind in the Age of Machines

One of the most provocative sections of the book concerns the future of training. An emerging distinction is being drawn between traditional programmers and the new figure of the AI Engineer—someone who knows how to orchestrate intelligent systems but may have never implemented a sorting algorithm from scratch.

I ask him if the "pure" programmer is destined to disappear or if AI is simply raising the bar for the necessary skills.

"The programmer is not destined to disappear, but their role is changing a lot," he responds. "Think about what happened when high-level languages arrived. Assembly programmers didn't disappear; they became niche specialists. The bulk of the work moved up a level."

The current transition follows a similar pattern: "Something similar is happening now. A different figure is emerging, let's call them an 'orchestrator': someone who knows how to break down complex problems, specify requirements with precision, critically evaluate what AI produces, and make architectural decisions."

But here comes the paradox: "The paradox? It requires more experience, not less. A junior can use AI to generate code that seems to work. But only a senior recognizes when that code is a ticking time bomb, because they've seen enough disasters to recognize the signs."

The systemic risk he identifies affects every developer's career: "We also need to be careful not to think that everyone is born an orchestrator. If we delegate all the 'grunt work' to AI, how do we train the next generation of seniors? The data tells us that employment for younger developers is already declining. But even those entering the market risk never developing the deep skills that are only built by banging your head against problems."
Trio Programming

To solve this dilemma, Papalini proposes a model in the book that he calls "trio programming," an evolution of pair programming that includes AI as a third actor. It's a practical solution for those who work with these tools daily.

"In the book, I propose 'trio programming' as a solution to the training problem," he explains. "The junior works with the AI to implement features. The AI accelerates, suggests, generates code. So far, nothing new. The senior orchestrator doesn't write code; they are there to ask questions. 'Explain what this method does.' 'Why did the AI choose this data structure?' 'What happens if the input is null?' 'How would you handle a network error here?'"

The mechanism is pedagogically brilliant: "By answering these questions, the junior learns. The senior, in turn, transfers that tacit knowledge that isn't in any manual—the intuition about what can go wrong, the feel for code that 'smells,' the experience of someone who has seen systems collapse."

It's a model that can also be applied by those who work alone: replace the "senior" with a mental checklist of questions to ask yourself before each commit. Did AI generate that function for you? Perfect. Now explain it to yourself. If you can't, it's not ready.
Orchestrators, Not Typists

The conversation returns to the theme of professional change. If AI gets better and better at writing code, what is the distinctive value of the human engineer? What do we need to learn, and what do we need to preserve?

"The engineer's role is shifting towards specification and validation, less towards implementation," Papalini summarizes. "But this change will require more competence from workers, not less."

It's a crucial point that contradicts the popular narrative of AI as the democratization of programming. We are not lowering the barriers to entry; we are moving them. Before, you had to know how to write syntactically correct code. Now, you have to know how to recognize when syntactically correct code is semantically dangerous.

The skills that matter are changing: less syntax, more architecture. Less implementation, more design. Less "how to write a loop," more "why this approach is problematic at scale." It's like going from a bricklayer to a structural engineer: the hammer matters less, the physics matters more.
Skills Atrophy

There is a theme that runs through the entire book: the risk that the massive adoption of AI will not amplify our skills but cause them to atrophy. It's the question every developer should be asking themselves.

I ask him directly: how can we ensure that the daily use of AI does not devalue our professionalism but becomes a real amplifier of our abilities?

"This is the question that is dearest to my heart," Papalini replies. "The concrete risk I call 'skills atrophy.' An engineer interviewed by MIT Technology Review said that after months of intensive AI use, when he tried to program without it, he felt lost. Things that were once instinct had become laborious. This is exactly the alarm bell we should be listening to."

The solution is not rejection but intentionality: "The solution is not to reject AI; that would be like rejecting electricity. But we must be intentional about how we integrate it, especially in training paths. That's why in the book I propose models like 'trio programming' to cultivate the abilities of our talent and not make the mistake of sawing off the bottom rung of the ladder we climbed, the learning process that got us where we are."

It's a personal challenge for every developer: how you use AI says a lot about what kind of professional you will become. You can use it as a crutch that allows you to avoid understanding what you're doing, or as an amplifier that frees you from the boring parts to focus on those that require judgment. The first path leads to obsolescence. The second to growth.
The Changing Craft

At the end of this conversation, what emerges clearly is that we are living through one of those transitional moments that redefine a profession. The book *Intelligenza Artificiale e Ingegneria del Software* is not a manual on how to use Copilot faster. It's more like those texts that come out after a technological revolution, when someone who has lived through it on the front lines tries to map what has really changed and what has only changed in appearance.

The real question is not whether we will use AI to write code. We are already using it. The question is whether we can do so without losing the deep skills that allowed us to become capable developers. Those skills that are only built by debugging a segfault for hours, only by reading horrible code written by others, only by making creative mistakes and learning from disasters.

AI can generate code in seconds. But it cannot generate the experience that allows you to recognize when that code hides a problem. Not yet. And perhaps never. It is that space, between generation and judgment, that defines the value of a developer in the age of artificial intelligence. The question is: are you cultivating that judgment, or are you delegating it?
